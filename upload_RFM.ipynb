{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01_import dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01-1_import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "#https://drive.google.com/drive/folders/18qV82fNY3IIWu3BRoGqm_LNgJzE8Akbr?usp=drive_link\n",
    "#base_dir = \"/Users/Andypon/10_äº¤å¤§ç ”ç©¶æ‰€/1141_01_æ©Ÿå™¨å­¸ç¿’èˆ‡é‡‘èç§‘æŠ€/data\"\n",
    "base_dir= '/Users/andyw.p.chen/Documents/Project/datasets'\n",
    "#base_dir=  \"c:\\Users\\user\\Downloads\\datasets\"\n",
    "\n",
    "def load_json_to_df(filename: str) -> pd.DataFrame:\n",
    "    file_path = os.path.join(base_dir, filename)\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # å¦‚æœæ˜¯ { \"target\": {id: value, ...} }\n",
    "    if isinstance(data, dict) and len(data) == 1 and isinstance(next(iter(data.values())), dict):\n",
    "        key, inner = next(iter(data.items()))\n",
    "        return pd.DataFrame(list(inner.items()), columns=[\"id\", key])\n",
    "\n",
    "    # dict of scalar\n",
    "    if isinstance(data, dict):\n",
    "        return pd.DataFrame([{\"code\": k, \"desc\": v} for k, v in data.items()])\n",
    "\n",
    "    # list of dict\n",
    "    elif isinstance(data, list):\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported JSON structure in {filename}: {type(data)}\")\n",
    "\n",
    "\n",
    "def load_csv_to_df(filename: str) -> pd.DataFrame:\n",
    "    \"\"\"è®€å– CSV ä¸¦è½‰ç‚º DataFrameã€‚\"\"\"\n",
    "    return pd.read_csv(os.path.join(base_dir, filename))\n",
    "\n",
    "# JSON è³‡æ–™\n",
    "##mcc_codes_df = load_json_to_df(\"mcc_codes.json\")\n",
    "train_fraud_labels_df = load_json_to_df(\"train_fraud_labels.json\")\n",
    "\n",
    "# CSV è³‡æ–™\n",
    "cards_df = load_csv_to_df(\"cards_data.csv\")\n",
    "transactions_df = load_csv_to_df(\"transactions_data.csv\")\n",
    "users_df = load_csv_to_df(\"users_data.csv\")\n",
    "\n",
    "# ç°¡å–®æª¢æŸ¥\n",
    "#print(mcc_codes_df.head())\n",
    "#print(train_fraud_labels_df.head())\n",
    "#print(cards_df.head())\n",
    "#print(transactions_df.head())\n",
    "#print(users_df.apthead())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01-2_rename variable in each data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fraud_labels_df = train_fraud_labels_df.rename(columns={'id': 'transactions_id'})\n",
    "train_fraud_labels_df = train_fraud_labels_df.rename(columns={'target': 'is_fraud'})\n",
    "\n",
    "cards_df = cards_df.rename(columns={'id':'card_id'})\n",
    "\n",
    "users_df = users_df.rename(columns={'id':'client_id'})\n",
    "\n",
    "transactions_df = transactions_df.rename(columns={'mcc': 'mcc_code'})\n",
    "transactions_df = transactions_df.rename(columns={'id': 'transaction_id'})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01-3_è®Šæ•¸å‹æ…‹çµ±ä¸€åŠç¼ºå¤±å€¼è™•ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing_flags(df: pd.DataFrame, cols: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    åœ¨ DataFrame ä¸­å°æŒ‡å®šæ¬„ä½å»ºç«‹ missing flag æ¬„ä½\n",
    "    flag=1 è¡¨ç¤ºç¼ºå¤±å€¼ï¼Œflag=0 è¡¨ç¤ºéç¼ºå¤±å€¼\n",
    "    \n",
    "    åƒæ•¸\n",
    "    ----\n",
    "    df : pd.DataFrame\n",
    "        è¼¸å…¥çš„è³‡æ–™æ¡†\n",
    "    cols : list\n",
    "        è¦æª¢æŸ¥çš„æ¬„ä½åç¨±æ¸…å–®\n",
    "    \n",
    "    å›å‚³\n",
    "    ----\n",
    "    pd.DataFrame : æ–°çš„è³‡æ–™æ¡† (å«æ–°å¢çš„ flag æ¬„ä½)\n",
    "    \"\"\"\n",
    "    for col in cols:\n",
    "        df[f\"{col}_missing_flag\"] = df[col].isna().astype(int)\n",
    "    return df\n",
    "\n",
    "transactions_df = add_missing_flags(transactions_df, [\"merchant_state\", \"zip\", \"errors\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##train_fraud_labels_df##\n",
    "train_fraud_labels_df[\"is_fraud\"]=train_fraud_labels_df[\"is_fraud\"].astype(\"category\") \n",
    "train_fraud_labels_df[\"transactions_id\"]=train_fraud_labels_df[\"transactions_id\"].astype(int) #åˆä½µè³‡æ–™éœ€è¦\n",
    "\n",
    "##cards_df##\n",
    "cards_df[\"card_brand\"]=cards_df[\"card_brand\"].astype(\"category\") \n",
    "cards_df[\"card_type\"]=cards_df[\"card_type\"].astype(\"category\")\n",
    "#####ä¸è¦loadé€™è¡Œ cards_df[\"expires\"]=pd.to_datetime(cards_df[\"expires\"], format=\"%m/%Y\")\n",
    "cards_df[\"expires\"] = pd.to_datetime(cards_df[\"expires\"], format=\"%m/%Y\").dt.to_period(\"M\")\n",
    "cards_df[\"has_chip\"]=cards_df[\"has_chip\"].astype(\"category\")\n",
    "\n",
    "cards_df['credit_limit'] = cards_df['credit_limit'].replace(r'[\\$,]', '', regex=True).astype(int)\n",
    "#####ä¸è¦loadé€™è¡Œ cards_df[\"acct_open_date\"]=pd.to_datetime(cards_df[\"acct_open_date\"], format=\"%m/%Y\")\n",
    "cards_df[\"acct_open_date\"] = pd.to_datetime(cards_df[\"acct_open_date\"], format=\"%m/%Y\").dt.to_period(\"M\")\n",
    "#####ä¸è¦loadé€™è¡Œ cards_df[\"year_pin_last_changed\"]=pd.to_datetime(cards_df[\"year_pin_last_changed\"], format=\"%Y\")\n",
    "cards_df[\"year_pin_last_changed\"] = pd.to_datetime(cards_df[\"year_pin_last_changed\"], format=\"%Y\").dt.to_period(\"Y\")\n",
    "cards_df[\"card_on_dark_web\"]=cards_df[\"card_on_dark_web\"].astype(\"category\") \n",
    "\n",
    "##users_df##\n",
    "users_df[\"birth_year\"] = pd.to_datetime(users_df[\"birth_year\"], format=\"%Y\").dt.to_period(\"Y\")\n",
    "users_df[\"birth_month\"] = pd.to_datetime(users_df[\"birth_month\"], format=\"%m\").dt.to_period(\"M\")\n",
    "users_df[\"gender\"]=users_df[\"gender\"].astype(\"category\") \n",
    "users_df['per_capita_income'] = users_df['per_capita_income'].replace(r'[\\$,]', '', regex=True).astype(int)\n",
    "users_df['yearly_income'] = users_df['yearly_income'].replace(r'[\\$,]', '', regex=True).astype(int)\n",
    "users_df['total_debt'] = users_df['total_debt'].replace(r'[\\$,]', '', regex=True).astype(int)\n",
    "\n",
    "##transactions_df##\n",
    "transactions_df[\"date\"] = pd.to_datetime(transactions_df[\"date\"])\n",
    "#æµ®é»æ•¸è½‰æ•´æ•¸åŸå› ç¢ºå®šï¼Ÿ\n",
    "transactions_df['amount'] = transactions_df['amount'].replace(r'[\\$,]', '', regex=True).astype(float).astype(int)\n",
    "##è² æ•¸å–logèª¿æˆ1\n",
    "#transactions_df['amount'] = transactions_df['amount'].replace(r'[\\$,]', '', regex=True).astype(float)\n",
    "\n",
    "transactions_df[\"use_chip\"]=transactions_df[\"use_chip\"].astype(\"category\") \n",
    "\n",
    "transactions_df.loc[\n",
    "    transactions_df['merchant_city'].str.lower() == 'online',\n",
    "    'merchant_state'\n",
    "] = 'online'\n",
    "\n",
    "transactions_df.loc[\n",
    "    transactions_df['merchant_city'].str.lower() == 'online',\n",
    "    'zip'\n",
    "] = 20000 #åŸæœ¬æ˜¯-1\n",
    "## æˆ‘æ²’æœ‰å…¨éƒ¨æ”¹ï¼Œé€™æ¨£å®Œä¹‹å¾Œä»æœ‰89006ç­†Missingï¼Œå‰©ä¸‹éƒ½æ˜¯åœ¨åœ‹å¤–\n",
    "transactions_df['zip'] = transactions_df['zip'].fillna(10000) #åŸæœ¬æ˜¯-999\n",
    "transactions_df[\"zip\"]=transactions_df[\"zip\"].astype(\"int64\")\n",
    "\n",
    "transactions_df['errors'] = transactions_df['errors'].astype('category')\n",
    "transactions_df['errors'] = transactions_df['errors'].cat.add_categories('No_error').fillna('No_error')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cars one hot encoding\n",
    "##çµ±ä¸€é¡åˆ¥è®Šæ•¸è½‰dummy variable(è¦æ³¨æ„å…±ç·šæ€§å•é¡Œï¼Œæ‡‰åˆªæ‰å…¶ä¸­ä¹‹ä¸€)\n",
    "\n",
    "#card_type åŸå§‹ç¨®é¡ï¼šDebit_57%, Credit_33%, Debit(Prepaid)_9%\n",
    "#card_brand åŸå§‹ç¨®é¡ï¼šMasterCard_52%, Visa_38%, Amex_7%, Discovery_3%\n",
    "#has_chip åŸå§‹ç¨®é¡ï¼šYes_89%, No_11%\n",
    "#card_on_dark_web åŸå§‹ç¨®é¡ï¼šNo_0%\n",
    "cols_to_encode = ['card_type', 'card_brand', 'has_chip']\n",
    "cards_df[cols_to_encode] = cards_df[cols_to_encode].astype('category')\n",
    "dummies_cards = pd.get_dummies(\n",
    "    cards_df[cols_to_encode], \n",
    "    prefix=cols_to_encode, \n",
    "    dtype='uint8'\n",
    "    )\n",
    "cards_df = pd.concat([cards_df, dummies_cards], axis=1)\n",
    "\n",
    "#use_chip åŸå§‹ç¨®é¡ï¼šSwiped_52%, Chipe_36%, Online_12%\n",
    "dummies_use = pd.get_dummies(transactions_df['use_chip'], prefix='use_chip', dtype='uint8')\n",
    "transactions_df = pd.concat([transactions_df, dummies_use], axis=1)\n",
    "\n",
    "#gender åŸå§‹ç¨®é¡ï¼šFemale_51%, Male_49%\n",
    "dummies_gender = pd.get_dummies(users_df['gender'], prefix='gender', dtype='uint8')\n",
    "users_df = pd.concat([users_df, dummies_gender], axis=1)\n",
    "\n",
    "\n",
    "cards_df.drop(columns=[\"has_chip_NO\",\"has_chip\"], inplace=True)\n",
    "transactions_df.drop(columns=[\"use_chip\"], inplace=True)\n",
    "users_df.drop(columns=[\"gender_Female\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02_è³‡æ–™æ•´ä½µæˆä¸€å¼µdataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02-1_è³‡æ–™æ•´ä½µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transactions_df.loc[transactions_df[\"transaction_id\"] == 10649266] #transaction_id vs id\n",
    "\n",
    "#åŸå§‹è³‡æ–™ç­†æ•¸ï¼š13305915\n",
    "### transactions_df+train_fraud_labels_df      left æœƒæœ‰4390952 missing values\n",
    "merged = pd.merge(transactions_df, train_fraud_labels_df, left_on=\"transaction_id\", right_on=\"transactions_id\", how=\"outer\")\n",
    "### transactions_df train_fraud_labels_df(8914963) + users_df å°éå»ä¸æœƒæœ‰missing values\n",
    "merged = pd.merge(merged,users_df , left_on=\"client_id\", right_on=\"client_id\", how=\"left\")\n",
    "### transactions_df train_fraud_labels_df users_df + cards_df å°éå»ä¸æœƒæœ‰missing values\n",
    "merged = pd.merge(merged,cards_df , left_on=\"card_id\", right_on=\"card_id\", how=\"left\")\n",
    "\n",
    "#åˆªæ‰é‡è¤‡çš„columns\n",
    "merged.drop(columns=[\"transactions_id\"], inplace=True)\n",
    "merged.drop(columns=[\"client_id_y\"], inplace=True)\n",
    "\n",
    "## åˆä½µå®Œä¹‹å¾Œæœ€å¾Œè™•ç†is_fraud(åŸæœƒæœ‰missing valueså•é¡Œ)\n",
    "merged[\"is_fraud\"] = merged[\"is_fraud\"].astype(str)\n",
    "merged.loc[merged['is_fraud'].str.lower() == 'no','is_fraud'] = '0'\n",
    "merged.loc[merged['is_fraud'].str.lower() == 'yes','is_fraud'] = '1'\n",
    "merged[\"is_fraud\"] = pd.to_numeric(merged[\"is_fraud\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "merged = add_missing_flags(merged, [\"is_fraud\"])\n",
    "\n",
    "#merged.to_csv(\"merged.csv\", index=False)\n",
    "\n",
    "# å…ˆåˆªé™¤ä¸éœ€è¦çš„DataFrameä»¥ç¯€çœè¨˜æ†¶é«”\n",
    "del transactions_df, users_df, cards_df, train_fraud_labels_df, cols_to_encode, dummies_cards, dummies_use, dummies_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_merged = merged.copy()\n",
    "#merged = backup_merged.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04_RFM features engineering model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04-1_è³‡æ–™é€²è¡Œè®Šæ•¸è½‰æ›ä»¥æ±‚æ¨¡å‹é…é£¾æ›´ä½³è¡¨ç¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##æœ‰å‡ºäº‹å†è¶•å¿«å›å¾©åŸç‹€\n",
    "merged = backup_merged.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ç¢ºä¿æ—¥æœŸæ˜¯ datetime ä¸¦æ’åº\n",
    "merged['date'] = pd.to_datetime(merged['date'])\n",
    "merged = merged.sort_values(by=['client_id_x', 'date']).reset_index(drop=True)\n",
    "\n",
    "# --- RecencyInterval ---\n",
    "merged['RecencyInterval'] = merged.groupby('client_id_x')['date'].diff().dt.total_seconds().fillna(0)/60\n",
    "\n",
    "# --- TxnFrequency for multiple windows (å‘é‡åŒ–æ»‘å‹•çª—å£) ---\n",
    "window_days = [7, 30, 60, 90]\n",
    "for w in window_days:\n",
    "    merged[f'TxnFrequency_{w}d'] = 0\n",
    "\n",
    "def compute_freq_vectorized(dates, windows):\n",
    "    \"\"\"å‘é‡åŒ–è¨ˆç®—æ¯ç­†äº¤æ˜“åœ¨æ¯å€‹ window å…§çš„äº¤æ˜“æ•¸\"\"\"\n",
    "    n = len(dates)\n",
    "    dates_int = dates.values.astype('datetime64[D]').astype(int)\n",
    "    res = {w: np.zeros(n, dtype=int) for w in windows}\n",
    "    for w in windows:\n",
    "        left = 0\n",
    "        counts = np.zeros(n, dtype=int)\n",
    "        for right in range(n):\n",
    "            while dates_int[right] - dates_int[left] > w:\n",
    "                left += 1\n",
    "            counts[right] = right - left + 1\n",
    "        res[w] = counts\n",
    "    return res\n",
    "\n",
    "# åˆ†çµ„è¨ˆç®—\n",
    "for cid, g in merged.groupby('client_id_x', sort=False):\n",
    "    freq_dict = compute_freq_vectorized(g['date'], window_days)\n",
    "    for w in window_days:\n",
    "        merged.loc[g.index, f'TxnFrequency_{w}d'] = freq_dict[w]\n",
    "\n",
    "# --- AmtDelta ---\n",
    "merged['prev_amount'] = merged.groupby('client_id_x')['amount'].shift(1)\n",
    "merged['AmtDelta'] = merged['amount'] - merged['prev_amount']\n",
    "merged['AmtDelta'] = merged['AmtDelta'].fillna(0)\n",
    "merged.drop(columns='prev_amount', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# US region mapping\n",
    "us_region_map = {\n",
    "    'Northeast': ['NY','NJ','PA','MA','CT','RI','NH','VT','ME'],\n",
    "    'Midwest': ['IL','OH','MI','IN','WI','MN','IA','MO','ND','SD','NE','KS'],\n",
    "    'South': ['FL','GA','SC','NC','AL','MS','LA','TX','OK','TN','KY','VA','WV','AR','MD','DE','DC'],\n",
    "    'West': ['CA','WA','OR','NV','AZ','NM','CO','UT','ID','MT','WY','AK','HI'],\n",
    "}\n",
    "continent_map = {\n",
    "    'Europe': [ ... ],  # åŸæœ¬ continent_map['Europe'] å¯ç›´æ¥ä½¿ç”¨\n",
    "    'Online': ['online','AA']\n",
    "}\n",
    "\n",
    "us_region_lookup = {state: region for region, states in us_region_map.items() for state in states}\n",
    "\n",
    "# --- å‘é‡åŒ– location ç‰¹å¾µ ---\n",
    "merged['merchant_online'] = merged['merchant_state'].eq('online').astype('uint8')\n",
    "merged['merchant_us'] = merged['merchant_state'].isin(us_region_lookup.keys()).astype('uint8')\n",
    "merged['merchant_eu'] = merged['merchant_state'].isin(continent_map['Europe']).astype('uint8')\n",
    "merged['merchant_others'] = (~merged[['merchant_online','merchant_us','merchant_eu']].any(axis=1)).astype('uint8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- é¦–æ¬¡äº¤æ˜“æ¨™è¨˜ ---\n",
    "merged['FirstTxnInRegion'] = (~merged.duplicated(subset=['client_id_x', 'merchant_state'])).astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DifferentState\n",
    "merged['prev_state']=(merged\n",
    "                     .groupby('client_id_x')['merchant_state']\n",
    "                     .shift(1))\n",
    "\n",
    "merged['DifferentState'] = (\n",
    "    (merged['merchant_state'] != merged['prev_state'])\n",
    "    & merged['prev_state'].notna()\n",
    ").astype(int)\n",
    "\n",
    "merged = merged.drop(columns=['prev_state'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged[[\"card_id\",\"card_number\"]]\n",
    "import numpy as np\n",
    "from scipy import stats \n",
    "\n",
    "# === (1) logè½‰æ› ===\n",
    "merged['amount'] = np.where(merged['amount'] < 0, 0, merged['amount'])  # è² æ•¸è®Š 0\n",
    "merged['amount'] = np.log(merged['amount'] + 1)  \n",
    "\n",
    "# === (3) å¹³æ–¹æ ¹è½‰æ› ===\n",
    "merged['credit_limit']=np.sqrt(merged['credit_limit'])\n",
    "merged['total_debt']=np.sqrt(merged['total_debt'])\n",
    "\n",
    "# === (3) ç«‹æ–¹æ ¹è½‰æ› ===\n",
    "merged['yearly_income']=np.cbrt(merged['yearly_income'])\n",
    "merged['per_capita_income']=np.cbrt(merged['per_capita_income'])\n",
    "\n",
    "## Box-Cox Transformation\n",
    "###merged['yearly_income'], fitted_lambda = stats.boxcox(merged['yearly_income'])\n",
    "\n",
    "# === (5) Yeoâ€“Johnson è½‰æ›ï¼ˆå¯è™•ç†è² å€¼ï¼‰ ===\n",
    "###merged['per_capita_income'], lambdaValue =stats.yeojohnson(merged['per_capita_income'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04-2_åˆ†å‰²è¨“ç·´é›†åŠæ¸¬è©¦é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Block 0\n",
      "\n",
      "Processing Block 1\n",
      "\n",
      "Processing Block 2\n",
      "\n",
      "Processing Block 3\n",
      "\n",
      "Processing Block 4\n",
      "\n",
      "Block 0\n",
      "Train fraud count:\n",
      "is_fraud\n",
      "0    1353355\n",
      "1       2610\n",
      "Name: count, dtype: Int64\n",
      "Test fraud count:\n",
      "is_fraud\n",
      "0    338992\n",
      "Name: count, dtype: Int64\n",
      "\n",
      "Block 1\n",
      "Train fraud count:\n",
      "is_fraud\n",
      "0    1433028\n",
      "1       1152\n",
      "Name: count, dtype: Int64\n",
      "Test fraud count:\n",
      "is_fraud\n",
      "0    357437\n",
      "1      1108\n",
      "Name: count, dtype: Int64\n",
      "\n",
      "Block 2\n",
      "Train fraud count:\n",
      "is_fraud\n",
      "0    1474582\n",
      "1       1655\n",
      "Name: count, dtype: Int64\n",
      "Test fraud count:\n",
      "is_fraud\n",
      "0    367862\n",
      "1      1198\n",
      "Name: count, dtype: Int64\n",
      "\n",
      "Block 3\n",
      "Train fraud count:\n",
      "is_fraud\n",
      "0    1493588\n",
      "1       2448\n",
      "Name: count, dtype: Int64\n",
      "Test fraud count:\n",
      "is_fraud\n",
      "0    373838\n",
      "1       172\n",
      "Name: count, dtype: Int64\n",
      "\n",
      "Block 4\n",
      "Train fraud count:\n",
      "is_fraud\n",
      "0    1367157\n",
      "1       2393\n",
      "Name: count, dtype: Int64\n",
      "Test fraud count:\n",
      "is_fraud\n",
      "0    341792\n",
      "1       596\n",
      "Name: count, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "# --- é¸å–æ•¸å€¼å‹è®Šæ•¸ ---\n",
    "num_cols = merged.select_dtypes(include=['int64', 'float64','uint8','datetime64[ns]']).columns\n",
    "df2 = merged[num_cols]\n",
    "\n",
    "# --- dropna ---\n",
    "df_cleaned = df2.dropna()\n",
    "del df2\n",
    "\n",
    "# --- é¿å…å…±ç·šæ€§ ---\n",
    "df_cleaned.drop(columns=[\"is_fraud_missing_flag\",\"card_type_Debit (Prepaid)\", \n",
    "                         \"card_brand_Discover\", \"use_chip_Online Transaction\"], inplace=True)\n",
    "\n",
    "# --- ç¢ºä¿ date æ¬„ä½åœ¨ df_cleaned ä¸­ ---\n",
    "if 'date' not in df_cleaned.columns:\n",
    "    df_cleaned['date'] = merged.loc[df_cleaned.index, 'date']\n",
    "\n",
    "# --- ä¾æ™‚é–“æ’åº ---\n",
    "df_sorted = df_cleaned.sort_values('date')\n",
    "df_sorted['year'] = df_sorted['date'].dt.year\n",
    "\n",
    "# 2010â€“2011 â†’ block 0\n",
    "# 2012â€“2013 â†’ block 1\n",
    "# ...\n",
    "# 2018â€“2019 â†’ block 4ï¼ˆå¦‚æœä½ çœŸçš„æ˜¯ 2010â€“2019 å…± 10 å¹´ï¼Œæœƒæœ‰ 5 å€‹ blockï¼‰\n",
    "df_sorted['time_block'] = (df_sorted['year'] - 2010) // 2\n",
    "\n",
    "\n",
    "#å°æ¯å€‹æ™‚é–“blockåšåˆ‡å‰²\n",
    "train_list = []\n",
    "test_list = []\n",
    "\n",
    "for block_id, block_df in df_sorted.groupby('time_block'):\n",
    "    print(f\"\\nProcessing Block {block_id}\")\n",
    "\n",
    "    block_df = block_df.sort_values('date')\n",
    "    split_index = int(len(block_df) * 0.8)\n",
    "\n",
    "    train_block = block_df.iloc[:split_index].copy()\n",
    "    test_block  = block_df.iloc[split_index:].copy()\n",
    "\n",
    "    # -----------------------------\n",
    "    # 1ï¸âƒ£ ç”¨ã€Œè©² block çš„ trainã€ç®— fraud rate\n",
    "    # -----------------------------\n",
    "    fraud_rate = (\n",
    "        train_block\n",
    "        .groupby('mcc_code')['is_fraud']\n",
    "        .mean()\n",
    "    )\n",
    "\n",
    "    high_risk_mcc = fraud_rate[fraud_rate > 0.02].index\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2ï¸âƒ£ å¥—ç”¨åˆ°è©² block çš„ train / test\n",
    "    # -----------------------------\n",
    "    train_block['HighRiskMCC'] = train_block['mcc_code'].isin(high_risk_mcc).astype('uint8')\n",
    "    test_block['HighRiskMCC']  = test_block['mcc_code'].isin(high_risk_mcc).astype('uint8')\n",
    "\n",
    "    train_list.append(train_block)\n",
    "    test_list.append(test_block)\n",
    "\n",
    "\n",
    "train_df = pd.concat(train_list).drop(columns=['date', 'year', 'time_block'])\n",
    "test_df  = pd.concat(test_list).drop(columns=['date', 'year', 'time_block'])\n",
    "\n",
    "\n",
    "#æ¯”ä¾‹æª¢æŸ¥\n",
    "for block_id in sorted(df_sorted['time_block'].unique()):\n",
    "    print(f\"\\nBlock {block_id}\")\n",
    "    print(\"Train fraud count:\")\n",
    "    print(train_df.loc[df_sorted['time_block'] == block_id, 'is_fraud'].value_counts())\n",
    "    print(\"Test fraud count:\")\n",
    "    print(test_df.loc[df_sorted['time_block'] == block_id, 'is_fraud'].value_counts())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "result = pd.DataFrame(columns=[\n",
    "    \"Model\", \"Features\", \n",
    "    \"Train AUC\", \"Test AUC\", \n",
    "    \"Train PR AUC\", \"Test PR AUC\"\n",
    "])\n",
    "\n",
    "'''\n",
    "\n",
    "# ALL features\n",
    "\n",
    "all_cols = ['transaction_id', 'date', 'client_id_x', 'card_id', 'amount',\n",
    "       'merchant_id', 'merchant_city', 'merchant_state', 'zip', 'mcc_code',\n",
    "       'errors', 'merchant_state_missing_flag', 'zip_missing_flag',\n",
    "       'errors_missing_flag', 'use_chip_Chip Transaction',\n",
    "       'use_chip_Online Transaction', 'use_chip_Swipe Transaction',\n",
    "       'current_age', 'retirement_age', 'birth_year', 'birth_month', 'gender',\n",
    "       'address', 'latitude', 'longitude', 'per_capita_income',\n",
    "       'yearly_income', 'total_debt', 'credit_score', 'num_credit_cards',\n",
    "       'gender_Male', 'card_brand', 'card_type', 'card_number', 'expires',\n",
    "       'cvv', 'num_cards_issued', 'credit_limit', 'acct_open_date',\n",
    "       'year_pin_last_changed', 'card_on_dark_web', 'card_type_Credit',\n",
    "       'card_type_Debit', 'card_type_Debit (Prepaid)', 'card_brand_Amex',\n",
    "       'card_brand_Discover', 'card_brand_Mastercard', 'card_brand_Visa',\n",
    "       'has_chip_YES', 'is_fraud_missing_flag']\n",
    "\n",
    "VIF_col = [\"is_fraud_missing_flag\",\"card_type_Debit (Prepaid)\", \n",
    "                         \"card_brand_Discover\", \"use_chip_Online Transaction\",'is_fraud_missing_flag','merchant_state_missing_flag', 'zip_missing_flag','card_on_dark_web']\n",
    "\n",
    "identifier = [\n",
    "    'transaction_id',\n",
    "    'client_id_x',\n",
    "    'card_id',\n",
    "    'card_number',\n",
    "    'cvv'\n",
    "]\n",
    "\n",
    "set_VIF = set(VIF_col)\n",
    "set_identifier =set(identifier)\n",
    "exclude_cols = set(VIF_col) | set(identifier)\n",
    "all_cols = [x for x in all_cols if x not in exclude_cols]\n",
    "\n",
    "\n",
    "# RFM features\n",
    "rfm_cols = [\n",
    "    'RecencyInterval', 'TxnFrequency_7d','TxnFrequency_30d',\n",
    "    'TxnFrequency_60d', 'TxnFrequency_90d','AmtDelta'\n",
    "]\n",
    "\n",
    "# DK features\n",
    "dk_cols = [\n",
    "    'merchant_online', 'merchant_us', 'merchant_eu', 'merchant_others',\n",
    "    'FirstTxnInRegion','HighRiskMCC','DifferentState'\n",
    "]\n",
    "\n",
    "# Grouping\n",
    "feature_groups = {\n",
    "    \"X_all\": all_cols,\n",
    "    \"X_rfm\": rfm_cols,\n",
    "    \"X_dk\": dk_cols,\n",
    "    \"X_all + X_rfm\": all_cols + rfm_cols,\n",
    "    \"X_all + X_dk\": all_cols + dk_cols,\n",
    "    \"X_rfm + X_dk\": rfm_cols + dk_cols,\n",
    "    \"X_all + X_rfm + X_dk\": all_cols + rfm_cols + dk_cols\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04-3 è€å¸«å»ºè­°æˆ‘å€‘å…ˆçœç•¥ Assumption: Avoid Multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##è™•ç†å…±ç·šæ€§\n",
    "train_df.drop(columns=[\"per_capita_income\"], inplace=True)\n",
    "train_df.drop(columns=[\"use_chip_Chip Transaction\",\"merchant_state_missing_flag\",\"zip_missing_flag\"], inplace=True)           \n",
    "train_df.drop(columns=[\"card_brand_Visa\" ,\"card_brand_Amex\",\"card_type_Credit\"], inplace=True)\n",
    "#å†é‡è·‘ä¸€æ¬¡VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.drop(columns=[\"per_capita_income\"], inplace=True)\n",
    "test_df.drop(columns=[\"use_chip_Chip Transaction\",\"merchant_state_missing_flag\",\"zip_missing_flag\"], inplace=True)           \n",
    "test_df.drop(columns=[\"card_brand_Visa\" ,\"card_brand_Amex\",\"card_type_Credit\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04-stepwise selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "ğŸš€ Running Block 0 (Year: 2010 - 2011)\n",
      "========================================\n",
      "\n",
      "ğŸ”¹ Group: X_all\n",
      "      âš ï¸ Dropping constant cols: ['use_chip_Chip Transaction']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "âš ï¸ ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŸ¢ Step 1: Forward add zip (p=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "âš ï¸ ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŸ¢ Step 2: Forward add longitude (p=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "âš ï¸ RuntimeWarning: overflow encountered in exp\n",
      "âš ï¸ RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŸ¢ Step 3: Forward add merchant_state_missing_flag (p=0)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 280\u001b[39m\n\u001b[32m    278\u001b[39m \u001b[38;5;66;03m# åŸ·è¡Œ Stepwise\u001b[39;00m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m     overall_fit, coef_df, not_in_eq_df, final_model = \u001b[43mstepwise_logit_with_k_tables\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdep_var\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mis_fraud\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m314657018\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# âœ… ç¢ºä¿é€™è£¡é–‹å•Ÿé¡¯ç¤º\u001b[39;49;00m\n\u001b[32m    286\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    288\u001b[39m     train_table = classification_table(final_model, train_df)\n\u001b[32m    289\u001b[39m     test_table  = classification_table(final_model, test_df)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 74\u001b[39m, in \u001b[36mstepwise_logit_with_k_tables\u001b[39m\u001b[34m(train_df, test_df, dep_var, k, threshold_in, threshold_out, verbose)\u001b[39m\n\u001b[32m     72\u001b[39m     X_try = sm.add_constant(X_train[included + [new_var]], has_constant=\u001b[33m\"\u001b[39m\u001b[33madd\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     73\u001b[39m     \u001b[38;5;66;03m# é€™è£¡ä¸ä½¿ç”¨ catch_warningsï¼Œè®“ç¸®çŸ­å¾Œçš„è­¦å‘Šé¡¯ç¤ºå‡ºä¾†\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     model = \u001b[43msm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLogit\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_try\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdisp\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     new_pvals[new_var] = model.pvalues[new_var]\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/discrete/discrete_model.py:2601\u001b[39m, in \u001b[36mLogit.fit\u001b[39m\u001b[34m(self, start_params, method, maxiter, full_output, disp, callback, **kwargs)\u001b[39m\n\u001b[32m   2598\u001b[39m \u001b[38;5;129m@Appender\u001b[39m(DiscreteModel.fit.\u001b[34m__doc__\u001b[39m)\n\u001b[32m   2599\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, start_params=\u001b[38;5;28;01mNone\u001b[39;00m, method=\u001b[33m'\u001b[39m\u001b[33mnewton\u001b[39m\u001b[33m'\u001b[39m, maxiter=\u001b[32m35\u001b[39m,\n\u001b[32m   2600\u001b[39m         full_output=\u001b[32m1\u001b[39m, disp=\u001b[32m1\u001b[39m, callback=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m-> \u001b[39m\u001b[32m2601\u001b[39m     bnryfit = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2602\u001b[39m \u001b[43m                          \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2603\u001b[39m \u001b[43m                          \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2604\u001b[39m \u001b[43m                          \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2605\u001b[39m \u001b[43m                          \u001b[49m\u001b[43mdisp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2606\u001b[39m \u001b[43m                          \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2607\u001b[39m \u001b[43m                          \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2609\u001b[39m     discretefit = LogitResults(\u001b[38;5;28mself\u001b[39m, bnryfit)\n\u001b[32m   2610\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m BinaryResultsWrapper(discretefit)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/discrete/discrete_model.py:243\u001b[39m, in \u001b[36mDiscreteModel.fit\u001b[39m\u001b[34m(self, start_params, method, maxiter, full_output, disp, callback, **kwargs)\u001b[39m\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# TODO: make a function factory to have multiple call-backs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m mlefit = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mdisp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m                     \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m mlefit\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/base/model.py:566\u001b[39m, in \u001b[36mLikelihoodModel.fit\u001b[39m\u001b[34m(self, start_params, method, maxiter, full_output, disp, fargs, callback, retall, skip_hessian, **kwargs)\u001b[39m\n\u001b[32m    563\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_t\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    565\u001b[39m optimizer = Optimizer()\n\u001b[32m--> \u001b[39m\u001b[32m566\u001b[39m xopt, retvals, optim_settings = \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[43m                                               \u001b[49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[43m                                               \u001b[49m\u001b[43mhessian\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[43m                                               \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[43m                                               \u001b[49m\u001b[43mdisp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    571\u001b[39m \u001b[43m                                               \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m                                               \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m                                               \u001b[49m\u001b[43mretall\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretall\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[43m                                               \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[38;5;66;03m# Restore cov_type, cov_kwds and use_t\u001b[39;00m\n\u001b[32m    576\u001b[39m optim_settings.update(kwds)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/base/optimizer.py:245\u001b[39m, in \u001b[36mOptimizer._fit\u001b[39m\u001b[34m(self, objective, gradient, start_params, fargs, kwargs, hessian, method, maxiter, full_output, disp, callback, retall)\u001b[39m\n\u001b[32m    242\u001b[39m     fit_funcs.update(extra_fit_funcs)\n\u001b[32m    244\u001b[39m func = fit_funcs[method]\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m xopt, retvals = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mdisp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mretall\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mhess\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhessian\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    250\u001b[39m optim_settings = {\u001b[33m'\u001b[39m\u001b[33moptimizer\u001b[39m\u001b[33m'\u001b[39m: method, \u001b[33m'\u001b[39m\u001b[33mstart_params\u001b[39m\u001b[33m'\u001b[39m: start_params,\n\u001b[32m    251\u001b[39m                   \u001b[33m'\u001b[39m\u001b[33mmaxiter\u001b[39m\u001b[33m'\u001b[39m: maxiter, \u001b[33m'\u001b[39m\u001b[33mfull_output\u001b[39m\u001b[33m'\u001b[39m: full_output,\n\u001b[32m    252\u001b[39m                   \u001b[33m'\u001b[39m\u001b[33mdisp\u001b[39m\u001b[33m'\u001b[39m: disp, \u001b[33m'\u001b[39m\u001b[33mfargs\u001b[39m\u001b[33m'\u001b[39m: fargs, \u001b[33m'\u001b[39m\u001b[33mcallback\u001b[39m\u001b[33m'\u001b[39m: callback,\n\u001b[32m    253\u001b[39m                   \u001b[33m'\u001b[39m\u001b[33mretall\u001b[39m\u001b[33m'\u001b[39m: retall, \u001b[33m\"\u001b[39m\u001b[33mextra_fit_funcs\u001b[39m\u001b[33m\"\u001b[39m: extra_fit_funcs}\n\u001b[32m    254\u001b[39m optim_settings.update(kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/base/optimizer.py:444\u001b[39m, in \u001b[36m_fit_newton\u001b[39m\u001b[34m(f, score, start_params, fargs, kwargs, disp, maxiter, callback, retall, full_output, hess, ridge_factor)\u001b[39m\n\u001b[32m    441\u001b[39m     history = [oldparams, newparams]\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m (iterations < maxiter \u001b[38;5;129;01mand\u001b[39;00m np.any(np.abs(newparams -\n\u001b[32m    443\u001b[39m                                               oldparams) > tol)):\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m     H = np.asarray(\u001b[43mhess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnewparams\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    445\u001b[39m     \u001b[38;5;66;03m# regularize Hessian, not clear what ridge factor should be\u001b[39;00m\n\u001b[32m    446\u001b[39m     \u001b[38;5;66;03m# keyword option with absolute default 1e-10, see #1847\u001b[39;00m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.all(ridge_factor == \u001b[32m0\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/base/model.py:542\u001b[39m, in \u001b[36mLikelihoodModel.fit.<locals>.hess\u001b[39m\u001b[34m(params, *args)\u001b[39m\n\u001b[32m    541\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhess\u001b[39m(params, *args):\n\u001b[32m--> \u001b[39m\u001b[32m542\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhessian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m / nobs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/discrete/discrete_model.py:2578\u001b[39m, in \u001b[36mLogit.hessian\u001b[39m\u001b[34m(self, params)\u001b[39m\n\u001b[32m   2576\u001b[39m X = \u001b[38;5;28mself\u001b[39m.exog\n\u001b[32m   2577\u001b[39m L = \u001b[38;5;28mself\u001b[39m.predict(params)\n\u001b[32m-> \u001b[39m\u001b[32m2578\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m -\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mL\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m-\u001b[49m\u001b[43mL\u001b[49m\u001b[43m)\u001b[49m\u001b[43m*\u001b[49m\u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, confusion_matrix\n",
    "import warnings\n",
    "import copy\n",
    "\n",
    "# ===========================================================\n",
    "# 1. è¨­å®šè­¦å‘Šæ ¼å¼ (åªé¡¯ç¤ºé‡é»ï¼Œä¸é¡¯ç¤ºè·¯å¾‘)\n",
    "# ===========================================================\n",
    "def short_warning(message, category, filename, lineno, file=None, line=None):\n",
    "    return f\"âš ï¸ {category.__name__}: {message}\\n\"\n",
    "\n",
    "warnings.formatwarning = short_warning\n",
    "# è‹¥è¦ºå¾—å¤ªåµï¼Œå¯ä»¥å–æ¶ˆè¨»è§£ä¸‹é¢é€™è¡Œï¼Œè®“åŒé¡å‹è­¦å‘Šåªé¡¯ç¤ºä¸€æ¬¡\n",
    "# warnings.simplefilter('once', category=RuntimeWarning)\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "# 2. Stepwise Logistic Regression (ä¿®æ­£ç‰ˆ)\n",
    "# ===========================================================\n",
    "def stepwise_logit_with_k_tables(\n",
    "    train_df,\n",
    "    test_df,\n",
    "    dep_var=\"is_fraud\",\n",
    "    k=314657018,\n",
    "    threshold_in=0.05,\n",
    "    threshold_out=0.10,\n",
    "    verbose=True\n",
    "):\n",
    "    y_train = train_df[dep_var]\n",
    "    X_train = train_df.drop(columns=[dep_var])\n",
    "    y_test = test_df[dep_var]\n",
    "    X_test = test_df.drop(columns=[dep_var])\n",
    "\n",
    "    included = []\n",
    "    step = 0\n",
    "    full_mode = (k == 314657018)\n",
    "\n",
    "    # --- K=0 (Intercept Only) ---\n",
    "    if k == 0:\n",
    "        X_const = sm.add_constant(np.ones(len(y_train)), has_constant=\"add\")\n",
    "        final_model = sm.Logit(y_train, X_const).fit(disp=False)\n",
    "        \n",
    "        # å»ºç«‹å›å‚³æ ¼å¼\n",
    "        ll_full = final_model.llf\n",
    "        overall_fit = pd.DataFrame({\n",
    "            \"Measure\": [\"-2 Log Likelihood\"], \"Value\": [round(-2 * ll_full, 3)]\n",
    "        })\n",
    "        coef_df = pd.DataFrame({\n",
    "            \"Independent Variable\": [\"const\"],\n",
    "            \"B\": final_model.params.values,\n",
    "            \"Exp(B)\": np.exp(final_model.params.values)\n",
    "        })\n",
    "        not_in_eq_df = pd.DataFrame() # ç©ºçš„\n",
    "        \n",
    "        if verbose: print(\"âœ… Stepwise completed (Intercept only).\")\n",
    "        return overall_fit, coef_df, not_in_eq_df, final_model\n",
    "\n",
    "    # --- Regular Stepwise ---\n",
    "    while True:\n",
    "        step += 1\n",
    "        changed = False\n",
    "\n",
    "        # Forward\n",
    "        excluded = list(set(X_train.columns) - set(included))\n",
    "        new_pvals = pd.Series(index=excluded, dtype=float)\n",
    "\n",
    "        for new_var in excluded:\n",
    "            try:\n",
    "                X_try = sm.add_constant(X_train[included + [new_var]], has_constant=\"add\")\n",
    "                # é€™è£¡ä¸ä½¿ç”¨ catch_warningsï¼Œè®“ç¸®çŸ­å¾Œçš„è­¦å‘Šé¡¯ç¤ºå‡ºä¾†\n",
    "                model = sm.Logit(y_train, X_try).fit(disp=False)\n",
    "                new_pvals[new_var] = model.pvalues[new_var]\n",
    "            except Exception:\n",
    "                new_pvals[new_var] = np.nan\n",
    "\n",
    "        if new_pvals.empty:\n",
    "            break\n",
    "\n",
    "        best_pval = new_pvals.min()\n",
    "        if best_pval < threshold_in:\n",
    "            best_var = new_pvals.idxmin()\n",
    "            included.append(best_var)\n",
    "            changed = True\n",
    "            if verbose:\n",
    "                print(f\"ğŸŸ¢ Step {step}: Forward add {best_var} (p={best_pval:.4g})\")\n",
    "\n",
    "        # Backward\n",
    "        if included:\n",
    "            X_curr = sm.add_constant(X_train[included], has_constant=\"add\")\n",
    "            model = sm.Logit(y_train, X_curr).fit(disp=False)\n",
    "            pvalues = model.pvalues.drop(\"const\", errors=\"ignore\")\n",
    "\n",
    "            if not pvalues.empty:\n",
    "                worst_pval = pvalues.max()\n",
    "                if worst_pval > threshold_out:\n",
    "                    worst_var = pvalues.idxmax()\n",
    "                    included.remove(worst_var)\n",
    "                    changed = True\n",
    "                    if verbose:\n",
    "                        print(f\"ğŸ”´ Step {step}: Backward remove {worst_var} (p={worst_pval:.4g})\")\n",
    "\n",
    "        if not changed:\n",
    "            if verbose: print(f\"âšª Step {step}: No change â€” stop.\")\n",
    "            break\n",
    "\n",
    "        if not full_mode and len(included) >= k:\n",
    "            if verbose: print(f\"ğŸŸ¡ Reached k={k}, stop.\")\n",
    "            break\n",
    "\n",
    "    # --- Final Model ---\n",
    "    if len(included) == 0:\n",
    "        X_final = sm.add_constant(np.ones(len(y_train)), has_constant=\"add\")\n",
    "    else:\n",
    "        X_final = sm.add_constant(X_train[included], has_constant=\"add\")\n",
    "\n",
    "    try:\n",
    "        final_model = sm.Logit(y_train, X_final).fit(disp=False)\n",
    "    except ValueError:\n",
    "        if verbose: print(\"âš ï¸ Final model failed. Fallback to intercept-only.\")\n",
    "        X_const = sm.add_constant(np.ones(len(y_train)), has_constant=\"add\")\n",
    "        final_model = sm.Logit(y_train, X_const).fit(disp=False)\n",
    "        included = []\n",
    "\n",
    "    # --- Report Generation (Simplified for brevity) ---\n",
    "    ll_full = final_model.llf\n",
    "    ll_null = sm.Logit(y_train, sm.add_constant(np.ones(len(y_train)), has_constant=\"add\")).fit(disp=False).llf\n",
    "    \n",
    "    overall_fit = pd.DataFrame({\n",
    "        \"Measure\": [\"-2 Log Likelihood\", \"Pseudo R2\"],\n",
    "        \"Value\": [round(-2 * ll_full, 3), round(1 - (ll_full / ll_null), 3)]\n",
    "    })\n",
    "\n",
    "    coef_df = pd.DataFrame({\n",
    "        \"Independent Variable\": final_model.params.index,\n",
    "        \"B\": final_model.params.values,\n",
    "        \"Std. Error\": final_model.bse.values,\n",
    "        \"Sig.\": final_model.pvalues.values,\n",
    "        \"Exp(B)\": np.exp(final_model.params.values)\n",
    "    }).reset_index(drop=True)\n",
    "\n",
    "    # ç°¡å–®å»ºç«‹ Not in Eq (çœç•¥ LRT è¨ˆç®—ä»¥åŠ é€Ÿ)\n",
    "    not_in_eq_df = pd.DataFrame({\"Independent Variable\": list(set(X_train.columns) - set(included))})\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nâœ… Stepwise completed with {len(included)} variables\")\n",
    "\n",
    "    return overall_fit, coef_df, not_in_eq_df, final_model\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "# 3. Classification Table\n",
    "# ===========================================================\n",
    "def classification_table(model, df, target_col=\"is_fraud\", cutoff=0.0015):\n",
    "    y_true = df[target_col].astype(int)\n",
    "    features = model.params.index.drop(\"const\", errors=\"ignore\")\n",
    "    \n",
    "    if len(features) == 0:\n",
    "        X = sm.add_constant(np.ones(len(df)), has_constant=\"add\")\n",
    "    else:\n",
    "        X = sm.add_constant(df[features], has_constant=\"add\")\n",
    "\n",
    "    y_pred_prob = model.predict(X)\n",
    "    y_pred = (y_pred_prob >= cutoff).astype(int)\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[1, 0])\n",
    "    TP, FN, FP, TN = cm.ravel()\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        \"Type\": [\"Fraud(1)\", \"Normal(0)\", \"Total\"],\n",
    "        \"Correct\": [TP, TN, TP+TN],\n",
    "        \"Total\": [TP+FN, FP+TN, len(df)],\n",
    "        \"Accuracy (%)\": [\n",
    "            round(TP/(TP+FN)*100, 1) if (TP+FN)>0 else 0,\n",
    "            round(TN/(FP+TN)*100, 1) if (FP+TN)>0 else 0,\n",
    "            round((TP+TN)/len(df)*100, 1)\n",
    "        ]\n",
    "    })\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "# 4. Feature Selector (é˜²å‘†ä¿®æ­£ç‰ˆ)\n",
    "# ===========================================================\n",
    "def select_features(df, feature_list, dep_var=\"is_fraud\"):\n",
    "    # 1. ç¢ºä¿ dep_var ä¸åœ¨ç‰¹å¾µåˆ—è¡¨ä¸­\n",
    "    features_to_use = [f for f in feature_list if f != dep_var and f in df.columns]\n",
    "    \n",
    "    # 2. é¸å– ç‰¹å¾µ + ç›®æ¨™\n",
    "    final_cols = features_to_use + [dep_var]\n",
    "    \n",
    "    # 3. å»ºç«‹å­é›†\n",
    "    df_sub = df[final_cols].copy()\n",
    "    \n",
    "    # 4. è½‰æ•¸å€¼\n",
    "    df_sub = df_sub.select_dtypes(include=[np.number])\n",
    "    df_sub = df_sub.astype(float)\n",
    "\n",
    "    return df_sub\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "# 5. å®šç¾©ç‰¹å¾µç¾¤çµ„ (Feature Groups)\n",
    "# ===========================================================\n",
    "# å‡è¨­ all_cols, rfm_cols, dk_cols å·²ç¶“åœ¨ä¹‹å‰çš„æ­¥é©Ÿå®šç¾©å¥½äº†\n",
    "# é€™è£¡åšæœ€å¾Œçš„é˜²å‘†æ¸…æ´—\n",
    "\n",
    "identifier = ['transaction_id', 'client_id_x', 'card_id', 'card_number', 'cvv']\n",
    "# ç¢ºä¿ all_cols è£¡é¢æ²’æœ‰ ID å’Œ ç›®æ¨™è®Šæ•¸\n",
    "all_cols = [x for x in all_cols if x not in identifier and x != 'is_fraud']\n",
    "\n",
    "feature_groups = {\n",
    "    \"X_all\": all_cols,\n",
    "    \"X_rfm\": rfm_cols,\n",
    "    \"X_dk\": dk_cols,\n",
    "    \"X_all + X_rfm\": all_cols + rfm_cols,\n",
    "    \"X_all + X_dk\": all_cols + dk_cols,\n",
    "    \"X_rfm + X_dk\": rfm_cols + dk_cols,\n",
    "    \"X_all + X_rfm + X_dk\": all_cols + rfm_cols + dk_cols\n",
    "}\n",
    "\n",
    "# å‚™ä»½ä¸€ä»½ï¼Œä»¥å…ä¿®æ”¹åˆ°åŸå§‹è¨­å®š\n",
    "current_feature_groups = copy.deepcopy(feature_groups)\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "# 6. ä¸»ç¨‹å¼è¿´åœˆ (Main Loop)\n",
    "# ===========================================================\n",
    "results = {}\n",
    "\n",
    "for block_id, block_df in df_sorted.groupby('time_block'):\n",
    "\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"ğŸš€ Running Block {block_id} (Year: {2010 + block_id*2} - {2011 + block_id*2})\")\n",
    "    print(f\"{'='*40}\")\n",
    "\n",
    "    # 1. æ™‚é–“æ’åºèˆ‡åˆ‡åˆ†\n",
    "    block_df = block_df.sort_values('date')\n",
    "    split_index = int(len(block_df) * 0.8)\n",
    "    \n",
    "    train_raw = block_df.iloc[:split_index].copy()\n",
    "    test_raw  = block_df.iloc[split_index:].copy()\n",
    "\n",
    "    # 2. è¨ˆç®— HighRiskMCC (é˜²æ­¢ Data Leakage)\n",
    "    fraud_rate = train_raw.groupby('mcc_code')['is_fraud'].mean()\n",
    "    high_risk_mcc = fraud_rate[fraud_rate > 0.02].index\n",
    "    \n",
    "    train_raw['HighRiskMCC'] = train_raw['mcc_code'].isin(high_risk_mcc).astype('uint8')\n",
    "    test_raw['HighRiskMCC']  = test_raw['mcc_code'].isin(high_risk_mcc).astype('uint8')\n",
    "\n",
    "    # åˆå§‹åŒ–çµæœå­˜æ”¾\n",
    "    results[block_id] = {}\n",
    "\n",
    "    # 3. åŸ·è¡Œ Feature Groups\n",
    "    for group_name, feature_list in current_feature_groups.items():\n",
    "\n",
    "        print(f\"\\nğŸ”¹ Group: {group_name}\")\n",
    "\n",
    "        # æª¢æŸ¥æ¬„ä½æ˜¯å¦å­˜åœ¨\n",
    "        valid_features = [f for f in feature_list if f in train_raw.columns]\n",
    "        if not valid_features:\n",
    "            print(\"      âŒ No valid features found. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # é¸å–ç‰¹å¾µ (å·²å«é˜²å‘†)\n",
    "        train_df = select_features(train_raw, valid_features, dep_var=\"is_fraud\")\n",
    "        test_df  = select_features(test_raw, valid_features, dep_var=\"is_fraud\")\n",
    "        \n",
    "        # æ’é™¤å¸¸æ•¸æ¬„ä½ (é¿å… LinAlgError)\n",
    "        std_check = train_df.std()\n",
    "        constant_cols = std_check[std_check == 0].index.tolist()\n",
    "        if constant_cols:\n",
    "            print(f\"      âš ï¸ Dropping constant cols: {constant_cols}\")\n",
    "            train_df = train_df.drop(columns=constant_cols)\n",
    "            test_df = test_df.drop(columns=constant_cols)\n",
    "\n",
    "        # åŸ·è¡Œ Stepwise\n",
    "        try:\n",
    "            overall_fit, coef_df, not_in_eq_df, final_model = stepwise_logit_with_k_tables(\n",
    "                train_df,\n",
    "                test_df,\n",
    "                dep_var=\"is_fraud\",\n",
    "                k=314657018, \n",
    "                verbose=True  # âœ… ç¢ºä¿é€™è£¡é–‹å•Ÿé¡¯ç¤º\n",
    "            )\n",
    "\n",
    "            train_table = classification_table(final_model, train_df)\n",
    "            test_table  = classification_table(final_model, test_df)\n",
    "\n",
    "            results[block_id][group_name] = {\n",
    "                \"overall_fit\": overall_fit,\n",
    "                \"coef_df\": coef_df,\n",
    "                \"model\": final_model,\n",
    "                \"train_table\": train_table,\n",
    "                \"test_table\": test_table\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"      âŒ Error: {str(e)}\")\n",
    "            results[block_id][group_name] = {\"error\": str(e)}\n",
    "\n",
    "print(\"\\nğŸ‰ All blocks processed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## åˆ†æç”¨ï¼ï¼\n",
    "\n",
    "results[\n",
    "  block_id\n",
    "][\n",
    "  \"X_all + X_rfm + X_dk\"\n",
    "][\"coef_df\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04-3 allä¸Ÿæ¨¡å‹å…§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸš€ Running Full Logit for Block 0 (Year: 2010 - 2011)\n",
      "==================================================\n",
      "   ğŸ”¹ Group: X_all (25 features)... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/sklearn/metrics/_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/sklearn/metrics/_ranking.py:1046: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Done.\n",
      "      Train AUC: 0.9526 | Test AUC: nan\n",
      "      Train PR : 0.0372 | Test PR : 0.0\n",
      "   ğŸ”¹ Group: X_rfm (6 features)... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/sklearn/metrics/_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/sklearn/metrics/_ranking.py:1046: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Done.\n",
      "      Train AUC: 0.7276 | Test AUC: nan\n",
      "      Train PR : 0.0062 | Test PR : 0.0\n",
      "   ğŸ”¹ Group: X_dk (7 features)... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/sklearn/metrics/_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/sklearn/metrics/_ranking.py:1046: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Done.\n",
      "      Train AUC: 0.9704 | Test AUC: nan\n",
      "      Train PR : 0.1757 | Test PR : 0.0\n",
      "   ğŸ”¹ Group: X_all + X_rfm (31 features)... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/sklearn/metrics/_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/sklearn/metrics/_ranking.py:1046: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Done.\n",
      "      Train AUC: 0.9609 | Test AUC: nan\n",
      "      Train PR : 0.0643 | Test PR : 0.0\n",
      "   ğŸ”¹ Group: X_all + X_dk (32 features)... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/sklearn/metrics/_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/sklearn/metrics/_ranking.py:1046: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Done.\n",
      "      Train AUC: 0.9769 | Test AUC: nan\n",
      "      Train PR : 0.2107 | Test PR : 0.0\n",
      "   ğŸ”¹ Group: X_rfm + X_dk (13 features)... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/sklearn/metrics/_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/sklearn/metrics/_ranking.py:1046: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Done.\n",
      "      Train AUC: 0.9805 | Test AUC: nan\n",
      "      Train PR : 0.2253 | Test PR : 0.0\n",
      "   ğŸ”¹ Group: X_all + X_rfm + X_dk (38 features)... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/sklearn/metrics/_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/sklearn/metrics/_ranking.py:1046: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Done.\n",
      "      Train AUC: 0.9826 | Test AUC: nan\n",
      "      Train PR : 0.2445 | Test PR : 0.0\n",
      "\n",
      "==================================================\n",
      "ğŸš€ Running Full Logit for Block 1 (Year: 2012 - 2013)\n",
      "==================================================\n",
      "   ğŸ”¹ Group: X_all (25 features)... âœ… Done.\n",
      "      Train AUC: 0.8542 | Test AUC: 0.9019\n",
      "      Train PR : 0.0084 | Test PR : 0.0338\n",
      "   ğŸ”¹ Group: X_rfm (6 features)... âœ… Done.\n",
      "      Train AUC: 0.6996 | Test AUC: 0.7287\n",
      "      Train PR : 0.0024 | Test PR : 0.0138\n",
      "   ğŸ”¹ Group: X_dk (7 features)... âœ… Done.\n",
      "      Train AUC: 0.814 | Test AUC: 0.8753\n",
      "      Train PR : 0.021 | Test PR : 0.0801\n",
      "   ğŸ”¹ Group: X_all + X_rfm (31 features)... âœ… Done.\n",
      "      Train AUC: 0.887 | Test AUC: 0.9293\n",
      "      Train PR : 0.0143 | Test PR : 0.0644\n",
      "   ğŸ”¹ Group: X_all + X_dk (32 features)... âœ… Done.\n",
      "      Train AUC: 0.8458 | Test AUC: 0.8936\n",
      "      Train PR : 0.0279 | Test PR : 0.0885\n",
      "   ğŸ”¹ Group: X_rfm + X_dk (13 features)... âœ… Done.\n",
      "      Train AUC: 0.8738 | Test AUC: 0.921\n",
      "      Train PR : 0.0432 | Test PR : 0.1773\n",
      "   ğŸ”¹ Group: X_all + X_rfm + X_dk (38 features)... âœ… Done.\n",
      "      Train AUC: 0.8756 | Test AUC: 0.92\n",
      "      Train PR : 0.0426 | Test PR : 0.1507\n",
      "\n",
      "==================================================\n",
      "ğŸš€ Running Full Logit for Block 2 (Year: 2014 - 2015)\n",
      "==================================================\n",
      "   ğŸ”¹ Group: X_all (25 features)... âœ… Done.\n",
      "      Train AUC: 0.9129 | Test AUC: 0.9096\n",
      "      Train PR : 0.0352 | Test PR : 0.077\n",
      "   ğŸ”¹ Group: X_rfm (6 features)... âœ… Done.\n",
      "      Train AUC: 0.7522 | Test AUC: 0.7256\n",
      "      Train PR : 0.0066 | Test PR : 0.0144\n",
      "   ğŸ”¹ Group: X_dk (7 features)... âœ… Done.\n",
      "      Train AUC: 0.8648 | Test AUC: 0.8611\n",
      "      Train PR : 0.0664 | Test PR : 0.1105\n",
      "   ğŸ”¹ Group: X_all + X_rfm (31 features)... âœ… Done.\n",
      "      Train AUC: 0.9332 | Test AUC: 0.9264\n",
      "      Train PR : 0.0827 | Test PR : 0.1392\n",
      "   ğŸ”¹ Group: X_all + X_dk (32 features)... âœ… Done.\n",
      "      Train AUC: 0.8939 | Test AUC: 0.8915\n",
      "      Train PR : 0.0934 | Test PR : 0.1585\n",
      "   ğŸ”¹ Group: X_rfm + X_dk (13 features)... âœ… Done.\n",
      "      Train AUC: 0.9184 | Test AUC: 0.9055\n",
      "      Train PR : 0.1183 | Test PR : 0.2259\n",
      "   ğŸ”¹ Group: X_all + X_rfm + X_dk (38 features)... âœ… Done.\n",
      "      Train AUC: 0.9278 | Test AUC: 0.9196\n",
      "      Train PR : 0.1617 | Test PR : 0.2497\n",
      "\n",
      "==================================================\n",
      "ğŸš€ Running Full Logit for Block 3 (Year: 2016 - 2017)\n",
      "==================================================\n",
      "   ğŸ”¹ Group: X_all (25 features)... âœ… Done.\n",
      "      Train AUC: 0.917 | Test AUC: 0.5701\n",
      "      Train PR : 0.0385 | Test PR : 0.0005\n",
      "   ğŸ”¹ Group: X_rfm (6 features)... âœ… Done.\n",
      "      Train AUC: 0.7126 | Test AUC: 0.6716\n",
      "      Train PR : 0.0056 | Test PR : 0.001\n",
      "   ğŸ”¹ Group: X_dk (7 features)... âœ… Done.\n",
      "      Train AUC: 0.8704 | Test AUC: 0.1473\n",
      "      Train PR : 0.0419 | Test PR : 0.0004\n",
      "   ğŸ”¹ Group: X_all + X_rfm (31 features)... âœ… Done.\n",
      "      Train AUC: 0.9335 | Test AUC: 0.6183\n",
      "      Train PR : 0.062 | Test PR : 0.0006\n",
      "   ğŸ”¹ Group: X_all + X_dk (32 features)... âœ… Done.\n",
      "      Train AUC: 0.9154 | Test AUC: 0.221\n",
      "      Train PR : 0.0725 | Test PR : 0.0003\n",
      "   ğŸ”¹ Group: X_rfm + X_dk (13 features)... âœ… Done.\n",
      "      Train AUC: 0.9154 | Test AUC: 0.3024\n",
      "      Train PR : 0.0959 | Test PR : 0.0003\n",
      "   ğŸ”¹ Group: X_all + X_rfm + X_dk (38 features)... âœ… Done.\n",
      "      Train AUC: 0.9331 | Test AUC: 0.2188\n",
      "      Train PR : 0.1211 | Test PR : 0.0003\n",
      "\n",
      "==================================================\n",
      "ğŸš€ Running Full Logit for Block 4 (Year: 2018 - 2019)\n",
      "==================================================\n",
      "   ğŸ”¹ Group: X_all (25 features)... âœ… Done.\n",
      "      Train AUC: 0.9621 | Test AUC: 0.9709\n",
      "      Train PR : 0.0982 | Test PR : 0.0785\n",
      "   ğŸ”¹ Group: X_rfm (6 features)... âœ… Done.\n",
      "      Train AUC: 0.7243 | Test AUC: 0.6992\n",
      "      Train PR : 0.0056 | Test PR : 0.0045\n",
      "   ğŸ”¹ Group: X_dk (7 features)... âœ… Done.\n",
      "      Train AUC: 0.9926 | Test AUC: 0.9986\n",
      "      Train PR : 0.4685 | Test PR : 0.4608\n",
      "   ğŸ”¹ Group: X_all + X_rfm (31 features)... âœ… Done.\n",
      "      Train AUC: 0.9663 | Test AUC: 0.975\n",
      "      Train PR : 0.1242 | Test PR : 0.0901\n",
      "   ğŸ”¹ Group: X_all + X_dk (32 features)... âœ… Done.\n",
      "      Train AUC: 0.9951 | Test AUC: 0.9988\n",
      "      Train PR : 0.6605 | Test PR : 0.5942\n",
      "   ğŸ”¹ Group: X_rfm + X_dk (13 features)... âœ… Done.\n",
      "      Train AUC: 0.996 | Test AUC: 0.999\n",
      "      Train PR : 0.6185 | Test PR : 0.6124\n",
      "   ğŸ”¹ Group: X_all + X_rfm + X_dk (38 features)... âœ… Done.\n",
      "      Train AUC: 0.9965 | Test AUC: 0.9989\n",
      "      Train PR : 0.7142 | Test PR : 0.6538\n",
      "\n",
      "ğŸ‰ All Full Logistic Regression models completed!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import copy\n",
    "\n",
    "# ===========================================================\n",
    "# 1. Full Logistic Regression è¨“ç·´å‡½æ•¸\n",
    "# ===========================================================\n",
    "def fit_full_logit(train_df, test_df, feature_cols, dep_var=\"is_fraud\"):\n",
    "    \"\"\"\n",
    "    é‡å°çµ¦å®šçš„ç‰¹å¾µåˆ—è¡¨é€²è¡Œå…¨è®Šæ•¸ Logistic Regression (L2 Penalty)\n",
    "    \"\"\"\n",
    "    # æº–å‚™ X å’Œ y\n",
    "    X_train = train_df[feature_cols]\n",
    "    y_train = train_df[dep_var]\n",
    "\n",
    "    X_test = test_df[feature_cols]\n",
    "    y_test = test_df[dep_var]\n",
    "\n",
    "    # è™•ç†ç¼ºå¤±å€¼ (Sklearn ä¸æ¥å— NaN)\n",
    "    # ç°¡å–®ç­–ç•¥ï¼šå¡«è£œ 0 æˆ–åˆªé™¤ (é€™é‚Šå‡è¨­ä½ å‰é¢çš„è³‡æ–™æ¸…æ´—å·²ç¶“è™•ç†å®Œ NaNï¼Œè‹¥æœ‰æ®˜ç•™å»ºè­° fillna)\n",
    "    X_train = X_train.fillna(0)\n",
    "    X_test = X_test.fillna(0)\n",
    "\n",
    "    # 1. æ¨™æº–åŒ– (Standardization) - éå¸¸é‡è¦ï¼\n",
    "    # å¿…é ˆç”¨ Train çš„æ•¸æ“šä¾† fitï¼Œç„¶å¾Œ transform åˆ° Train å’Œ Test\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # 2. è¨“ç·´æ¨¡å‹ (ä½¿ç”¨ L2 Regularization é˜²æ­¢éæ“¬åˆ)\n",
    "    model = LogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=2000,  # å¢åŠ è¿­ä»£æ¬¡æ•¸é¿å…æ”¶æ–‚å¤±æ•—\n",
    "        n_jobs=-1,      # å¹³è¡Œé‹ç®—\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # 3. é æ¸¬æ©Ÿç‡\n",
    "        train_pred_prob = model.predict_proba(X_train_scaled)[:, 1]\n",
    "        test_pred_prob = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "        # 4. è¨ˆç®—æŒ‡æ¨™\n",
    "        train_auc = roc_auc_score(y_train, train_pred_prob)\n",
    "        test_auc = roc_auc_score(y_test, test_pred_prob)\n",
    "        \n",
    "        train_prauc = average_precision_score(y_train, train_pred_prob)\n",
    "        test_prauc = average_precision_score(y_test, test_pred_prob)\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"model\": model,\n",
    "            \"scaler\": scaler,\n",
    "            \"metrics\": {\n",
    "                \"Train AUC\": round(train_auc, 4),\n",
    "                \"Test AUC\": round(test_auc, 4),\n",
    "                \"Train PR-AUC\": round(train_prauc, 4),\n",
    "                \"Test PR-AUC\": round(test_prauc, 4)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "# 2. æº–å‚™ Feature Groups (ç¢ºä¿ç„¡ç›®æ¨™è®Šæ•¸èˆ‡ID)\n",
    "# ===========================================================\n",
    "# ç¢ºä¿ all_cols, rfm_cols, dk_cols å·²ç¶“å®šç¾©\n",
    "identifier = ['transaction_id', 'client_id_x', 'card_id', 'card_number', 'cvv']\n",
    "exclude_cols = set(identifier) | {'is_fraud', 'date', 'year', 'time_block'}\n",
    "\n",
    "# é€™è£¡å‡è¨­ä½ çš„ cols åˆ—è¡¨å·²ç¶“å­˜åœ¨ï¼Œåšæœ€å¾Œä¸€æ¬¡æ¸…æ´—ç¢ºä¿å®‰å…¨\n",
    "# å¦‚æœä½ ä¹‹å‰çš„ all_cols å·²ç¶“æ¸…ä¹¾æ·¨äº†ï¼Œé€™è£¡åªæ˜¯é›™é‡ä¿éšª\n",
    "def clean_cols(cols):\n",
    "    return [c for c in cols if c not in exclude_cols]\n",
    "\n",
    "feature_groups = {\n",
    "    \"X_all\": clean_cols(all_cols),\n",
    "    \"X_rfm\": clean_cols(rfm_cols),\n",
    "    \"X_dk\": clean_cols(dk_cols),\n",
    "    \"X_all + X_rfm\": clean_cols(all_cols + rfm_cols),\n",
    "    \"X_all + X_dk\": clean_cols(all_cols + dk_cols),\n",
    "    \"X_rfm + X_dk\": clean_cols(rfm_cols + dk_cols),\n",
    "    \"X_all + X_rfm + X_dk\": clean_cols(all_cols + rfm_cols + dk_cols)\n",
    "}\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "# 3. ä¸»è¿´åœˆ (Block + Feature Groups)\n",
    "# ===========================================================\n",
    "full_logit_results = {}\n",
    "\n",
    "for block_id, block_df in df_sorted.groupby('time_block'):\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"ğŸš€ Running Full Logit for Block {block_id} (Year: {2010 + block_id*2} - {2011 + block_id*2})\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    # --- è³‡æ–™åˆ‡åˆ† (Split) ---\n",
    "    block_df = block_df.sort_values('date')\n",
    "    split_index = int(len(block_df) * 0.8)\n",
    "    \n",
    "    train_raw = block_df.iloc[:split_index].copy()\n",
    "    test_raw  = block_df.iloc[split_index:].copy()\n",
    "\n",
    "    # --- é‡ç®— HighRiskMCC (Anti-Leakage) ---\n",
    "    fraud_rate = train_raw.groupby('mcc_code')['is_fraud'].mean()\n",
    "    high_risk_mcc = fraud_rate[fraud_rate > 0.02].index\n",
    "    \n",
    "    train_raw['HighRiskMCC'] = train_raw['mcc_code'].isin(high_risk_mcc).astype('uint8')\n",
    "    test_raw['HighRiskMCC']  = test_raw['mcc_code'].isin(high_risk_mcc).astype('uint8')\n",
    "\n",
    "    # åˆå§‹åŒ–çµæœå­˜æ”¾\n",
    "    full_logit_results[block_id] = {}\n",
    "\n",
    "    # --- é‡å°æ¯å€‹ Feature Group è·‘æ¨¡å‹ ---\n",
    "    for group_name, feature_list in feature_groups.items():\n",
    "        \n",
    "        # ç¢ºä¿ç‰¹å¾µå­˜åœ¨æ–¼ DataFrame\n",
    "        valid_features = [f for f in feature_list if f in train_raw.columns]\n",
    "        \n",
    "        # ç°¡å–®æª¢æŸ¥ï¼šè‹¥ç‰¹å¾µå°‘æ–¼ 1 å€‹å°±è·³é\n",
    "        if not valid_features:\n",
    "            print(f\"   âš ï¸ Group: {group_name} - No valid features found. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"   ğŸ”¹ Group: {group_name} ({len(valid_features)} features)...\", end=\" \")\n",
    "\n",
    "        # åŸ·è¡Œ Full Logistic Regression\n",
    "        res = fit_full_logit(\n",
    "            train_raw, \n",
    "            test_raw, \n",
    "            feature_cols=valid_features, \n",
    "            dep_var=\"is_fraud\"\n",
    "        )\n",
    "\n",
    "        if res[\"status\"] == \"success\":\n",
    "            metrics = res[\"metrics\"]\n",
    "            print(f\"âœ… Done.\")\n",
    "            print(f\"      Train AUC: {metrics['Train AUC']} | Test AUC: {metrics['Test AUC']}\")\n",
    "            print(f\"      Train PR : {metrics['Train PR-AUC']} | Test PR : {metrics['Test PR-AUC']}\")\n",
    "            \n",
    "            # å„²å­˜çµæœ\n",
    "            full_logit_results[block_id][group_name] = metrics\n",
    "        else:\n",
    "            print(f\"âŒ Error: {res['message']}\")\n",
    "            full_logit_results[block_id][group_name] = {\"error\": res['message']}\n",
    "\n",
    "print(\"\\nğŸ‰ All Full Logistic Regression models completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== è©³ç´°çµæœç¸½è¡¨ (å‰ 5 ç­†) ===\n",
      "   Block       Year  Feature Group  Train AUC  Test AUC  Train PR-AUC  \\\n",
      "0      0  2010-2011          X_all     0.9526       NaN        0.0372   \n",
      "1      0  2010-2011          X_rfm     0.7276       NaN        0.0062   \n",
      "2      0  2010-2011           X_dk     0.9704       NaN        0.1757   \n",
      "3      0  2010-2011  X_all + X_rfm     0.9609       NaN        0.0643   \n",
      "4      0  2010-2011   X_all + X_dk     0.9769       NaN        0.2107   \n",
      "\n",
      "   Test PR-AUC  \n",
      "0          0.0  \n",
      "1          0.0  \n",
      "2          0.0  \n",
      "3          0.0  \n",
      "4          0.0  \n",
      "\n",
      "=== Traing PR-AUC ç¸¾æ•ˆçŸ©é™£ (æ•¸å€¼è¶Šé«˜è¶Šå¥½) ===\n",
      "Year                  2010-2011  2012-2013  2014-2015  2016-2017  2018-2019\n",
      "Feature Group                                                              \n",
      "X_all                    0.0372     0.0084     0.0352     0.0385     0.0982\n",
      "X_all + X_dk             0.2107     0.0279     0.0934     0.0725     0.6605\n",
      "X_all + X_rfm            0.0643     0.0143     0.0827     0.0620     0.1242\n",
      "X_all + X_rfm + X_dk     0.2445     0.0426     0.1617     0.1211     0.7142\n",
      "X_dk                     0.1757     0.0210     0.0664     0.0419     0.4685\n",
      "X_rfm                    0.0062     0.0024     0.0066     0.0056     0.0056\n",
      "X_rfm + X_dk             0.2253     0.0432     0.1183     0.0959     0.6185\n",
      "\n",
      "=== Test PR-AUC ç¸¾æ•ˆçŸ©é™£ (æ•¸å€¼è¶Šé«˜è¶Šå¥½) ===\n",
      "Year                  2010-2011  2012-2013  2014-2015  2016-2017  2018-2019\n",
      "Feature Group                                                              \n",
      "X_all                       0.0     0.0338     0.0770     0.0005     0.0785\n",
      "X_all + X_dk                0.0     0.0885     0.1585     0.0003     0.5942\n",
      "X_all + X_rfm               0.0     0.0644     0.1392     0.0006     0.0901\n",
      "X_all + X_rfm + X_dk        0.0     0.1507     0.2497     0.0003     0.6538\n",
      "X_dk                        0.0     0.0801     0.1105     0.0004     0.4608\n",
      "X_rfm                       0.0     0.0138     0.0144     0.0010     0.0045\n",
      "X_rfm + X_dk                0.0     0.1773     0.2259     0.0003     0.6124\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==========================================\n",
    "# 1. å°‡å·¢ç‹€å­—å…¸è½‰ç‚º DataFrame\n",
    "# ==========================================\n",
    "data_list = []\n",
    "\n",
    "for block_id, groups in full_logit_results.items():\n",
    "    for group_name, metrics in groups.items():\n",
    "        # æ’é™¤ç™¼ç”ŸéŒ¯èª¤çš„çµ„åˆ¥\n",
    "        if \"error\" in metrics:\n",
    "            continue\n",
    "            \n",
    "        row = {\n",
    "            \"Block\": block_id,\n",
    "            \"Year\": f\"{2010 + block_id*2}-{2011 + block_id*2}\",\n",
    "            \"Feature Group\": group_name,\n",
    "            \"Train AUC\": metrics.get(\"Train AUC\"),\n",
    "            \"Test AUC\": metrics.get(\"Test AUC\"),\n",
    "            \"Train PR-AUC\": metrics.get(\"Train PR-AUC\"),\n",
    "            \"Test PR-AUC\": metrics.get(\"Test PR-AUC\")\n",
    "        }\n",
    "        data_list.append(row)\n",
    "\n",
    "df_results = pd.DataFrame(data_list)\n",
    "\n",
    "# èª¿æ•´æ¬„ä½é †åº\n",
    "cols = [\"Block\", \"Year\", \"Feature Group\", \"Train AUC\", \"Test AUC\", \"Train PR-AUC\", \"Test PR-AUC\"]\n",
    "df_results = df_results[cols]\n",
    "\n",
    "# é¡¯ç¤ºå‰å¹¾ç­†\n",
    "print(\"=== è©³ç´°çµæœç¸½è¡¨ (å‰ 5 ç­†) ===\")\n",
    "print(df_results.head())\n",
    "\n",
    "# ==========================================\n",
    "# 2. è£½ä½œ Test AUC æ¯”è¼ƒçŸ©é™£ (Pivot Table)\n",
    "# ==========================================\n",
    "# é€™æ˜¯æœ€ç›´è§€çš„è¡¨æ ¼ï¼šæ©«è»¸æ˜¯æ™‚é–“ï¼Œç¸±è»¸æ˜¯ç‰¹å¾µç¾¤çµ„ï¼Œæ•¸å€¼æ˜¯ Test AUC\n",
    "pivot_auc = df_results.pivot(index=\"Feature Group\", columns=\"Year\", values=\"Test PR-AUC\")\n",
    "pivot_trauc = df_results.pivot(index=\"Feature Group\", columns=\"Year\", values=\"Train PR-AUC\")\n",
    "\n",
    "print(\"\\n=== Traing PR-AUC ç¸¾æ•ˆçŸ©é™£ (æ•¸å€¼è¶Šé«˜è¶Šå¥½) ===\")\n",
    "print(pivot_trauc)\n",
    "print(\"\\n=== Test PR-AUC ç¸¾æ•ˆçŸ©é™£ (æ•¸å€¼è¶Šé«˜è¶Šå¥½) ===\")\n",
    "print(pivot_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸš€ Running XGBoost for Block 0 (Year: 2010 - 2011)\n",
      "============================================================\n",
      "   ğŸ”¹ Group: X_all (25 features)... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/xgboost/callback.py:266: UserWarning: [16:14:53] WARNING: /Users/runner/work/xgboost/xgboost/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/xgboost/callback.py:266: UserWarning: [16:14:54] WARNING: /Users/runner/work/xgboost/xgboost/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/sklearn/metrics/_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/sklearn/metrics/_ranking.py:1046: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Done (Best Iter: 0)\n",
      "      Train AUC: 0.9878 | Test AUC: nan\n",
      "      Train PR : 0.2217 | Test PR : 0.0\n",
      "   ğŸ”¹ Group: X_rfm (6 features)... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/xgboost/callback.py:266: UserWarning: [16:14:55] WARNING: /Users/runner/work/xgboost/xgboost/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/xgboost/callback.py:266: UserWarning: [16:14:56] WARNING: /Users/runner/work/xgboost/xgboost/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/sklearn/metrics/_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/sklearn/metrics/_ranking.py:1046: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/xgboost/callback.py:266: UserWarning: [16:14:56] WARNING: /Users/runner/work/xgboost/xgboost/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Done (Best Iter: 0)\n",
      "      Train AUC: 0.7702 | Test AUC: nan\n",
      "      Train PR : 0.0084 | Test PR : 0.0\n",
      "   ğŸ”¹ Group: X_dk (7 features)... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/xgboost/callback.py:266: UserWarning: [16:14:57] WARNING: /Users/runner/work/xgboost/xgboost/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/sklearn/metrics/_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/sklearn/metrics/_ranking.py:1046: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Done (Best Iter: 0)\n",
      "      Train AUC: 0.9743 | Test AUC: nan\n",
      "      Train PR : 0.1308 | Test PR : 0.0\n",
      "   ğŸ”¹ Group: X_all + X_rfm (31 features)... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/xgboost/callback.py:266: UserWarning: [16:14:57] WARNING: /Users/runner/work/xgboost/xgboost/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/xgboost/callback.py:266: UserWarning: [16:14:58] WARNING: /Users/runner/work/xgboost/xgboost/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/xgboost/callback.py:266: UserWarning: [16:14:59] WARNING: /Users/runner/work/xgboost/xgboost/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/sklearn/metrics/_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/sklearn/metrics/_ranking.py:1046: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Done (Best Iter: 0)\n",
      "      Train AUC: 0.9793 | Test AUC: nan\n",
      "      Train PR : 0.0874 | Test PR : 0.0\n",
      "   ğŸ”¹ Group: X_all + X_dk (32 features)... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/xgboost/callback.py:266: UserWarning: [16:14:59] WARNING: /Users/runner/work/xgboost/xgboost/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/xgboost/callback.py:266: UserWarning: [16:15:00] WARNING: /Users/runner/work/xgboost/xgboost/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/sklearn/metrics/_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/sklearn/metrics/_ranking.py:1046: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Done (Best Iter: 0)\n",
      "      Train AUC: 0.9946 | Test AUC: nan\n",
      "      Train PR : 0.2525 | Test PR : 0.0\n",
      "   ğŸ”¹ Group: X_rfm + X_dk (13 features)... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/xgboost/callback.py:266: UserWarning: [16:15:01] WARNING: /Users/runner/work/xgboost/xgboost/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/sklearn/metrics/_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/sklearn/metrics/_ranking.py:1046: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Done (Best Iter: 0)\n",
      "      Train AUC: 0.9782 | Test AUC: nan\n",
      "      Train PR : 0.071 | Test PR : 0.0\n",
      "   ğŸ”¹ Group: X_all + X_rfm + X_dk (38 features)... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/xgboost/callback.py:266: UserWarning: [16:15:02] WARNING: /Users/runner/work/xgboost/xgboost/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/xgboost/callback.py:266: UserWarning: [16:15:03] WARNING: /Users/runner/work/xgboost/xgboost/src/metric/auc.cc:324: Dataset is empty, or contains only positive or negative samples.\n",
      "  score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/sklearn/metrics/_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/sklearn/metrics/_ranking.py:1046: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Done (Best Iter: 0)\n",
      "      Train AUC: 0.9949 | Test AUC: nan\n",
      "      Train PR : 0.2657 | Test PR : 0.0\n",
      "\n",
      "============================================================\n",
      "ğŸš€ Running XGBoost for Block 1 (Year: 2012 - 2013)\n",
      "============================================================\n",
      "   ğŸ”¹ Group: X_all (25 features)... âœ… Done (Best Iter: 66)\n",
      "      Train AUC: 0.9832 | Test AUC: 0.9861\n",
      "      Train PR : 0.6527 | Test PR : 0.7896\n",
      "   ğŸ”¹ Group: X_rfm (6 features)... âœ… Done (Best Iter: 43)\n",
      "      Train AUC: 0.8576 | Test AUC: 0.7977\n",
      "      Train PR : 0.0177 | Test PR : 0.0192\n",
      "   ğŸ”¹ Group: X_dk (7 features)... âœ… Done (Best Iter: 38)\n",
      "      Train AUC: 0.8765 | Test AUC: 0.9387\n",
      "      Train PR : 0.0213 | Test PR : 0.083\n",
      "   ğŸ”¹ Group: X_all + X_rfm (31 features)... âœ… Done (Best Iter: 118)\n",
      "      Train AUC: 0.9954 | Test AUC: 0.992\n",
      "      Train PR : 0.7646 | Test PR : 0.8077\n",
      "   ğŸ”¹ Group: X_all + X_dk (32 features)... âœ… Done (Best Iter: 52)\n",
      "      Train AUC: 0.9763 | Test AUC: 0.9904\n",
      "      Train PR : 0.6197 | Test PR : 0.7917\n",
      "   ğŸ”¹ Group: X_rfm + X_dk (13 features)... âœ… Done (Best Iter: 23)\n",
      "      Train AUC: 0.9396 | Test AUC: 0.9664\n",
      "      Train PR : 0.1107 | Test PR : 0.2751\n",
      "   ğŸ”¹ Group: X_all + X_rfm + X_dk (38 features)... âœ… Done (Best Iter: 78)\n",
      "      Train AUC: 0.9902 | Test AUC: 0.9939\n",
      "      Train PR : 0.6893 | Test PR : 0.8042\n",
      "\n",
      "============================================================\n",
      "ğŸš€ Running XGBoost for Block 2 (Year: 2014 - 2015)\n",
      "============================================================\n",
      "   ğŸ”¹ Group: X_all (25 features)... âœ… Done (Best Iter: 198)\n",
      "      Train AUC: 0.9985 | Test AUC: 0.9596\n",
      "      Train PR : 0.9058 | Test PR : 0.7663\n",
      "   ğŸ”¹ Group: X_rfm (6 features)... âœ… Done (Best Iter: 62)\n",
      "      Train AUC: 0.8951 | Test AUC: 0.7968\n",
      "      Train PR : 0.0415 | Test PR : 0.0195\n",
      "   ğŸ”¹ Group: X_dk (7 features)... âœ… Done (Best Iter: 7)\n",
      "      Train AUC: 0.926 | Test AUC: 0.9154\n",
      "      Train PR : 0.067 | Test PR : 0.1115\n",
      "   ğŸ”¹ Group: X_all + X_rfm (31 features)... âœ… Done (Best Iter: 294)\n",
      "      Train AUC: 0.9998 | Test AUC: 0.968\n",
      "      Train PR : 0.968 | Test PR : 0.7706\n",
      "   ğŸ”¹ Group: X_all + X_dk (32 features)... âœ… Done (Best Iter: 133)\n",
      "      Train AUC: 0.9967 | Test AUC: 0.9719\n",
      "      Train PR : 0.8607 | Test PR : 0.7556\n",
      "   ğŸ”¹ Group: X_rfm + X_dk (13 features)... âœ… Done (Best Iter: 77)\n",
      "      Train AUC: 0.9791 | Test AUC: 0.9481\n",
      "      Train PR : 0.319 | Test PR : 0.305\n",
      "   ğŸ”¹ Group: X_all + X_rfm + X_dk (38 features)... âœ… Done (Best Iter: 201)\n",
      "      Train AUC: 0.9994 | Test AUC: 0.9755\n",
      "      Train PR : 0.9464 | Test PR : 0.7873\n",
      "\n",
      "============================================================\n",
      "ğŸš€ Running XGBoost for Block 3 (Year: 2016 - 2017)\n",
      "============================================================\n",
      "   ğŸ”¹ Group: X_all (25 features)... âœ… Done (Best Iter: 3)\n",
      "      Train AUC: 0.9661 | Test AUC: 0.6733\n",
      "      Train PR : 0.3223 | Test PR : 0.001\n",
      "   ğŸ”¹ Group: X_rfm (6 features)... âœ… Done (Best Iter: 25)\n",
      "      Train AUC: 0.8386 | Test AUC: 0.6891\n",
      "      Train PR : 0.0198 | Test PR : 0.0012\n",
      "   ğŸ”¹ Group: X_dk (7 features)... âœ… Done (Best Iter: 27)\n",
      "      Train AUC: 0.9103 | Test AUC: 0.7891\n",
      "      Train PR : 0.0423 | Test PR : 0.005\n",
      "   ğŸ”¹ Group: X_all + X_rfm (31 features)... âœ… Done (Best Iter: 3)\n",
      "      Train AUC: 0.9759 | Test AUC: 0.6928\n",
      "      Train PR : 0.5021 | Test PR : 0.0009\n",
      "   ğŸ”¹ Group: X_all + X_dk (32 features)... âœ… Done (Best Iter: 9)\n",
      "      Train AUC: 0.9731 | Test AUC: 0.9273\n",
      "      Train PR : 0.5034 | Test PR : 0.0043\n",
      "   ğŸ”¹ Group: X_rfm + X_dk (13 features)... âœ… Done (Best Iter: 0)\n",
      "      Train AUC: 0.9338 | Test AUC: 0.9564\n",
      "      Train PR : 0.0473 | Test PR : 0.0074\n",
      "   ğŸ”¹ Group: X_all + X_rfm + X_dk (38 features)... âœ… Done (Best Iter: 14)\n",
      "      Train AUC: 0.9791 | Test AUC: 0.9305\n",
      "      Train PR : 0.6182 | Test PR : 0.0032\n",
      "\n",
      "============================================================\n",
      "ğŸš€ Running XGBoost for Block 4 (Year: 2018 - 2019)\n",
      "============================================================\n",
      "   ğŸ”¹ Group: X_all (25 features)... âœ… Done (Best Iter: 161)\n",
      "      Train AUC: 0.9998 | Test AUC: 0.999\n",
      "      Train PR : 0.9393 | Test PR : 0.745\n",
      "   ğŸ”¹ Group: X_rfm (6 features)... âœ… Done (Best Iter: 62)\n",
      "      Train AUC: 0.8349 | Test AUC: 0.7262\n",
      "      Train PR : 0.0233 | Test PR : 0.0056\n",
      "   ğŸ”¹ Group: X_dk (7 features)... âœ… Done (Best Iter: 6)\n",
      "      Train AUC: 0.9954 | Test AUC: 0.9986\n",
      "      Train PR : 0.4699 | Test PR : 0.4616\n",
      "   ğŸ”¹ Group: X_all + X_rfm (31 features)... âœ… Done (Best Iter: 201)\n",
      "      Train AUC: 0.9999 | Test AUC: 0.999\n",
      "      Train PR : 0.9738 | Test PR : 0.7201\n",
      "   ğŸ”¹ Group: X_all + X_dk (32 features)... âœ… Done (Best Iter: 126)\n",
      "      Train AUC: 0.9998 | Test AUC: 0.9992\n",
      "      Train PR : 0.9234 | Test PR : 0.7101\n",
      "   ğŸ”¹ Group: X_rfm + X_dk (13 features)... âœ… Done (Best Iter: 174)\n",
      "      Train AUC: 0.9992 | Test AUC: 0.9991\n",
      "      Train PR : 0.8218 | Test PR : 0.6335\n",
      "   ğŸ”¹ Group: X_all + X_rfm + X_dk (38 features)... âœ… Done (Best Iter: 198)\n",
      "      Train AUC: 1.0 | Test AUC: 0.9994\n",
      "      Train PR : 0.9852 | Test PR : 0.8078\n",
      "\n",
      "ğŸ‰ All XGBoost models completed!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import copy\n",
    "\n",
    "# ===========================================================\n",
    "# 1. å®šç¾©å–®æ¬¡ XGBoost è¨“ç·´å‡½æ•¸\n",
    "# ===========================================================\n",
    "def run_xgb_single(train_df, test_df, feature_cols, dep_var=\"is_fraud\"):\n",
    "    \"\"\"\n",
    "    é‡å°çµ¦å®šçš„ Train/Test å’Œç‰¹å¾µåˆ—è¡¨è¨“ç·´ XGBoost\n",
    "    \"\"\"\n",
    "    X_train = train_df[feature_cols]\n",
    "    y_train = train_df[dep_var]\n",
    "    X_test = test_df[feature_cols]\n",
    "    y_test = test_df[dep_var]\n",
    "\n",
    "    # åˆå§‹åŒ– XGBoost\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        eval_metric=\"auc\", # æ”¹ç”¨ AUC ä½œç‚ºè©•ä¼°æŒ‡æ¨™é€šå¸¸æ¯”è¼ƒç›´è§€ï¼Œlogloss ä¹Ÿå¯ä»¥\n",
    "        random_state=42,\n",
    "        tree_method=\"hist\", # åŠ é€Ÿè¨“ç·´\n",
    "        n_jobs=-1,\n",
    "        early_stopping_rounds=50 # å¦‚æœ 50 è¼ªå…§é©—è­‰é›† AUC æ²’æå‡å°±åœæ­¢\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # è¨“ç·´æ¨¡å‹\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_test, y_test)],\n",
    "            verbose=False # é—œé–‰è©³ç´°è¨“ç·´éç¨‹è¼¸å‡ºï¼Œé¿å…æ´—ç‰ˆ\n",
    "        )\n",
    "\n",
    "        # é æ¸¬æ©Ÿç‡\n",
    "        train_pred = model.predict_proba(X_train)[:, 1]\n",
    "        test_pred = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # è¨ˆç®—æŒ‡æ¨™\n",
    "        train_roc = roc_auc_score(y_train, train_pred)\n",
    "        test_roc = roc_auc_score(y_test, test_pred)\n",
    "        train_pr = average_precision_score(y_train, train_pred)\n",
    "        test_pr = average_precision_score(y_test, test_pred)\n",
    "\n",
    "        # å–å¾—æœ€ä½³è¿­ä»£æ¬¡æ•¸ (å¦‚æœè§¸ç™¼æ—©åœ)\n",
    "        best_iter = model.best_iteration if hasattr(model, 'best_iteration') else 300\n",
    "\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"model\": model,\n",
    "            \"metrics\": {\n",
    "                \"Train AUC\": round(train_roc, 4),\n",
    "                \"Test AUC\": round(test_roc, 4),\n",
    "                \"Train PR-AUC\": round(train_pr, 4),\n",
    "                \"Test PR-AUC\": round(test_pr, 4),\n",
    "                \"Best Iteration\": best_iter\n",
    "            }\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "# 2. æº–å‚™ Feature Groups (ç¢ºä¿ç„¡ç›®æ¨™è®Šæ•¸èˆ‡ID)\n",
    "# ===========================================================\n",
    "# å‡è¨­ identifier, all_cols, rfm_cols, dk_cols å·²ç¶“å®šç¾©å¥½\n",
    "identifier = ['transaction_id', 'client_id_x', 'card_id', 'card_number', 'cvv']\n",
    "exclude_cols = set(identifier) | {'is_fraud', 'date', 'year', 'time_block'}\n",
    "\n",
    "def clean_cols(cols):\n",
    "    return [c for c in cols if c not in exclude_cols]\n",
    "\n",
    "feature_groups = {\n",
    "    \"X_all\": clean_cols(all_cols),\n",
    "    \"X_rfm\": clean_cols(rfm_cols),\n",
    "    \"X_dk\": clean_cols(dk_cols),\n",
    "    \"X_all + X_rfm\": clean_cols(all_cols + rfm_cols),\n",
    "    \"X_all + X_dk\": clean_cols(all_cols + dk_cols),\n",
    "    \"X_rfm + X_dk\": clean_cols(rfm_cols + dk_cols),\n",
    "    \"X_all + X_rfm + X_dk\": clean_cols(all_cols + rfm_cols + dk_cols)\n",
    "}\n",
    "\n",
    "# ===========================================================\n",
    "# 3. ä¸»è¿´åœˆ (Block + Feature Groups)\n",
    "# ===========================================================\n",
    "xgb_results = {}\n",
    "\n",
    "# ç¢ºä¿ df_sorted å­˜åœ¨\n",
    "if 'df_sorted' not in locals():\n",
    "    raise ValueError(\"df_sorted is not defined. Please ensure your DataFrame is prepared.\")\n",
    "\n",
    "for block_id, block_df in df_sorted.groupby('time_block'):\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸš€ Running XGBoost for Block {block_id} (Year: {2010 + block_id*2} - {2011 + block_id*2})\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # --- è³‡æ–™åˆ‡åˆ† (Split) ---\n",
    "    block_df = block_df.sort_values('date')\n",
    "    split_index = int(len(block_df) * 0.8)\n",
    "    \n",
    "    train_raw = block_df.iloc[:split_index].copy()\n",
    "    test_raw  = block_df.iloc[split_index:].copy()\n",
    "\n",
    "    # --- é‡ç®— HighRiskMCC (Anti-Leakage) ---\n",
    "    fraud_rate = train_raw.groupby('mcc_code')['is_fraud'].mean()\n",
    "    high_risk_mcc = fraud_rate[fraud_rate > 0.02].index\n",
    "    \n",
    "    train_raw['HighRiskMCC'] = train_raw['mcc_code'].isin(high_risk_mcc).astype('uint8')\n",
    "    test_raw['HighRiskMCC']  = test_raw['mcc_code'].isin(high_risk_mcc).astype('uint8')\n",
    "\n",
    "    # åˆå§‹åŒ–çµæœå­˜æ”¾\n",
    "    xgb_results[block_id] = {}\n",
    "\n",
    "    # --- é‡å°æ¯å€‹ Feature Group è·‘æ¨¡å‹ ---\n",
    "    for group_name, feature_list in feature_groups.items():\n",
    "        \n",
    "        # ç¢ºä¿ç‰¹å¾µå­˜åœ¨æ–¼ DataFrame\n",
    "        valid_features = [f for f in feature_list if f in train_raw.columns]\n",
    "        \n",
    "        # ç°¡å–®æª¢æŸ¥\n",
    "        if not valid_features:\n",
    "            print(f\"   âš ï¸ Group: {group_name} - No valid features found. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"   ğŸ”¹ Group: {group_name} ({len(valid_features)} features)...\", end=\" \")\n",
    "\n",
    "        # åŸ·è¡Œ XGBoost\n",
    "        res = run_xgb_single(\n",
    "            train_raw, \n",
    "            test_raw, \n",
    "            feature_cols=valid_features, \n",
    "            dep_var=\"is_fraud\"\n",
    "        )\n",
    "\n",
    "        if res[\"status\"] == \"success\":\n",
    "            metrics = res[\"metrics\"]\n",
    "            print(f\"âœ… Done (Best Iter: {metrics['Best Iteration']})\")\n",
    "            print(f\"      Train AUC: {metrics['Train AUC']} | Test AUC: {metrics['Test AUC']}\")\n",
    "            print(f\"      Train PR : {metrics['Train PR-AUC']} | Test PR : {metrics['Test PR-AUC']}\")\n",
    "            \n",
    "            xgb_results[block_id][group_name] = metrics\n",
    "        else:\n",
    "            print(f\"âŒ Error: {res['message']}\")\n",
    "            xgb_results[block_id][group_name] = {\"error\": res['message']}\n",
    "\n",
    "print(\"\\nğŸ‰ All XGBoost models completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Block       Year  Feature Group  Train AUC  Test AUC  Train PR-AUC  \\\n",
      "0      0  2010-2011          X_all     0.9878       NaN        0.2217   \n",
      "1      0  2010-2011          X_rfm     0.7702       NaN        0.0084   \n",
      "2      0  2010-2011           X_dk     0.9743       NaN        0.1308   \n",
      "3      0  2010-2011  X_all + X_rfm     0.9793       NaN        0.0874   \n",
      "4      0  2010-2011   X_all + X_dk     0.9946       NaN        0.2525   \n",
      "\n",
      "   Test PR-AUC  Best Iter  \n",
      "0          0.0          0  \n",
      "1          0.0          0  \n",
      "2          0.0          0  \n",
      "3          0.0          0  \n",
      "4          0.0          0  \n",
      "\n",
      "=== XGBoost Train PR-AUC Matrix ===\n",
      "Year                  2010-2011  2012-2013  2014-2015  2016-2017  2018-2019\n",
      "Feature Group                                                              \n",
      "X_all                    0.2217     0.6527     0.9058     0.3223     0.9393\n",
      "X_all + X_dk             0.2525     0.6197     0.8607     0.5034     0.9234\n",
      "X_all + X_rfm            0.0874     0.7646     0.9680     0.5021     0.9738\n",
      "X_all + X_rfm + X_dk     0.2657     0.6893     0.9464     0.6182     0.9852\n",
      "X_dk                     0.1308     0.0213     0.0670     0.0423     0.4699\n",
      "X_rfm                    0.0084     0.0177     0.0415     0.0198     0.0233\n",
      "X_rfm + X_dk             0.0710     0.1107     0.3190     0.0473     0.8218\n",
      "\n",
      "\n",
      "\n",
      "=== XGBoost Test PR-AUC Matrix ===\n",
      "Year                  2010-2011  2012-2013  2014-2015  2016-2017  2018-2019\n",
      "Feature Group                                                              \n",
      "X_all                       0.0     0.7896     0.7663     0.0010     0.7450\n",
      "X_all + X_dk                0.0     0.7917     0.7556     0.0043     0.7101\n",
      "X_all + X_rfm               0.0     0.8077     0.7706     0.0009     0.7201\n",
      "X_all + X_rfm + X_dk        0.0     0.8042     0.7873     0.0032     0.8078\n",
      "X_dk                        0.0     0.0830     0.1115     0.0050     0.4616\n",
      "X_rfm                       0.0     0.0192     0.0195     0.0012     0.0056\n",
      "X_rfm + X_dk                0.0     0.2751     0.3050     0.0074     0.6335\n"
     ]
    }
   ],
   "source": [
    "# æ•´ç† XGBoost çµæœè¡¨æ ¼\n",
    "xgb_data_list = []\n",
    "\n",
    "for block_id, groups in xgb_results.items():\n",
    "    for group_name, metrics in groups.items():\n",
    "        if \"error\" in metrics: continue\n",
    "        row = {\n",
    "            \"Block\": block_id,\n",
    "            \"Year\": f\"{2010 + block_id*2}-{2011 + block_id*2}\",\n",
    "            \"Feature Group\": group_name,\n",
    "            \"Train AUC\": metrics.get(\"Train AUC\"),\n",
    "            \"Test AUC\": metrics.get(\"Test AUC\"),\n",
    "            \"Train PR-AUC\": metrics.get(\"Train PR-AUC\"),\n",
    "            \"Test PR-AUC\": metrics.get(\"Test PR-AUC\"),\n",
    "            \"Best Iter\": metrics.get(\"Best Iteration\")\n",
    "        }\n",
    "        xgb_data_list.append(row)\n",
    "\n",
    "df_xgb = pd.DataFrame(xgb_data_list)\n",
    "print(df_xgb.head())\n",
    "\n",
    "# è£½ä½œ Test AUC çŸ©é™£\n",
    "pivot_trxgb = df_xgb.pivot(index=\"Feature Group\", columns=\"Year\", values=\"Train PR-AUC\")\n",
    "pivot_testxgb = df_xgb.pivot(index=\"Feature Group\", columns=\"Year\", values=\"Test PR-AUC\")\n",
    "print(\"\\n=== XGBoost Train PR-AUC Matrix ===\")\n",
    "print(pivot_trxgb)\n",
    "print(\"\\n\")\n",
    "print(\"\\n=== XGBoost Test PR-AUC Matrix ===\")\n",
    "print(pivot_testxgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\n",
    "    'transaction_id',\n",
    "    'client_id_x',\n",
    "    'card_id',\n",
    "    'card_number',\n",
    "    'cvv'\n",
    "]\n",
    "\n",
    "train_df = train_df.drop(columns=cols_to_drop, errors='ignore')\n",
    "test_df  = test_df.drop(columns=cols_to_drop, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fit_full_logit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m result = \u001b[43mfit_full_logit\u001b[49m(train_df, test_df, dep_var=\u001b[33m\"\u001b[39m\u001b[33mis_fraud\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTrain AUC:\u001b[39m\u001b[33m\"\u001b[39m, result[\u001b[33m\"\u001b[39m\u001b[33mtrain_auc\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTest AUC:\u001b[39m\u001b[33m\"\u001b[39m, result[\u001b[33m\"\u001b[39m\u001b[33mtest_auc\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'fit_full_logit' is not defined"
     ]
    }
   ],
   "source": [
    "result = fit_full_logit(train_df, test_df, dep_var=\"is_fraud\")\n",
    "\n",
    "print(\"Train AUC:\", result[\"train_auc\"])\n",
    "print(\"Test AUC:\", result[\"test_auc\"])\n",
    "\n",
    "print(\"Train PR-AUC:\", result[\"train_prauc\"])\n",
    "print(\"Test PR-AUC:\", result[\"test_prauc\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, average_precision_score\n",
    "\n",
    "def fit_full_logit(train_df, test_df, dep_var=\"is_fraud\"):\n",
    "    # 1. split\n",
    "    X_train = train_df.drop(columns=[dep_var])\n",
    "    y_train = train_df[dep_var]\n",
    "\n",
    "    X_test = test_df.drop(columns=[dep_var])\n",
    "    y_test = test_df[dep_var]\n",
    "\n",
    "    # ğŸ¯ ç¢ºä¿ test æ¬„ä½é †åº = train æ¬„ä½é †åº\n",
    "    X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "    # 2. æ¨™æº–åŒ–\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # 3. Logistic Regression\n",
    "    model = LogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=1000,\n",
    "        n_jobs=-1,\n",
    "        class_weight=\"balanced\"\n",
    "    )\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # 4. probabilities\n",
    "    train_pred = model.predict_proba(X_train_scaled)[:, 1]\n",
    "    test_pred = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "    # 5. ROC AUC\n",
    "    train_auc = roc_auc_score(y_train, train_pred)\n",
    "    test_auc = roc_auc_score(y_test, test_pred)\n",
    "\n",
    "    # 6. PR AUC\n",
    "    train_prauc = average_precision_score(y_train, train_pred)\n",
    "    test_prauc = average_precision_score(y_test, test_pred)\n",
    "\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"scaler\": scaler,\n",
    "        \"train_pred\": train_pred,\n",
    "        \"test_pred\": test_pred,\n",
    "        \"train_auc\": train_auc,\n",
    "        \"test_auc\": test_auc,\n",
    "        \"train_prauc\": train_prauc,\n",
    "        \"test_prauc\": test_prauc\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train AUC: 0.8963198198876533\n",
      "Test AUC: 0.8912327090965326\n",
      "Train PR-AUC: 0.04883650297518295\n",
      "Test PR-AUC: 0.03905783518019472\n"
     ]
    }
   ],
   "source": [
    "result = fit_full_logit(train_df, test_df, dep_var=\"is_fraud\")\n",
    "\n",
    "print(\"Train AUC:\", result[\"train_auc\"])\n",
    "print(\"Test AUC:\", result[\"test_auc\"])\n",
    "\n",
    "print(\"Train PR-AUC:\", result[\"train_prauc\"])\n",
    "print(\"Test PR-AUC:\", result[\"test_prauc\"])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "virtual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
