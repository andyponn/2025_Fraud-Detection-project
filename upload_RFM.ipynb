{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01_import dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01-1_import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "#https://drive.google.com/drive/folders/18qV82fNY3IIWu3BRoGqm_LNgJzE8Akbr?usp=drive_link\n",
    "#base_dir = \"/Users/Andypon/10_‰∫§Â§ßÁ†îÁ©∂ÊâÄ/1141_01_Ê©üÂô®Â≠∏ÁøíËàáÈáëËûçÁßëÊäÄ/data\"\n",
    "base_dir= '/Users/andyw.p.chen/Documents/Project/datasets'\n",
    "#base_dir=  \"c:\\Users\\user\\Downloads\\datasets\"\n",
    "\n",
    "def load_json_to_df(filename: str) -> pd.DataFrame:\n",
    "    file_path = os.path.join(base_dir, filename)\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Â¶ÇÊûúÊòØ { \"target\": {id: value, ...} }\n",
    "    if isinstance(data, dict) and len(data) == 1 and isinstance(next(iter(data.values())), dict):\n",
    "        key, inner = next(iter(data.items()))\n",
    "        return pd.DataFrame(list(inner.items()), columns=[\"id\", key])\n",
    "\n",
    "    # dict of scalar\n",
    "    if isinstance(data, dict):\n",
    "        return pd.DataFrame([{\"code\": k, \"desc\": v} for k, v in data.items()])\n",
    "\n",
    "    # list of dict\n",
    "    elif isinstance(data, list):\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported JSON structure in {filename}: {type(data)}\")\n",
    "\n",
    "\n",
    "def load_csv_to_df(filename: str) -> pd.DataFrame:\n",
    "    \"\"\"ËÆÄÂèñ CSV ‰∏¶ËΩâÁÇ∫ DataFrame„ÄÇ\"\"\"\n",
    "    return pd.read_csv(os.path.join(base_dir, filename))\n",
    "\n",
    "# JSON Ë≥áÊñô\n",
    "##mcc_codes_df = load_json_to_df(\"mcc_codes.json\")\n",
    "train_fraud_labels_df = load_json_to_df(\"train_fraud_labels.json\")\n",
    "\n",
    "# CSV Ë≥áÊñô\n",
    "cards_df = load_csv_to_df(\"cards_data.csv\")\n",
    "transactions_df = load_csv_to_df(\"transactions_data.csv\")\n",
    "users_df = load_csv_to_df(\"users_data.csv\")\n",
    "\n",
    "# Á∞°ÂñÆÊ™¢Êü•\n",
    "#print(mcc_codes_df.head())\n",
    "#print(train_fraud_labels_df.head())\n",
    "#print(cards_df.head())\n",
    "#print(transactions_df.head())\n",
    "#print(users_df.apthead())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01-2_rename variable in each data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fraud_labels_df = train_fraud_labels_df.rename(columns={'id': 'transactions_id'})\n",
    "train_fraud_labels_df = train_fraud_labels_df.rename(columns={'target': 'is_fraud'})\n",
    "\n",
    "cards_df = cards_df.rename(columns={'id':'card_id'})\n",
    "\n",
    "users_df = users_df.rename(columns={'id':'client_id'})\n",
    "\n",
    "transactions_df = transactions_df.rename(columns={'mcc': 'mcc_code'})\n",
    "transactions_df = transactions_df.rename(columns={'id': 'transaction_id'})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01-3_ËÆäÊï∏ÂûãÊÖãÁµ±‰∏ÄÂèäÁº∫Â§±ÂÄºËôïÁêÜ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing_flags(df: pd.DataFrame, cols: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Âú® DataFrame ‰∏≠Â∞çÊåáÂÆöÊ¨Ñ‰ΩçÂª∫Á´ã missing flag Ê¨Ñ‰Ωç\n",
    "    flag=1 Ë°®Á§∫Áº∫Â§±ÂÄºÔºåflag=0 Ë°®Á§∫ÈùûÁº∫Â§±ÂÄº\n",
    "    \n",
    "    ÂèÉÊï∏\n",
    "    ----\n",
    "    df : pd.DataFrame\n",
    "        Ëº∏ÂÖ•ÁöÑË≥áÊñôÊ°Ü\n",
    "    cols : list\n",
    "        Ë¶ÅÊ™¢Êü•ÁöÑÊ¨Ñ‰ΩçÂêçÁ®±Ê∏ÖÂñÆ\n",
    "    \n",
    "    ÂõûÂÇ≥\n",
    "    ----\n",
    "    pd.DataFrame : Êñ∞ÁöÑË≥áÊñôÊ°Ü (Âê´Êñ∞Â¢ûÁöÑ flag Ê¨Ñ‰Ωç)\n",
    "    \"\"\"\n",
    "    for col in cols:\n",
    "        df[f\"{col}_missing_flag\"] = df[col].isna().astype(int)\n",
    "    return df\n",
    "\n",
    "transactions_df = add_missing_flags(transactions_df, [\"merchant_state\", \"zip\", \"errors\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##train_fraud_labels_df##\n",
    "train_fraud_labels_df[\"is_fraud\"]=train_fraud_labels_df[\"is_fraud\"].astype(\"category\") \n",
    "train_fraud_labels_df[\"transactions_id\"]=train_fraud_labels_df[\"transactions_id\"].astype(int) #Âêà‰ΩµË≥áÊñôÈúÄË¶Å\n",
    "\n",
    "##cards_df##\n",
    "cards_df[\"card_brand\"]=cards_df[\"card_brand\"].astype(\"category\") \n",
    "cards_df[\"card_type\"]=cards_df[\"card_type\"].astype(\"category\")\n",
    "#####‰∏çË¶ÅloadÈÄôË°å cards_df[\"expires\"]=pd.to_datetime(cards_df[\"expires\"], format=\"%m/%Y\")\n",
    "cards_df[\"expires\"] = pd.to_datetime(cards_df[\"expires\"], format=\"%m/%Y\").dt.to_period(\"M\")\n",
    "cards_df[\"has_chip\"]=cards_df[\"has_chip\"].astype(\"category\")\n",
    "\n",
    "cards_df['credit_limit'] = cards_df['credit_limit'].replace(r'[\\$,]', '', regex=True).astype(int)\n",
    "#####‰∏çË¶ÅloadÈÄôË°å cards_df[\"acct_open_date\"]=pd.to_datetime(cards_df[\"acct_open_date\"], format=\"%m/%Y\")\n",
    "cards_df[\"acct_open_date\"] = pd.to_datetime(cards_df[\"acct_open_date\"], format=\"%m/%Y\").dt.to_period(\"M\")\n",
    "#####‰∏çË¶ÅloadÈÄôË°å cards_df[\"year_pin_last_changed\"]=pd.to_datetime(cards_df[\"year_pin_last_changed\"], format=\"%Y\")\n",
    "cards_df[\"year_pin_last_changed\"] = pd.to_datetime(cards_df[\"year_pin_last_changed\"], format=\"%Y\").dt.to_period(\"Y\")\n",
    "cards_df[\"card_on_dark_web\"]=cards_df[\"card_on_dark_web\"].astype(\"category\") \n",
    "\n",
    "##users_df##\n",
    "users_df[\"birth_year\"] = pd.to_datetime(users_df[\"birth_year\"], format=\"%Y\").dt.to_period(\"Y\")\n",
    "users_df[\"birth_month\"] = pd.to_datetime(users_df[\"birth_month\"], format=\"%m\").dt.to_period(\"M\")\n",
    "users_df[\"gender\"]=users_df[\"gender\"].astype(\"category\") \n",
    "users_df['per_capita_income'] = users_df['per_capita_income'].replace(r'[\\$,]', '', regex=True).astype(int)\n",
    "users_df['yearly_income'] = users_df['yearly_income'].replace(r'[\\$,]', '', regex=True).astype(int)\n",
    "users_df['total_debt'] = users_df['total_debt'].replace(r'[\\$,]', '', regex=True).astype(int)\n",
    "\n",
    "##transactions_df##\n",
    "transactions_df[\"date\"] = pd.to_datetime(transactions_df[\"date\"])\n",
    "#ÊµÆÈªûÊï∏ËΩâÊï¥Êï∏ÂéüÂõ†Á¢∫ÂÆöÔºü\n",
    "transactions_df['amount'] = transactions_df['amount'].replace(r'[\\$,]', '', regex=True).astype(float).astype(int)\n",
    "##Ë≤†Êï∏ÂèñlogË™øÊàê1\n",
    "#transactions_df['amount'] = transactions_df['amount'].replace(r'[\\$,]', '', regex=True).astype(float)\n",
    "\n",
    "transactions_df[\"use_chip\"]=transactions_df[\"use_chip\"].astype(\"category\") \n",
    "\n",
    "transactions_df.loc[\n",
    "    transactions_df['merchant_city'].str.lower() == 'online',\n",
    "    'merchant_state'\n",
    "] = 'online'\n",
    "\n",
    "transactions_df.loc[\n",
    "    transactions_df['merchant_city'].str.lower() == 'online',\n",
    "    'zip'\n",
    "] = 20000 #ÂéüÊú¨ÊòØ-1\n",
    "## ÊàëÊ≤íÊúâÂÖ®ÈÉ®ÊîπÔºåÈÄôÊ®£ÂÆå‰πãÂæå‰ªçÊúâ89006Á≠ÜMissingÔºåÂâ©‰∏ãÈÉΩÊòØÂú®ÂúãÂ§ñ\n",
    "transactions_df['zip'] = transactions_df['zip'].fillna(10000) #ÂéüÊú¨ÊòØ-999\n",
    "transactions_df[\"zip\"]=transactions_df[\"zip\"].astype(\"int64\")\n",
    "\n",
    "transactions_df['errors'] = transactions_df['errors'].astype('category')\n",
    "transactions_df['errors'] = transactions_df['errors'].cat.add_categories('No_error').fillna('No_error')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cars one hot encoding\n",
    "##Áµ±‰∏ÄÈ°ûÂà•ËÆäÊï∏ËΩâdummy variable(Ë¶ÅÊ≥®ÊÑèÂÖ±Á∑öÊÄßÂïèÈ°åÔºåÊáâÂà™ÊéâÂÖ∂‰∏≠‰πã‰∏Ä)\n",
    "\n",
    "#card_type ÂéüÂßãÁ®ÆÈ°ûÔºöDebit_57%, Credit_33%, Debit(Prepaid)_9%\n",
    "#card_brand ÂéüÂßãÁ®ÆÈ°ûÔºöMasterCard_52%, Visa_38%, Amex_7%, Discovery_3%\n",
    "#has_chip ÂéüÂßãÁ®ÆÈ°ûÔºöYes_89%, No_11%\n",
    "#card_on_dark_web ÂéüÂßãÁ®ÆÈ°ûÔºöNo_0%\n",
    "cols_to_encode = ['card_type', 'card_brand', 'has_chip']\n",
    "cards_df[cols_to_encode] = cards_df[cols_to_encode].astype('category')\n",
    "dummies_cards = pd.get_dummies(\n",
    "    cards_df[cols_to_encode], \n",
    "    prefix=cols_to_encode, \n",
    "    dtype='uint8'\n",
    "    )\n",
    "cards_df = pd.concat([cards_df, dummies_cards], axis=1)\n",
    "\n",
    "#use_chip ÂéüÂßãÁ®ÆÈ°ûÔºöSwiped_52%, Chipe_36%, Online_12%\n",
    "dummies_use = pd.get_dummies(transactions_df['use_chip'], prefix='use_chip', dtype='uint8')\n",
    "transactions_df = pd.concat([transactions_df, dummies_use], axis=1)\n",
    "\n",
    "#gender ÂéüÂßãÁ®ÆÈ°ûÔºöFemale_51%, Male_49%\n",
    "dummies_gender = pd.get_dummies(users_df['gender'], prefix='gender', dtype='uint8')\n",
    "users_df = pd.concat([users_df, dummies_gender], axis=1)\n",
    "\n",
    "\n",
    "cards_df.drop(columns=[\"has_chip_NO\",\"has_chip\"], inplace=True)\n",
    "transactions_df.drop(columns=[\"use_chip\"], inplace=True)\n",
    "users_df.drop(columns=[\"gender_Female\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02_Ë≥áÊñôÊï¥‰ΩµÊàê‰∏ÄÂºµdataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02-1_Ë≥áÊñôÊï¥‰Ωµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transactions_df.loc[transactions_df[\"transaction_id\"] == 10649266] #transaction_id vs id\n",
    "\n",
    "#ÂéüÂßãË≥áÊñôÁ≠ÜÊï∏Ôºö13305915\n",
    "### transactions_df+train_fraud_labels_df      left ÊúÉÊúâ4390952 missing values\n",
    "merged = pd.merge(transactions_df, train_fraud_labels_df, left_on=\"transaction_id\", right_on=\"transactions_id\", how=\"outer\")\n",
    "### transactions_df train_fraud_labels_df(8914963) + users_df Â∞çÈÅéÂéª‰∏çÊúÉÊúâmissing values\n",
    "merged = pd.merge(merged,users_df , left_on=\"client_id\", right_on=\"client_id\", how=\"left\")\n",
    "### transactions_df train_fraud_labels_df users_df + cards_df Â∞çÈÅéÂéª‰∏çÊúÉÊúâmissing values\n",
    "merged = pd.merge(merged,cards_df , left_on=\"card_id\", right_on=\"card_id\", how=\"left\")\n",
    "\n",
    "#Âà™ÊéâÈáçË§áÁöÑcolumns\n",
    "merged.drop(columns=[\"transactions_id\"], inplace=True)\n",
    "merged.drop(columns=[\"client_id_y\"], inplace=True)\n",
    "\n",
    "## Âêà‰ΩµÂÆå‰πãÂæåÊúÄÂæåËôïÁêÜis_fraud(ÂéüÊúÉÊúâmissing valuesÂïèÈ°å)\n",
    "merged[\"is_fraud\"] = merged[\"is_fraud\"].astype(str)\n",
    "merged.loc[merged['is_fraud'].str.lower() == 'no','is_fraud'] = '0'\n",
    "merged.loc[merged['is_fraud'].str.lower() == 'yes','is_fraud'] = '1'\n",
    "merged[\"is_fraud\"] = pd.to_numeric(merged[\"is_fraud\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "merged = add_missing_flags(merged, [\"is_fraud\"])\n",
    "\n",
    "#merged.to_csv(\"merged.csv\", index=False)\n",
    "\n",
    "# ÂÖàÂà™Èô§‰∏çÈúÄË¶ÅÁöÑDataFrame‰ª•ÁØÄÁúÅË®òÊÜ∂È´î\n",
    "del transactions_df, users_df, cards_df, train_fraud_labels_df, cols_to_encode, dummies_cards, dummies_use, dummies_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_merged = merged.copy()\n",
    "#merged = backup_merged.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04_RFM features engineering model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04-1_Ë≥áÊñôÈÄ≤Ë°åËÆäÊï∏ËΩâÊèõ‰ª•Ê±ÇÊ®°ÂûãÈÖçÈ£æÊõ¥‰Ω≥Ë°®Áèæ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ÊúâÂá∫‰∫ãÂÜçË∂ïÂø´ÂõûÂæ©ÂéüÁãÄ\n",
    "merged = backup_merged.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Á¢∫‰øùÊó•ÊúüÊòØ datetime ‰∏¶ÊéíÂ∫è\n",
    "merged['date'] = pd.to_datetime(merged['date'])\n",
    "merged = merged.sort_values(by=['client_id_x', 'date']).reset_index(drop=True)\n",
    "\n",
    "# --- RecencyInterval ---\n",
    "merged['RecencyInterval'] = merged.groupby('client_id_x')['date'].diff().dt.total_seconds().fillna(0)/60\n",
    "\n",
    "# --- TxnFrequency for multiple windows (ÂêëÈáèÂåñÊªëÂãïÁ™óÂè£) ---\n",
    "window_days = [7, 30, 60, 90]\n",
    "for w in window_days:\n",
    "    merged[f'TxnFrequency_{w}d'] = 0\n",
    "\n",
    "def compute_freq_vectorized(dates, windows):\n",
    "    \"\"\"ÂêëÈáèÂåñË®àÁÆóÊØèÁ≠Ü‰∫§ÊòìÂú®ÊØèÂÄã window ÂÖßÁöÑ‰∫§ÊòìÊï∏\"\"\"\n",
    "    n = len(dates)\n",
    "    dates_int = dates.values.astype('datetime64[D]').astype(int)\n",
    "    res = {w: np.zeros(n, dtype=int) for w in windows}\n",
    "    for w in windows:\n",
    "        left = 0\n",
    "        counts = np.zeros(n, dtype=int)\n",
    "        for right in range(n):\n",
    "            while dates_int[right] - dates_int[left] > w:\n",
    "                left += 1\n",
    "            counts[right] = right - left + 1\n",
    "        res[w] = counts\n",
    "    return res\n",
    "\n",
    "# ÂàÜÁµÑË®àÁÆó\n",
    "for cid, g in merged.groupby('client_id_x', sort=False):\n",
    "    freq_dict = compute_freq_vectorized(g['date'], window_days)\n",
    "    for w in window_days:\n",
    "        merged.loc[g.index, f'TxnFrequency_{w}d'] = freq_dict[w]\n",
    "\n",
    "# --- AmtDelta ---\n",
    "merged['prev_amount'] = merged.groupby('client_id_x')['amount'].shift(1)\n",
    "merged['AmtDelta'] = merged['amount'] - merged['prev_amount']\n",
    "merged['AmtDelta'] = merged['AmtDelta'].fillna(0)\n",
    "merged.drop(columns='prev_amount', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# US region mapping\n",
    "us_region_map = {\n",
    "    'Northeast': ['NY','NJ','PA','MA','CT','RI','NH','VT','ME'],\n",
    "    'Midwest': ['IL','OH','MI','IN','WI','MN','IA','MO','ND','SD','NE','KS'],\n",
    "    'South': ['FL','GA','SC','NC','AL','MS','LA','TX','OK','TN','KY','VA','WV','AR','MD','DE','DC'],\n",
    "    'West': ['CA','WA','OR','NV','AZ','NM','CO','UT','ID','MT','WY','AK','HI'],\n",
    "}\n",
    "continent_map = {\n",
    "    'Europe': [ ... ],  # ÂéüÊú¨ continent_map['Europe'] ÂèØÁõ¥Êé•‰ΩøÁî®\n",
    "    'Online': ['online','AA']\n",
    "}\n",
    "\n",
    "us_region_lookup = {state: region for region, states in us_region_map.items() for state in states}\n",
    "\n",
    "# --- ÂêëÈáèÂåñ location ÁâπÂæµ ---\n",
    "merged['merchant_online'] = merged['merchant_state'].eq('online').astype('uint8')\n",
    "merged['merchant_us'] = merged['merchant_state'].isin(us_region_lookup.keys()).astype('uint8')\n",
    "merged['merchant_eu'] = merged['merchant_state'].isin(continent_map['Europe']).astype('uint8')\n",
    "merged['merchant_others'] = (~merged[['merchant_online','merchant_us','merchant_eu']].any(axis=1)).astype('uint8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- È¶ñÊ¨°‰∫§ÊòìÊ®ôË®ò ---\n",
    "merged['FirstTxnInRegion'] = (~merged.duplicated(subset=['client_id_x', 'merchant_state'])).astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DifferentState\n",
    "merged['prev_state']=(merged\n",
    "                     .groupby('client_id_x')['merchant_state']\n",
    "                     .shift(1))\n",
    "\n",
    "merged['DifferentState'] = (\n",
    "    (merged['merchant_state'] != merged['prev_state'])\n",
    "    & merged['prev_state'].notna()\n",
    ").astype(int)\n",
    "\n",
    "merged = merged.drop(columns=['prev_state'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged[[\"card_id\",\"card_number\"]]\n",
    "import numpy as np\n",
    "from scipy import stats \n",
    "\n",
    "# === (1) logËΩâÊèõ ===\n",
    "merged['amount'] = np.where(merged['amount'] < 0, 0, merged['amount'])  # Ë≤†Êï∏ËÆä 0\n",
    "merged['amount'] = np.log(merged['amount'] + 1)  \n",
    "\n",
    "# === (3) Âπ≥ÊñπÊ†πËΩâÊèõ ===\n",
    "merged['credit_limit']=np.sqrt(merged['credit_limit'])\n",
    "merged['total_debt']=np.sqrt(merged['total_debt'])\n",
    "\n",
    "# === (3) Á´ãÊñπÊ†πËΩâÊèõ ===\n",
    "merged['yearly_income']=np.cbrt(merged['yearly_income'])\n",
    "merged['per_capita_income']=np.cbrt(merged['per_capita_income'])\n",
    "\n",
    "## Box-Cox Transformation\n",
    "###merged['yearly_income'], fitted_lambda = stats.boxcox(merged['yearly_income'])\n",
    "\n",
    "# === (5) Yeo‚ÄìJohnson ËΩâÊèõÔºàÂèØËôïÁêÜË≤†ÂÄºÔºâ ===\n",
    "###merged['per_capita_income'], lambdaValue =stats.yeojohnson(merged['per_capita_income'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04-2_ÂàÜÂâ≤Ë®ìÁ∑¥ÈõÜÂèäÊ∏¨Ë©¶ÈõÜ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Block 0\n",
      "\n",
      "Processing Block 1\n",
      "\n",
      "Processing Block 2\n",
      "\n",
      "Processing Block 3\n",
      "\n",
      "Processing Block 4\n",
      "\n",
      "Block 0\n",
      "Train fraud count:\n",
      "is_fraud\n",
      "0    1353355\n",
      "1       2610\n",
      "Name: count, dtype: Int64\n",
      "Test fraud count:\n",
      "is_fraud\n",
      "0    338992\n",
      "Name: count, dtype: Int64\n",
      "\n",
      "Block 1\n",
      "Train fraud count:\n",
      "is_fraud\n",
      "0    1433028\n",
      "1       1152\n",
      "Name: count, dtype: Int64\n",
      "Test fraud count:\n",
      "is_fraud\n",
      "0    357437\n",
      "1      1108\n",
      "Name: count, dtype: Int64\n",
      "\n",
      "Block 2\n",
      "Train fraud count:\n",
      "is_fraud\n",
      "0    1474582\n",
      "1       1655\n",
      "Name: count, dtype: Int64\n",
      "Test fraud count:\n",
      "is_fraud\n",
      "0    367862\n",
      "1      1198\n",
      "Name: count, dtype: Int64\n",
      "\n",
      "Block 3\n",
      "Train fraud count:\n",
      "is_fraud\n",
      "0    1493588\n",
      "1       2448\n",
      "Name: count, dtype: Int64\n",
      "Test fraud count:\n",
      "is_fraud\n",
      "0    373838\n",
      "1       172\n",
      "Name: count, dtype: Int64\n",
      "\n",
      "Block 4\n",
      "Train fraud count:\n",
      "is_fraud\n",
      "0    1367157\n",
      "1       2393\n",
      "Name: count, dtype: Int64\n",
      "Test fraud count:\n",
      "is_fraud\n",
      "0    341792\n",
      "1       596\n",
      "Name: count, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "# --- ÈÅ∏ÂèñÊï∏ÂÄºÂûãËÆäÊï∏ ---\n",
    "num_cols = merged.select_dtypes(include=['int64', 'float64','uint8','datetime64[ns]']).columns\n",
    "df2 = merged[num_cols]\n",
    "\n",
    "# --- dropna ---\n",
    "df_cleaned = df2.dropna()\n",
    "del df2\n",
    "\n",
    "# --- ÈÅøÂÖçÂÖ±Á∑öÊÄß ---\n",
    "df_cleaned.drop(columns=[\"is_fraud_missing_flag\",\"card_type_Debit (Prepaid)\", \n",
    "                         \"card_brand_Discover\", \"use_chip_Online Transaction\"], inplace=True)\n",
    "\n",
    "# --- Á¢∫‰øù date Ê¨Ñ‰ΩçÂú® df_cleaned ‰∏≠ ---\n",
    "if 'date' not in df_cleaned.columns:\n",
    "    df_cleaned['date'] = merged.loc[df_cleaned.index, 'date']\n",
    "\n",
    "# --- ‰æùÊôÇÈñìÊéíÂ∫è ---\n",
    "df_sorted = df_cleaned.sort_values('date')\n",
    "df_sorted['year'] = df_sorted['date'].dt.year\n",
    "\n",
    "# 2010‚Äì2011 ‚Üí block 0\n",
    "# 2012‚Äì2013 ‚Üí block 1\n",
    "# ...\n",
    "# 2018‚Äì2019 ‚Üí block 4ÔºàÂ¶ÇÊûú‰Ω†ÁúüÁöÑÊòØ 2010‚Äì2019 ÂÖ± 10 Âπ¥ÔºåÊúÉÊúâ 5 ÂÄã blockÔºâ\n",
    "df_sorted['time_block'] = (df_sorted['year'] - 2010) // 2\n",
    "\n",
    "\n",
    "#Â∞çÊØèÂÄãÊôÇÈñìblockÂÅöÂàáÂâ≤\n",
    "train_list = []\n",
    "test_list = []\n",
    "\n",
    "for block_id, block_df in df_sorted.groupby('time_block'):\n",
    "    print(f\"\\nProcessing Block {block_id}\")\n",
    "\n",
    "    block_df = block_df.sort_values('date')\n",
    "    split_index = int(len(block_df) * 0.8)\n",
    "\n",
    "    train_block = block_df.iloc[:split_index].copy()\n",
    "    test_block  = block_df.iloc[split_index:].copy()\n",
    "\n",
    "    # -----------------------------\n",
    "    # 1Ô∏è‚É£ Áî®„ÄåË©≤ block ÁöÑ train„ÄçÁÆó fraud rate\n",
    "    # -----------------------------\n",
    "    fraud_rate = (\n",
    "        train_block\n",
    "        .groupby('mcc_code')['is_fraud']\n",
    "        .mean()\n",
    "    )\n",
    "\n",
    "    high_risk_mcc = fraud_rate[fraud_rate > 0.02].index\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2Ô∏è‚É£ Â•óÁî®Âà∞Ë©≤ block ÁöÑ train / test\n",
    "    # -----------------------------\n",
    "    train_block['HighRiskMCC'] = train_block['mcc_code'].isin(high_risk_mcc).astype('uint8')\n",
    "    test_block['HighRiskMCC']  = test_block['mcc_code'].isin(high_risk_mcc).astype('uint8')\n",
    "\n",
    "    train_list.append(train_block)\n",
    "    test_list.append(test_block)\n",
    "\n",
    "\n",
    "train_df = pd.concat(train_list).drop(columns=['date', 'year', 'time_block'])\n",
    "test_df  = pd.concat(test_list).drop(columns=['date', 'year', 'time_block'])\n",
    "\n",
    "\n",
    "#ÊØî‰æãÊ™¢Êü•\n",
    "for block_id in sorted(df_sorted['time_block'].unique()):\n",
    "    print(f\"\\nBlock {block_id}\")\n",
    "    print(\"Train fraud count:\")\n",
    "    print(train_df.loc[df_sorted['time_block'] == block_id, 'is_fraud'].value_counts())\n",
    "    print(\"Test fraud count:\")\n",
    "    print(test_df.loc[df_sorted['time_block'] == block_id, 'is_fraud'].value_counts())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "result = pd.DataFrame(columns=[\n",
    "    \"Model\", \"Features\", \n",
    "    \"Train AUC\", \"Test AUC\", \n",
    "    \"Train PR AUC\", \"Test PR AUC\"\n",
    "])\n",
    "\n",
    "'''\n",
    "\n",
    "# ALL features\n",
    "\n",
    "all_cols = ['transaction_id', 'date', 'client_id_x', 'card_id', 'amount',\n",
    "       'merchant_id', 'merchant_city', 'merchant_state', 'zip', 'mcc_code',\n",
    "       'errors', 'merchant_state_missing_flag', 'zip_missing_flag',\n",
    "       'errors_missing_flag', 'use_chip_Chip Transaction',\n",
    "       'use_chip_Online Transaction', 'use_chip_Swipe Transaction',\n",
    "       'current_age', 'retirement_age', 'birth_year', 'birth_month', 'gender',\n",
    "       'address', 'latitude', 'longitude', 'per_capita_income',\n",
    "       'yearly_income', 'total_debt', 'credit_score', 'num_credit_cards',\n",
    "       'gender_Male', 'card_brand', 'card_type', 'card_number', 'expires',\n",
    "       'cvv', 'num_cards_issued', 'credit_limit', 'acct_open_date',\n",
    "       'year_pin_last_changed', 'card_on_dark_web', 'card_type_Credit',\n",
    "       'card_type_Debit', 'card_type_Debit (Prepaid)', 'card_brand_Amex',\n",
    "       'card_brand_Discover', 'card_brand_Mastercard', 'card_brand_Visa',\n",
    "       'has_chip_YES', 'is_fraud_missing_flag']\n",
    "\n",
    "VIF_col = [\"is_fraud_missing_flag\",\"card_type_Debit (Prepaid)\", \n",
    "                         \"card_brand_Discover\", \"use_chip_Online Transaction\"]\n",
    "\n",
    "identifier = [\n",
    "    'transaction_id',\n",
    "    'client_id_x',\n",
    "    'card_id',\n",
    "    'card_number',\n",
    "    'cvv'\n",
    "]\n",
    "\n",
    "set_VIF = set(VIF_col)\n",
    "set_identifier =set(identifier)\n",
    "exclude_cols = set(VIF_col) | set(identifier)\n",
    "all_cols = [x for x in all_cols if x not in exclude_cols]\n",
    "\n",
    "\n",
    "# RFM features\n",
    "rfm_cols = [\n",
    "    'RecencyInterval', 'TxnFrequency_7d','TxnFrequency_30d',\n",
    "    'TxnFrequency_60d', 'TxnFrequency_90d','AmtDelta'\n",
    "]\n",
    "\n",
    "# DK features\n",
    "dk_cols = [\n",
    "    'merchant_online', 'merchant_us', 'merchant_eu', 'merchant_others',\n",
    "    'FirstTxnInRegion','HighRiskMCC','DifferentState'\n",
    "]\n",
    "\n",
    "# Grouping\n",
    "feature_groups = {\n",
    "    \"X_all\": all_cols,\n",
    "    \"X_rfm\": rfm_cols,\n",
    "    \"X_dk\": dk_cols,\n",
    "    \"X_all + X_rfm\": all_cols + rfm_cols,\n",
    "    \"X_all + X_dk\": all_cols + dk_cols,\n",
    "    \"X_rfm + X_dk\": rfm_cols + dk_cols,\n",
    "    \"X_all + X_rfm + X_dk\": all_cols + rfm_cols + dk_cols\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04-3 ËÄÅÂ∏´Âª∫Ë≠∞ÊàëÂÄëÂÖàÁúÅÁï• Assumption: Avoid Multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ËôïÁêÜÂÖ±Á∑öÊÄß\n",
    "train_df.drop(columns=[\"per_capita_income\"], inplace=True)\n",
    "train_df.drop(columns=[\"use_chip_Chip Transaction\",\"merchant_state_missing_flag\",\"zip_missing_flag\"], inplace=True)           \n",
    "train_df.drop(columns=[\"card_brand_Visa\" ,\"card_brand_Amex\",\"card_type_Credit\"], inplace=True)\n",
    "#ÂÜçÈáçË∑ë‰∏ÄÊ¨°VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.drop(columns=[\"per_capita_income\"], inplace=True)\n",
    "test_df.drop(columns=[\"use_chip_Chip Transaction\",\"merchant_state_missing_flag\",\"zip_missing_flag\"], inplace=True)           \n",
    "test_df.drop(columns=[\"card_brand_Visa\" ,\"card_brand_Amex\",\"card_type_Credit\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04-stepwise selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, confusion_matrix\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "# Stepwise Logistic Regression with K control & tables\n",
    "# ===========================================================\n",
    "def stepwise_logit_with_k_tables(\n",
    "    train_df,\n",
    "    test_df,\n",
    "    dep_var=\"is_fraud\",\n",
    "    k=314657018,\n",
    "    threshold_in=0.05,\n",
    "    threshold_out=0.10,\n",
    "    verbose=True\n",
    "):\n",
    "\n",
    "    y_train = train_df[dep_var]\n",
    "    X_train = train_df.drop(columns=[dep_var])\n",
    "    y_test = test_df[dep_var]\n",
    "    X_test = test_df.drop(columns=[dep_var])\n",
    "\n",
    "    included = []\n",
    "    step = 0\n",
    "    full_mode = (k == 314657018)\n",
    "\n",
    "    # ======================================================\n",
    "    # üî∏ ÁâπÊÆäÊÉÖÊ≥ÅÔºök = 0ÔºàIntercept onlyÔºâ\n",
    "    # ======================================================\n",
    "    if k == 0:\n",
    "        X_const = sm.add_constant(np.ones(len(y_train)), has_constant=\"add\")\n",
    "        final_model = sm.Logit(y_train, X_const).fit(disp=False)\n",
    "\n",
    "        ll_full = final_model.llf\n",
    "        ll_null = ll_full\n",
    "\n",
    "        overall_fit = pd.DataFrame({\n",
    "            \"Measure\": [\"-2 Log Likelihood (‚àí2LL) value\"],\n",
    "            \"Value\": [round(-2 * ll_full, 3)],\n",
    "            \"Change_from_Base\": [\"\"],\n",
    "            \"Change_pvalue\": [\"\"]\n",
    "        })\n",
    "\n",
    "        coef_df = pd.DataFrame({\n",
    "            \"Independent Variable\": [\"const\"],\n",
    "            \"B\": final_model.params.values,\n",
    "            \"Std. Error\": final_model.bse.values,\n",
    "            \"Wald\": [np.nan],\n",
    "            \"df\": [1],\n",
    "            \"Sig.\": [\"\"],\n",
    "            \"Exp(B)\": np.exp(final_model.params.values)\n",
    "        })\n",
    "\n",
    "        not_in_eq_df = pd.DataFrame({\n",
    "            \"Independent Variable\": X_train.columns,\n",
    "            \"Score Statistic (LRT)\": [None] * len(X_train.columns),\n",
    "            \"Significance\": [None] * len(X_train.columns)\n",
    "        })\n",
    "\n",
    "        if verbose:\n",
    "            print(\"‚úÖ Stepwise completed with 0 variables (Intercept only).\")\n",
    "\n",
    "        return overall_fit, coef_df, not_in_eq_df, final_model\n",
    "\n",
    "    # ======================================================\n",
    "    # üîÅ Regular Stepwise (Forward + Backward)\n",
    "    # ======================================================\n",
    "    while True:\n",
    "        step += 1\n",
    "        changed = False\n",
    "\n",
    "        # ---------- Forward ----------\n",
    "        excluded = list(set(X_train.columns) - set(included))\n",
    "        new_pvals = pd.Series(index=excluded, dtype=float)\n",
    "\n",
    "        for new_var in excluded:\n",
    "            try:\n",
    "                X_try = sm.add_constant(\n",
    "                    X_train[included + [new_var]],\n",
    "                    has_constant=\"add\"\n",
    "                )\n",
    "                model = sm.Logit(y_train, X_try).fit(disp=False)\n",
    "                new_pvals[new_var] = model.pvalues[new_var]\n",
    "            except Exception:\n",
    "                new_pvals[new_var] = np.nan\n",
    "\n",
    "        if new_pvals.empty:\n",
    "            break\n",
    "\n",
    "        best_pval = new_pvals.min()\n",
    "        if best_pval < threshold_in:\n",
    "            best_var = new_pvals.idxmin()\n",
    "            included.append(best_var)\n",
    "            changed = True\n",
    "            if verbose:\n",
    "                print(f\"üü¢ Step {step}: Forward add {best_var} (p={best_pval:.4g})\")\n",
    "\n",
    "        # ---------- Backward ----------\n",
    "        if included:\n",
    "            X_curr = sm.add_constant(X_train[included], has_constant=\"add\")\n",
    "            model = sm.Logit(y_train, X_curr).fit(disp=False)\n",
    "            pvalues = model.pvalues.drop(\"const\", errors=\"ignore\")\n",
    "\n",
    "            if not pvalues.empty:\n",
    "                worst_pval = pvalues.max()\n",
    "                if worst_pval > threshold_out:\n",
    "                    worst_var = pvalues.idxmax()\n",
    "                    included.remove(worst_var)\n",
    "                    changed = True\n",
    "                    if verbose:\n",
    "                        print(f\"üî¥ Step {step}: Backward remove {worst_var} (p={worst_pval:.4g})\")\n",
    "\n",
    "        if not changed:\n",
    "            if verbose:\n",
    "                print(f\"‚ö™ Step {step}: No change ‚Äî stop.\")\n",
    "            break\n",
    "\n",
    "        if not full_mode and len(included) >= k:\n",
    "            if verbose:\n",
    "                print(f\"üü° Reached k={k}, stop.\")\n",
    "            break\n",
    "\n",
    "    # ======================================================\n",
    "    # üîπ Final Model\n",
    "    # ======================================================\n",
    "    if len(included) == 0:\n",
    "        X_final = sm.add_constant(np.ones(len(y_train)), has_constant=\"add\")\n",
    "    else:\n",
    "        X_final = sm.add_constant(X_train[included], has_constant=\"add\")\n",
    "\n",
    "    try:\n",
    "        final_model = sm.Logit(y_train, X_final).fit(disp=False)\n",
    "    except ValueError as e:\n",
    "        if verbose:\n",
    "            print(\"‚ö†Ô∏è Final model failed due to separation. Falling back to intercept-only.\")\n",
    "        X_const = sm.add_constant(np.ones(len(y_train)), has_constant=\"add\")\n",
    "        final_model = sm.Logit(y_train, X_const).fit(disp=False)\n",
    "        included = []   # ÈùûÂ∏∏ÈáçË¶ÅÔºöÂêåÊ≠•Ê∏ÖÁ©∫\n",
    "\n",
    "\n",
    "    ll_full = final_model.llf\n",
    "    ll_null = sm.Logit(\n",
    "        y_train,\n",
    "        sm.add_constant(np.ones(len(y_train)), has_constant=\"add\")\n",
    "    ).fit(disp=False).llf\n",
    "\n",
    "    # ======================================================\n",
    "    # 1Ô∏è‚É£ Overall Model Fit\n",
    "    # ======================================================\n",
    "    ll_diff = -2 * (ll_null - ll_full)\n",
    "    df_diff = len(final_model.params) - 1\n",
    "    p_value = stats.chi2.sf(ll_diff, df_diff)\n",
    "\n",
    "    overall_fit = pd.DataFrame({\n",
    "        \"Measure\": [\n",
    "            \"-2 Log Likelihood (‚àí2LL) value\",\n",
    "            \"Cox and Snell R2\",\n",
    "            \"Nagelkerke R2\",\n",
    "            \"Pseudo R2 (McFadden)\",\n",
    "            \"Hosmer-Lemeshow œá2\"\n",
    "        ],\n",
    "        \"Value\": [\n",
    "            round(-2 * ll_full, 3),\n",
    "            round(1 - np.exp((2 / len(y_train)) * (ll_null - ll_full)), 3),\n",
    "            round(\n",
    "                (1 - np.exp((2 / len(y_train)) * (ll_null - ll_full))) /\n",
    "                (1 - np.exp(2 * ll_null / len(y_train))),\n",
    "                3\n",
    "            ),\n",
    "            round(1 - (ll_full / ll_null), 3),\n",
    "            round(ll_diff, 3)\n",
    "        ],\n",
    "        \"Change_from_Base\": [round(ll_diff, 3), \"\", \"\", \"\", \"\"],\n",
    "        \"Change_pvalue\": [round(p_value, 4), \"\", \"\", \"\", \"\"]\n",
    "    })\n",
    "\n",
    "    # ======================================================\n",
    "    # 2Ô∏è‚É£ Variables in the Equation\n",
    "    # ======================================================\n",
    "    coef_df = pd.DataFrame({\n",
    "        \"Independent Variable\": final_model.params.index,\n",
    "        \"B\": final_model.params.values,\n",
    "        \"Std. Error\": final_model.bse.values,\n",
    "        \"Wald\": (final_model.params / final_model.bse) ** 2,\n",
    "        \"df\": 1,\n",
    "        \"Sig.\": final_model.pvalues.values,\n",
    "        \"Exp(B)\": np.exp(final_model.params.values)\n",
    "    }).reset_index(drop=True)\n",
    "\n",
    "    # ======================================================\n",
    "    # 3Ô∏è‚É£ Variables Not in Equation\n",
    "    # ======================================================\n",
    "    excluded_vars = [v for v in X_train.columns if v not in included]\n",
    "    not_in_eq = []\n",
    "\n",
    "    for var in excluded_vars:\n",
    "        try:\n",
    "            X_tmp = sm.add_constant(\n",
    "                X_train[included + [var]],\n",
    "                has_constant=\"add\"\n",
    "            )\n",
    "            temp_model = sm.Logit(y_train, X_tmp).fit(disp=False)\n",
    "            lr_stat = -2 * (final_model.llf - temp_model.llf)\n",
    "            p_val = stats.chi2.sf(lr_stat, 1)\n",
    "            not_in_eq.append({\n",
    "                \"Independent Variable\": var,\n",
    "                \"Score Statistic (LRT)\": round(lr_stat, 3),\n",
    "                \"Significance\": round(p_val, 4)\n",
    "            })\n",
    "        except Exception:\n",
    "            not_in_eq.append({\n",
    "                \"Independent Variable\": var,\n",
    "                \"Score Statistic (LRT)\": None,\n",
    "                \"Significance\": None\n",
    "            })\n",
    "\n",
    "    not_in_eq_df = pd.DataFrame(not_in_eq)\n",
    "\n",
    "    # ======================================================\n",
    "    # üîπ Prediction matrices (safe)\n",
    "    # ======================================================\n",
    "    if len(included) == 0:\n",
    "        X_train_pred = sm.add_constant(np.ones(len(y_train)), has_constant=\"add\")\n",
    "        X_test_pred = sm.add_constant(np.ones(len(y_test)), has_constant=\"add\")\n",
    "    else:\n",
    "        X_train_pred = sm.add_constant(X_train[included], has_constant=\"add\")\n",
    "        X_test_pred = sm.add_constant(X_test[included], has_constant=\"add\")\n",
    "\n",
    "    train_probs = final_model.predict(X_train_pred)\n",
    "    test_probs = final_model.predict(X_test_pred)\n",
    "\n",
    "    train_pred = (train_probs > 0.5).astype(int)\n",
    "    test_pred = (test_probs > 0.5).astype(int)\n",
    "\n",
    "    train_acc = (train_pred == y_train).mean()\n",
    "    test_acc = (test_pred == y_test).mean()\n",
    "\n",
    "    train_auc = roc_auc_score(y_train, train_probs)\n",
    "    test_auc = roc_auc_score(y_test, test_probs)\n",
    "    train_pr_auc = average_precision_score(y_train, train_probs)\n",
    "    test_pr_auc = average_precision_score(y_test, test_probs)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n‚úÖ Stepwise completed with {len(included)} variables\")\n",
    "        print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "        print(f\"Test Accuracy:  {test_acc:.4f}\")\n",
    "        print(f\"Train AUC:     {train_auc:.4f}\")\n",
    "        print(f\"Test AUC:      {test_auc:.4f}\")\n",
    "        print(f\"Train PR-AUC:  {train_pr_auc:.4f}\")\n",
    "        print(f\"Test PR-AUC:   {test_pr_auc:.4f}\")\n",
    "\n",
    "    return overall_fit, coef_df, not_in_eq_df, final_model\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "# Classification Table\n",
    "# ===========================================================\n",
    "def classification_table(model, df, target_col=\"is_fraud\", cutoff=0.0015):\n",
    "\n",
    "    y_true = df[target_col].astype(int)\n",
    "    features = model.params.index.drop(\"const\", errors=\"ignore\")\n",
    "\n",
    "    if len(features) == 0:\n",
    "        X = sm.add_constant(np.ones(len(df)), has_constant=\"add\")\n",
    "    else:\n",
    "        X = sm.add_constant(df[features], has_constant=\"add\")\n",
    "\n",
    "    y_pred_prob = model.predict(X)\n",
    "    y_pred = (y_pred_prob >= cutoff).astype(int)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[1, 0])\n",
    "    TP, FN, FP, TN = cm.ravel()\n",
    "\n",
    "    fraud_total = TP + FN\n",
    "    normal_total = FP + TN\n",
    "\n",
    "    fraud_correct = TP / fraud_total if fraud_total else 0\n",
    "    normal_correct = TN / normal_total if normal_total else 0\n",
    "    overall_correct = (TP + TN) / (fraud_total + normal_total)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"Actual Group\": [\"Fraud (1)\", \"Normal (0)\", \"Total\"],\n",
    "        \"Predicted Fraud (1)\": [TP, FP, TP + FP],\n",
    "        \"Predicted Normal (0)\": [FN, TN, FN + TN],\n",
    "        \"Total\": [fraud_total, normal_total, fraud_total + normal_total],\n",
    "        \"% Correct\": [\n",
    "            round(fraud_correct * 100, 1),\n",
    "            round(normal_correct * 100, 1),\n",
    "            round(overall_correct * 100, 1)\n",
    "        ],\n",
    "        \"cutoff\": [cutoff, \"\", \"\"],\n",
    "        \"F1 Score\": [round(2 * TP / (2 * TP + FP + FN), 4), \"\", \"\"]\n",
    "    })\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "# Feature Selector\n",
    "# ===========================================================\n",
    "def select_features(df, feature_list, dep_var=\"is_fraud\"):\n",
    "\n",
    "    cols = [c for c in feature_list if c in df.columns]\n",
    "    df_sub = df[cols + [dep_var]]\n",
    "\n",
    "    df_sub = df_sub.select_dtypes(include=[np.number])\n",
    "    df_sub = df_sub.astype(float)\n",
    "\n",
    "    return df_sub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "üöÄ Running Block 0 (Year: 2010 - 2011)\n",
      "========================================\n",
      "\n",
      "   üîπ Processing Group: X_all\n",
      "      ‚ö†Ô∏è Warning: {'year_pin_last_changed', 'errors', 'card_type', 'card_on_dark_web', 'expires', 'merchant_city', 'address', 'date', 'birth_month', 'birth_year', 'card_brand', 'gender', 'merchant_state', 'acct_open_date'} not found in dataset. Skipping them.\n",
      "      ‚ö†Ô∏è Dropping constant columns: ['use_chip_Chip Transaction']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/discrete/discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/discrete/discrete_model.py:2443: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.sum(np.log(self.cdf(q * linpred)))\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/discrete/discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/discrete/discrete_model.py:2443: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.sum(np.log(self.cdf(q * linpred)))\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/discrete/discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/discrete/discrete_model.py:2443: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.sum(np.log(self.cdf(q * linpred)))\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/discrete/discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/discrete/discrete_model.py:2443: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.sum(np.log(self.cdf(q * linpred)))\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/discrete/discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/discrete/discrete_model.py:2443: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.sum(np.log(self.cdf(q * linpred)))\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/discrete/discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/discrete/discrete_model.py:2443: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.sum(np.log(self.cdf(q * linpred)))\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/discrete/discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/discrete/discrete_model.py:2443: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.sum(np.log(self.cdf(q * linpred)))\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    }
   ],
   "source": [
    "# Âª∫Á´ã‰∏ÄÂÄãÂ≠òÊîæÁµêÊûúÁöÑÂ≠óÂÖ∏\n",
    "results = {}\n",
    "\n",
    "# ÁÇ∫‰∫ÜÈÅøÂÖç‰øÆÊîπÂà∞ÂéüÂßã feature_groupsÔºåÊàëÂÄëÂÖàË§áË£Ω‰∏Ä‰ªΩ\n",
    "import copy\n",
    "current_feature_groups = copy.deepcopy(feature_groups)\n",
    "\n",
    "for block_id, block_df in df_sorted.groupby('time_block'):\n",
    "\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"üöÄ Running Block {block_id} (Year: {2010 + block_id*2} - {2011 + block_id*2})\")\n",
    "    print(f\"{'='*40}\")\n",
    "\n",
    "    # 1. Á¢∫‰øùÊôÇÈñìÊéíÂ∫è‰∏¶ÈÄ≤Ë°å 80/20 ÂàáÂàÜ\n",
    "    block_df = block_df.sort_values('date')\n",
    "    split_index = int(len(block_df) * 0.8)\n",
    "    \n",
    "    # ÈÄôË£°ÂÖà‰øùÁïôÊâÄÊúâÊ¨Ñ‰ΩçÔºåÂõ†ÁÇ∫Ë¶ÅÁÆó HighRiskMCC\n",
    "    train_raw = block_df.iloc[:split_index].copy()\n",
    "    test_raw  = block_df.iloc[split_index:].copy()\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2. ‚ö†Ô∏è ÈóúÈçµ‰øÆÊ≠£ÔºöÂú®Ë©≤ Block ÁöÑ Train ‰∏äË®àÁÆó HighRiskMCC\n",
    "    # -----------------------------\n",
    "    # Ë®àÁÆó Fraud Rate\n",
    "    fraud_rate = train_raw.groupby('mcc_code')['is_fraud'].mean()\n",
    "    # ÊâæÂá∫È´òÈ¢®Èö™ MCC (Â§ßÊñº 2%)\n",
    "    high_risk_mcc = fraud_rate[fraud_rate > 0.02].index\n",
    "\n",
    "    # Êò†Â∞ÑÂõû Train Âíå Test\n",
    "    train_raw['HighRiskMCC'] = train_raw['mcc_code'].isin(high_risk_mcc).astype('uint8')\n",
    "    test_raw['HighRiskMCC']  = test_raw['mcc_code'].isin(high_risk_mcc).astype('uint8')\n",
    "    # -----------------------------\n",
    "\n",
    "    # ÂàùÂßãÂåñË©≤ Block ÁöÑÁµêÊûúÂ≠òÊîæÂçÄ\n",
    "    results[block_id] = {}\n",
    "\n",
    "    # 3. ÁßªÈô§‰∏çÂèÉËàáË®ìÁ∑¥ÁöÑÈùûÁâπÂæµÊ¨Ñ‰Ωç (Date/Year/Block Á≠â)\n",
    "    # ÈÄôË£°‰∏çÂà™Èô§ transaction_id Á≠âÔºåÂõ†ÁÇ∫ÂæåÈù¢ select_features ÊúÉÊåëÈÅ∏ÈúÄË¶ÅÁöÑ\n",
    "    drop_meta_cols = ['date', 'year', 'time_block']\n",
    "    train_raw = train_raw.drop(columns=drop_meta_cols, errors='ignore')\n",
    "    test_raw  = test_raw.drop(columns=drop_meta_cols, errors='ignore')\n",
    "\n",
    "    # üîÅ Â∞çÊØè‰∏ÄÁµÑ feature group Ë∑ë stepwise\n",
    "    for group_name, feature_list in current_feature_groups.items():\n",
    "\n",
    "        print(f\"\\n   üîπ Processing Group: {group_name}\")\n",
    "\n",
    "        # 4. ÁâπÂæµÁØ©ÈÅ∏ (‰ΩøÁî®‰Ω†ÂØ´Â•ΩÁöÑ select_features)\n",
    "        # Á¢∫‰øù feature_list Ë£°ÁöÑÊ¨Ñ‰ΩçÈÉΩÂú® df Ë£°Èù¢ (Èò≤ÂëÜ)\n",
    "        valid_features = [f for f in feature_list if f in train_raw.columns]\n",
    "        \n",
    "        # Ê™¢Êü•ÊòØÂê¶ÊúâÈÅ∫ÊºèÁöÑÊ¨Ñ‰Ωç\n",
    "        missing_features = set(feature_list) - set(valid_features)\n",
    "        if missing_features:\n",
    "            print(f\"      ‚ö†Ô∏è Warning: {missing_features} not found in dataset. Skipping them.\")\n",
    "\n",
    "        if not valid_features:\n",
    "            print(\"      ‚ùå No valid features found for this group. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        train_df = select_features(train_raw, valid_features, dep_var=\"is_fraud\")\n",
    "        test_df  = select_features(test_raw, valid_features, dep_var=\"is_fraud\")\n",
    "        \n",
    "        # 5. ÊéíÈô§Â∏∏Êï∏Ê¨Ñ‰Ωç (Constant Columns) ‰ª•ÂÖç Logit Â†±ÈåØ\n",
    "        # Ëã•ÊüêÂÄãÊ¨Ñ‰ΩçÂú® train_df Ë£°Âè™Êúâ‰∏ÄÁ®ÆÊï∏ÂÄº (‰æãÂ¶ÇÂÖ®ÁÇ∫ 0)ÔºåStepwise ÊúÉÂ¥©ÊΩ∞\n",
    "        std_check = train_df.std()\n",
    "        constant_cols = std_check[std_check == 0].index.tolist()\n",
    "        if constant_cols:\n",
    "            print(f\"      ‚ö†Ô∏è Dropping constant columns: {constant_cols}\")\n",
    "            train_df = train_df.drop(columns=constant_cols)\n",
    "            test_df = test_df.drop(columns=constant_cols)\n",
    "\n",
    "        # 6. Âü∑Ë°å Stepwise\n",
    "        try:\n",
    "            overall_fit, coef_df, not_in_eq_df, final_model = stepwise_logit_with_k_tables(\n",
    "                train_df,\n",
    "                test_df,\n",
    "                dep_var=\"is_fraud\",\n",
    "                k=314657018, # Ë®≠ÈÄôÈ∫ºÂ§ß‰ª£Ë°® Full Stepwise\n",
    "                verbose=False # Âª∫Ë≠∞ FalseÔºå‰∏çÁÑ∂ output ÊúÉË¢´Ê¥óÁâà\n",
    "            )\n",
    "\n",
    "            # 7. Áî¢ÁîüÂàÜÈ°ûË°® (Confusion Matrix info)\n",
    "            train_table = classification_table(final_model, train_df)\n",
    "            test_table  = classification_table(final_model, test_df)\n",
    "\n",
    "            # 8. ÂÑ≤Â≠òÁµêÊûú\n",
    "            results[block_id][group_name] = {\n",
    "                \"overall_fit\": overall_fit,\n",
    "                \"coef_df\": coef_df,\n",
    "                \"not_in_eq_df\": not_in_eq_df,\n",
    "                \"final_model\": final_model,\n",
    "                \"train_table\": train_table,\n",
    "                \"test_table\": test_table\n",
    "            }\n",
    "            print(f\"      ‚úÖ Done. Final variables: {len(coef_df)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"      ‚ùå Error processing {group_name} in Block {block_id}: {str(e)}\")\n",
    "            results[block_id][group_name] = {\"error\": str(e)}\n",
    "\n",
    "print(\"\\nüéâ All blocks and groups processed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ÂàÜÊûêÁî®ÔºÅÔºÅ\n",
    "\n",
    "results[\n",
    "  block_id\n",
    "][\n",
    "  \"X_all + X_rfm + X_dk\"\n",
    "][\"coef_df\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04-3 all‰∏üÊ®°ÂûãÂÖß"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, average_precision_score\n",
    "\n",
    "def fit_full_logit(train_df, test_df, dep_var=\"is_fraud\"):\n",
    "    # 1. split\n",
    "    X_train = train_df.drop(columns=[dep_var])\n",
    "    y_train = train_df[dep_var]\n",
    "\n",
    "    X_test = test_df.drop(columns=[dep_var])\n",
    "    y_test = test_df[dep_var]\n",
    "\n",
    "    # üéØ Á¢∫‰øù test Ê¨Ñ‰ΩçÈ†ÜÂ∫è = train Ê¨Ñ‰ΩçÈ†ÜÂ∫è\n",
    "    X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "    # 2. Ê®ôÊ∫ñÂåñ\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # 3. Logistic Regression\n",
    "    model = LogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=1000,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # 4. probabilities\n",
    "    train_pred = model.predict_proba(X_train_scaled)[:, 1]\n",
    "    test_pred = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "    # 5. ROC AUC\n",
    "    train_auc = roc_auc_score(y_train, train_pred)\n",
    "    test_auc = roc_auc_score(y_test, test_pred)\n",
    "\n",
    "    # 6. PR AUC\n",
    "    train_prauc = average_precision_score(y_train, train_pred)\n",
    "    test_prauc = average_precision_score(y_test, test_pred)\n",
    "\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"scaler\": scaler,\n",
    "        \"train_pred\": train_pred,\n",
    "        \"test_pred\": test_pred,\n",
    "        \"train_auc\": train_auc,\n",
    "        \"test_auc\": test_auc,\n",
    "        \"train_prauc\": train_prauc,\n",
    "        \"test_prauc\": test_prauc\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train AUC: 0.9154900057627916\n",
      "Test AUC: 0.8854641260446391\n",
      "Train PR-AUC: 0.08173481123106874\n",
      "Test PR-AUC: 0.06417943887443732\n"
     ]
    }
   ],
   "source": [
    "result = fit_full_logit(train_df, test_df, dep_var=\"is_fraud\")\n",
    "\n",
    "print(\"Train AUC:\", result[\"train_auc\"])\n",
    "print(\"Test AUC:\", result[\"test_auc\"])\n",
    "\n",
    "print(\"Train PR-AUC:\", result[\"train_prauc\"])\n",
    "print(\"Test PR-AUC:\", result[\"test_prauc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\n",
    "    'transaction_id',\n",
    "    'client_id_x',\n",
    "    'card_id',\n",
    "    'card_number',\n",
    "    'cvv'\n",
    "]\n",
    "\n",
    "train_df = train_df.drop(columns=cols_to_drop, errors='ignore')\n",
    "test_df  = test_df.drop(columns=cols_to_drop, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fit_full_logit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m result = \u001b[43mfit_full_logit\u001b[49m(train_df, test_df, dep_var=\u001b[33m\"\u001b[39m\u001b[33mis_fraud\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTrain AUC:\u001b[39m\u001b[33m\"\u001b[39m, result[\u001b[33m\"\u001b[39m\u001b[33mtrain_auc\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTest AUC:\u001b[39m\u001b[33m\"\u001b[39m, result[\u001b[33m\"\u001b[39m\u001b[33mtest_auc\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'fit_full_logit' is not defined"
     ]
    }
   ],
   "source": [
    "result = fit_full_logit(train_df, test_df, dep_var=\"is_fraud\")\n",
    "\n",
    "print(\"Train AUC:\", result[\"train_auc\"])\n",
    "print(\"Test AUC:\", result[\"test_auc\"])\n",
    "\n",
    "print(\"Train PR-AUC:\", result[\"train_prauc\"])\n",
    "print(\"Test PR-AUC:\", result[\"test_prauc\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, average_precision_score\n",
    "\n",
    "def fit_full_logit(train_df, test_df, dep_var=\"is_fraud\"):\n",
    "    # 1. split\n",
    "    X_train = train_df.drop(columns=[dep_var])\n",
    "    y_train = train_df[dep_var]\n",
    "\n",
    "    X_test = test_df.drop(columns=[dep_var])\n",
    "    y_test = test_df[dep_var]\n",
    "\n",
    "    # üéØ Á¢∫‰øù test Ê¨Ñ‰ΩçÈ†ÜÂ∫è = train Ê¨Ñ‰ΩçÈ†ÜÂ∫è\n",
    "    X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "    # 2. Ê®ôÊ∫ñÂåñ\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # 3. Logistic Regression\n",
    "    model = LogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=1000,\n",
    "        n_jobs=-1,\n",
    "        class_weight=\"balanced\"\n",
    "    )\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # 4. probabilities\n",
    "    train_pred = model.predict_proba(X_train_scaled)[:, 1]\n",
    "    test_pred = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "    # 5. ROC AUC\n",
    "    train_auc = roc_auc_score(y_train, train_pred)\n",
    "    test_auc = roc_auc_score(y_test, test_pred)\n",
    "\n",
    "    # 6. PR AUC\n",
    "    train_prauc = average_precision_score(y_train, train_pred)\n",
    "    test_prauc = average_precision_score(y_test, test_pred)\n",
    "\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"scaler\": scaler,\n",
    "        \"train_pred\": train_pred,\n",
    "        \"test_pred\": test_pred,\n",
    "        \"train_auc\": train_auc,\n",
    "        \"test_auc\": test_auc,\n",
    "        \"train_prauc\": train_prauc,\n",
    "        \"test_prauc\": test_prauc\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train AUC: 0.8963198198876533\n",
      "Test AUC: 0.8912327090965326\n",
      "Train PR-AUC: 0.04883650297518295\n",
      "Test PR-AUC: 0.03905783518019472\n"
     ]
    }
   ],
   "source": [
    "result = fit_full_logit(train_df, test_df, dep_var=\"is_fraud\")\n",
    "\n",
    "print(\"Train AUC:\", result[\"train_auc\"])\n",
    "print(\"Test AUC:\", result[\"test_auc\"])\n",
    "\n",
    "print(\"Train PR-AUC:\", result[\"train_prauc\"])\n",
    "print(\"Test PR-AUC:\", result[\"test_prauc\"])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "virtual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
