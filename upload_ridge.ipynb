{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01_import dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01-1_import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "#https://drive.google.com/drive/folders/18qV82fNY3IIWu3BRoGqm_LNgJzE8Akbr?usp=drive_link\n",
    "#base_dir = \"/Users/Andypon/10_äº¤å¤§ç ”ç©¶æ‰€/1141_01_æ©Ÿå™¨å­¸ç¿’èˆ‡é‡‘èç§‘æŠ€/data\"\n",
    "base_dir= '/Users/andyw.p.chen/Documents/Project/datasets'\n",
    "#base_dir=  \"c:\\Users\\user\\Downloads\\datasets\"\n",
    "\n",
    "def load_json_to_df(filename: str) -> pd.DataFrame:\n",
    "    file_path = os.path.join(base_dir, filename)\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # å¦‚æœæ˜¯ { \"target\": {id: value, ...} }\n",
    "    if isinstance(data, dict) and len(data) == 1 and isinstance(next(iter(data.values())), dict):\n",
    "        key, inner = next(iter(data.items()))\n",
    "        return pd.DataFrame(list(inner.items()), columns=[\"id\", key])\n",
    "\n",
    "    # dict of scalar\n",
    "    if isinstance(data, dict):\n",
    "        return pd.DataFrame([{\"code\": k, \"desc\": v} for k, v in data.items()])\n",
    "\n",
    "    # list of dict\n",
    "    elif isinstance(data, list):\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported JSON structure in {filename}: {type(data)}\")\n",
    "\n",
    "\n",
    "def load_csv_to_df(filename: str) -> pd.DataFrame:\n",
    "    \"\"\"è®€å– CSV ä¸¦è½‰ç‚º DataFrameã€‚\"\"\"\n",
    "    return pd.read_csv(os.path.join(base_dir, filename))\n",
    "\n",
    "# JSON è³‡æ–™\n",
    "##mcc_codes_df = load_json_to_df(\"mcc_codes.json\")\n",
    "train_fraud_labels_df = load_json_to_df(\"train_fraud_labels.json\")\n",
    "\n",
    "# CSV è³‡æ–™\n",
    "cards_df = load_csv_to_df(\"cards_data.csv\")\n",
    "transactions_df = load_csv_to_df(\"transactions_data.csv\")\n",
    "users_df = load_csv_to_df(\"users_data.csv\")\n",
    "\n",
    "# ç°¡å–®æª¢æŸ¥\n",
    "#print(mcc_codes_df.head())\n",
    "#print(train_fraud_labels_df.head())\n",
    "#print(cards_df.head())\n",
    "#print(transactions_df.head())\n",
    "#print(users_df.apthead())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01-2_rename variable in each data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fraud_labels_df = train_fraud_labels_df.rename(columns={'id': 'transactions_id'})\n",
    "train_fraud_labels_df = train_fraud_labels_df.rename(columns={'target': 'is_fraud'})\n",
    "\n",
    "cards_df = cards_df.rename(columns={'id':'card_id'})\n",
    "\n",
    "users_df = users_df.rename(columns={'id':'client_id'})\n",
    "\n",
    "transactions_df = transactions_df.rename(columns={'mcc': 'mcc_code'})\n",
    "transactions_df = transactions_df.rename(columns={'id': 'transaction_id'})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01-3_è®Šæ•¸å‹æ…‹çµ±ä¸€åŠç¼ºå¤±å€¼è™•ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing_flags(df: pd.DataFrame, cols: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    åœ¨ DataFrame ä¸­å°æŒ‡å®šæ¬„ä½å»ºç«‹ missing flag æ¬„ä½\n",
    "    flag=1 è¡¨ç¤ºç¼ºå¤±å€¼ï¼Œflag=0 è¡¨ç¤ºéç¼ºå¤±å€¼\n",
    "    \n",
    "    åƒæ•¸\n",
    "    ----\n",
    "    df : pd.DataFrame\n",
    "        è¼¸å…¥çš„è³‡æ–™æ¡†\n",
    "    cols : list\n",
    "        è¦æª¢æŸ¥çš„æ¬„ä½åç¨±æ¸…å–®\n",
    "    \n",
    "    å›å‚³\n",
    "    ----\n",
    "    pd.DataFrame : æ–°çš„è³‡æ–™æ¡† (å«æ–°å¢çš„ flag æ¬„ä½)\n",
    "    \"\"\"\n",
    "    for col in cols:\n",
    "        df[f\"{col}_missing_flag\"] = df[col].isna().astype(int)\n",
    "    return df\n",
    "\n",
    "transactions_df = add_missing_flags(transactions_df, [\"merchant_state\", \"zip\", \"errors\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##train_fraud_labels_df##\n",
    "train_fraud_labels_df[\"is_fraud\"]=train_fraud_labels_df[\"is_fraud\"].astype(\"category\") \n",
    "train_fraud_labels_df[\"transactions_id\"]=train_fraud_labels_df[\"transactions_id\"].astype(int) #åˆä½µè³‡æ–™éœ€è¦\n",
    "\n",
    "##cards_df##\n",
    "cards_df[\"card_brand\"]=cards_df[\"card_brand\"].astype(\"category\") \n",
    "cards_df[\"card_type\"]=cards_df[\"card_type\"].astype(\"category\")\n",
    "#####ä¸è¦loadé€™è¡Œ cards_df[\"expires\"]=pd.to_datetime(cards_df[\"expires\"], format=\"%m/%Y\")\n",
    "cards_df[\"expires\"] = pd.to_datetime(cards_df[\"expires\"], format=\"%m/%Y\").dt.to_period(\"M\")\n",
    "cards_df[\"has_chip\"]=cards_df[\"has_chip\"].astype(\"category\")\n",
    "\n",
    "cards_df['credit_limit'] = cards_df['credit_limit'].replace(r'[\\$,]', '', regex=True).astype(int)\n",
    "#####ä¸è¦loadé€™è¡Œ cards_df[\"acct_open_date\"]=pd.to_datetime(cards_df[\"acct_open_date\"], format=\"%m/%Y\")\n",
    "cards_df[\"acct_open_date\"] = pd.to_datetime(cards_df[\"acct_open_date\"], format=\"%m/%Y\").dt.to_period(\"M\")\n",
    "#####ä¸è¦loadé€™è¡Œ cards_df[\"year_pin_last_changed\"]=pd.to_datetime(cards_df[\"year_pin_last_changed\"], format=\"%Y\")\n",
    "cards_df[\"year_pin_last_changed\"] = pd.to_datetime(cards_df[\"year_pin_last_changed\"], format=\"%Y\").dt.to_period(\"Y\")\n",
    "cards_df[\"card_on_dark_web\"]=cards_df[\"card_on_dark_web\"].astype(\"category\") \n",
    "\n",
    "##users_df##\n",
    "users_df[\"birth_year\"] = pd.to_datetime(users_df[\"birth_year\"], format=\"%Y\").dt.to_period(\"Y\")\n",
    "users_df[\"birth_month\"] = pd.to_datetime(users_df[\"birth_month\"], format=\"%m\").dt.to_period(\"M\")\n",
    "users_df[\"gender\"]=users_df[\"gender\"].astype(\"category\") \n",
    "users_df['per_capita_income'] = users_df['per_capita_income'].replace(r'[\\$,]', '', regex=True).astype(int)\n",
    "users_df['yearly_income'] = users_df['yearly_income'].replace(r'[\\$,]', '', regex=True).astype(int)\n",
    "users_df['total_debt'] = users_df['total_debt'].replace(r'[\\$,]', '', regex=True).astype(int)\n",
    "\n",
    "##transactions_df##\n",
    "transactions_df[\"date\"] = pd.to_datetime(transactions_df[\"date\"])\n",
    "#æµ®é»æ•¸è½‰æ•´æ•¸åŸå› ç¢ºå®šï¼Ÿ\n",
    "transactions_df['amount'] = transactions_df['amount'].replace(r'[\\$,]', '', regex=True).astype(float).astype(int)\n",
    "##è² æ•¸å–logèª¿æˆ1\n",
    "#transactions_df['amount'] = transactions_df['amount'].replace(r'[\\$,]', '', regex=True).astype(float)\n",
    "\n",
    "transactions_df[\"use_chip\"]=transactions_df[\"use_chip\"].astype(\"category\") \n",
    "\n",
    "transactions_df.loc[\n",
    "    transactions_df['merchant_city'].str.lower() == 'online',\n",
    "    'merchant_state'\n",
    "] = 'online'\n",
    "\n",
    "transactions_df.loc[\n",
    "    transactions_df['merchant_city'].str.lower() == 'online',\n",
    "    'zip'\n",
    "] = 20000 #åŸæœ¬æ˜¯-1\n",
    "## æˆ‘æ²’æœ‰å…¨éƒ¨æ”¹ï¼Œé€™æ¨£å®Œä¹‹å¾Œä»æœ‰89006ç­†Missingï¼Œå‰©ä¸‹éƒ½æ˜¯åœ¨åœ‹å¤–\n",
    "transactions_df['zip'] = transactions_df['zip'].fillna(10000) #åŸæœ¬æ˜¯-999\n",
    "transactions_df[\"zip\"]=transactions_df[\"zip\"].astype(\"int64\")\n",
    "\n",
    "transactions_df['errors'] = transactions_df['errors'].astype('category')\n",
    "transactions_df['errors'] = transactions_df['errors'].cat.add_categories('No_error').fillna('No_error')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cars one hot encoding\n",
    "##çµ±ä¸€é¡åˆ¥è®Šæ•¸è½‰dummy variable(è¦æ³¨æ„å…±ç·šæ€§å•é¡Œï¼Œæ‡‰åˆªæ‰å…¶ä¸­ä¹‹ä¸€)\n",
    "\n",
    "#card_type åŸå§‹ç¨®é¡ï¼šDebit_57%, Credit_33%, Debit(Prepaid)_9%\n",
    "#card_brand åŸå§‹ç¨®é¡ï¼šMasterCard_52%, Visa_38%, Amex_7%, Discovery_3%\n",
    "#has_chip åŸå§‹ç¨®é¡ï¼šYes_89%, No_11%\n",
    "#card_on_dark_web åŸå§‹ç¨®é¡ï¼šNo_0%\n",
    "cols_to_encode = ['card_type', 'card_brand', 'has_chip']\n",
    "cards_df[cols_to_encode] = cards_df[cols_to_encode].astype('category')\n",
    "dummies_cards = pd.get_dummies(\n",
    "    cards_df[cols_to_encode], \n",
    "    prefix=cols_to_encode, \n",
    "    dtype='uint8'\n",
    "    )\n",
    "cards_df = pd.concat([cards_df, dummies_cards], axis=1)\n",
    "\n",
    "#use_chip åŸå§‹ç¨®é¡ï¼šSwiped_52%, Chipe_36%, Online_12%\n",
    "dummies_use = pd.get_dummies(transactions_df['use_chip'], prefix='use_chip', dtype='uint8')\n",
    "transactions_df = pd.concat([transactions_df, dummies_use], axis=1)\n",
    "\n",
    "#gender åŸå§‹ç¨®é¡ï¼šFemale_51%, Male_49%\n",
    "dummies_gender = pd.get_dummies(users_df['gender'], prefix='gender', dtype='uint8')\n",
    "users_df = pd.concat([users_df, dummies_gender], axis=1)\n",
    "\n",
    "\n",
    "cards_df.drop(columns=[\"has_chip_NO\",\"has_chip\"], inplace=True)\n",
    "transactions_df.drop(columns=[\"use_chip\"], inplace=True)\n",
    "users_df.drop(columns=[\"gender_Female\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02_è³‡æ–™æ•´ä½µæˆä¸€å¼µdataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02-1_è³‡æ–™æ•´ä½µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transactions_df.loc[transactions_df[\"transaction_id\"] == 10649266] #transaction_id vs id\n",
    "\n",
    "#åŸå§‹è³‡æ–™ç­†æ•¸ï¼š13305915\n",
    "### transactions_df+train_fraud_labels_df      left æœƒæœ‰4390952 missing values\n",
    "merged = pd.merge(transactions_df, train_fraud_labels_df, left_on=\"transaction_id\", right_on=\"transactions_id\", how=\"outer\")\n",
    "### transactions_df train_fraud_labels_df(8914963) + users_df å°éå»ä¸æœƒæœ‰missing values\n",
    "merged = pd.merge(merged,users_df , left_on=\"client_id\", right_on=\"client_id\", how=\"left\")\n",
    "### transactions_df train_fraud_labels_df users_df + cards_df å°éå»ä¸æœƒæœ‰missing values\n",
    "merged = pd.merge(merged,cards_df , left_on=\"card_id\", right_on=\"card_id\", how=\"left\")\n",
    "\n",
    "#åˆªæ‰é‡è¤‡çš„columns\n",
    "merged.drop(columns=[\"transactions_id\"], inplace=True)\n",
    "merged.drop(columns=[\"client_id_y\"], inplace=True)\n",
    "\n",
    "## åˆä½µå®Œä¹‹å¾Œæœ€å¾Œè™•ç†is_fraud(åŸæœƒæœ‰missing valueså•é¡Œ)\n",
    "merged[\"is_fraud\"] = merged[\"is_fraud\"].astype(str)\n",
    "merged.loc[merged['is_fraud'].str.lower() == 'no','is_fraud'] = '0'\n",
    "merged.loc[merged['is_fraud'].str.lower() == 'yes','is_fraud'] = '1'\n",
    "merged[\"is_fraud\"] = pd.to_numeric(merged[\"is_fraud\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "merged = add_missing_flags(merged, [\"is_fraud\"])\n",
    "\n",
    "#merged.to_csv(\"merged.csv\", index=False)\n",
    "\n",
    "# å…ˆåˆªé™¤ä¸éœ€è¦çš„DataFrameä»¥ç¯€çœè¨˜æ†¶é«”\n",
    "del transactions_df, users_df, cards_df, train_fraud_labels_df, cols_to_encode, dummies_cards, dummies_use, dummies_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_merged = merged.copy()\n",
    "#merged = backup_merged.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04_Benchmark model fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04-1_è³‡æ–™é€²è¡Œè®Šæ•¸è½‰æ›ä»¥æ±‚æ¨¡å‹é…é£¾æ›´ä½³è¡¨ç¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats \n",
    "\n",
    "# === (1) logè½‰æ› ===\n",
    "merged['amount'] = np.where(merged['amount'] < 0, 0, merged['amount'])  # è² æ•¸è®Š 0\n",
    "merged['amount'] = np.log(merged['amount'] + 1)  \n",
    "\n",
    "# === (3) å¹³æ–¹æ ¹è½‰æ› ===\n",
    "merged['credit_limit']=np.sqrt(merged['credit_limit'])\n",
    "merged['total_debt']=np.sqrt(merged['total_debt'])\n",
    "\n",
    "# === (3) ç«‹æ–¹æ ¹è½‰æ› ===\n",
    "merged['yearly_income']=np.cbrt(merged['yearly_income'])\n",
    "merged['per_capita_income']=np.cbrt(merged['per_capita_income'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04-2_åˆ†å‰²è¨“ç·´é›†åŠæ¸¬è©¦é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_fraud\n",
      "0    7121755\n",
      "1      10215\n",
      "Name: count, dtype: Int64\n",
      "is_fraud\n",
      "0    1779876\n",
      "1       3117\n",
      "Name: count, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "# --- é¸å–æ•¸å€¼å‹è®Šæ•¸ ---\n",
    "num_cols = merged.select_dtypes(include=['int64', 'float64','uint8','datetime64[ns]']).columns\n",
    "df2 = merged[num_cols]\n",
    "\n",
    "# --- dropna ---\n",
    "df_cleaned = df2.dropna()\n",
    "del df2\n",
    "\n",
    "# --- é¿å…å…±ç·šæ€§ ---\n",
    "df_cleaned.drop(columns=[\"is_fraud_missing_flag\",\"card_type_Debit (Prepaid)\", \n",
    "                         \"card_brand_Discover\", \"use_chip_Online Transaction\"], inplace=True)\n",
    "\n",
    "# --- ç¢ºä¿ date æ¬„ä½åœ¨ df_cleaned ä¸­ ---\n",
    "if 'date' not in df_cleaned.columns:\n",
    "    df_cleaned['date'] = merged.loc[df_cleaned.index, 'date']\n",
    "\n",
    "# --- ä¾æ™‚é–“æ’åº ---\n",
    "df_sorted = df_cleaned.sort_values('date')\n",
    "\n",
    "# --- æ™‚é–“åºåˆ—åˆ‡åˆ†ï¼ˆå‰ 80% è¨“ç·´, å¾Œ 20% æ¸¬è©¦ï¼‰ ---\n",
    "split_index = int(len(df_sorted) * 0.8)\n",
    "train_df = df_sorted.iloc[:split_index].drop(columns=['date'])  # å¯é¸æ“‡ä¸Ÿæ‰ date\n",
    "test_df  = df_sorted.iloc[split_index:].drop(columns=['date'])\n",
    "\n",
    "# --- æª¢æŸ¥è©æ¬ºè³‡æ–™åˆ†å¸ƒ ---\n",
    "print(train_df['is_fraud'].value_counts(normalize=False))\n",
    "print(test_df['is_fraud'].value_counts(normalize=False))\n",
    "\n",
    "# --- æ¸…ç†ä¸ç”¨çš„è®Šæ•¸ ---\n",
    "del df_cleaned, df_sorted, merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04-3(b1)_(ç•¥04-3(a))Assumption:Avoid Multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##è™•ç†å…±ç·šæ€§\n",
    "train_df.drop(columns=[\"per_capita_income\"], inplace=True)\n",
    "train_df.drop(columns=[\"use_chip_Chip Transaction\",\"merchant_state_missing_flag\",\"zip_missing_flag\"], inplace=True)           \n",
    "train_df.drop(columns=[\"card_brand_Visa\" ,\"card_brand_Amex\",\"card_type_Credit\"], inplace=True)\n",
    "#å†é‡è·‘ä¸€æ¬¡VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.drop(columns=[\"per_capita_income\"], inplace=True)\n",
    "test_df.drop(columns=[\"use_chip_Chip Transaction\",\"merchant_state_missing_flag\",\"zip_missing_flag\"], inplace=True)           \n",
    "test_df.drop(columns=[\"card_brand_Visa\" ,\"card_brand_Amex\",\"card_type_Credit\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04-3 lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C: 0.013257113655901095\n",
      "Best lambda = 1/C: 75.43120063354615\n",
      "Lasso é¸åˆ° 21 å€‹è®Šæ•¸ï¼š\n",
      "['transaction_id', 'client_id_x', 'amount', 'merchant_id', 'zip', 'mcc_code', 'errors_missing_flag', 'use_chip_Swipe Transaction', 'current_age', 'retirement_age', 'latitude', 'longitude', 'yearly_income', 'credit_score', 'num_credit_cards', 'cvv', 'num_cards_issued', 'credit_limit', 'card_type_Debit', 'card_brand_Mastercard', 'has_chip_YES']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "def lasso_select_features(train_df, dep_var=\"is_fraud\"):\n",
    "\n",
    "    y = train_df[dep_var]\n",
    "    X = train_df.drop(columns=[dep_var])\n",
    "\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    X = X[numeric_cols]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    lasso_cv = LogisticRegressionCV(\n",
    "        Cs=np.logspace(-3, 2, 50),  # C from 0.001 to 100\n",
    "        cv=5,\n",
    "        penalty=\"l1\",\n",
    "        solver=\"saga\",\n",
    "        scoring=\"roc_auc\",\n",
    "        max_iter=10000,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    lasso_cv.fit(X_scaled, y)\n",
    "\n",
    "    coef = lasso_cv.coef_.ravel()\n",
    "    selected = [col for col, c in zip(numeric_cols, coef) if abs(c) > 1e-6]\n",
    "\n",
    "    best_C = lasso_cv.C_[0]\n",
    "    best_lambda = 1 / best_C\n",
    "\n",
    "    print(\"Best C:\", best_C)\n",
    "    print(\"Best lambda = 1/C:\", best_lambda)\n",
    "\n",
    "    print(f\"Lasso é¸åˆ° {len(selected)} å€‹è®Šæ•¸ï¼š\")\n",
    "    print(selected)\n",
    "\n",
    "    return selected\n",
    "\n",
    "\n",
    "selected_features = lasso_select_features(train_df, dep_var=\"is_fraud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, average_precision_score\n",
    "\n",
    "def fit_full_logit(train_df, test_df, dep_var=\"is_fraud\"):\n",
    "    # 1. split\n",
    "    X_train = train_df.drop(columns=[dep_var])\n",
    "    y_train = train_df[dep_var]\n",
    "\n",
    "    X_test = test_df.drop(columns=[dep_var])\n",
    "    y_test = test_df[dep_var]\n",
    "\n",
    "    # ğŸ¯ ç¢ºä¿ test æ¬„ä½é †åº = train æ¬„ä½é †åº\n",
    "    X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "    # 2. æ¨™æº–åŒ–\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # 3. Logistic Regression\n",
    "    model = LogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        solver=\"saga\",\n",
    "        max_iter=200,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # 4. probabilities\n",
    "    train_pred = model.predict_proba(X_train_scaled)[:, 1]\n",
    "    test_pred = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "    # 5. ROC AUC\n",
    "    train_auc = roc_auc_score(y_train, train_pred)\n",
    "    test_auc = roc_auc_score(y_test, test_pred)\n",
    "\n",
    "    # 6. PR AUC\n",
    "    train_prauc = average_precision_score(y_train, train_pred)\n",
    "    test_prauc = average_precision_score(y_test, test_pred)\n",
    "\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"scaler\": scaler,\n",
    "        \"train_pred\": train_pred,\n",
    "        \"test_pred\": test_pred,\n",
    "        \"train_auc\": train_auc,\n",
    "        \"test_auc\": test_auc,\n",
    "        \"train_prauc\": train_prauc,\n",
    "        \"test_prauc\": test_prauc\n",
    "    }\n",
    "\n",
    "\n",
    "selected_features = lasso_select_features(train_df, dep_var=\"is_fraud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_var = \"is_fraud\"\n",
    "\n",
    "# â¬‡ ç¢ºä¿æ¨¡å‹åªåƒ selected features + label\n",
    "train_selected = train_df[selected_features + [dep_var]].copy()\n",
    "test_selected = test_df[selected_features + [dep_var]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train AUC: 0.8840543005059368\n",
      "Test AUC: 0.8378398129846902\n",
      "Train PR-AUC: 0.022293644601833036\n",
      "Test PR-AUC: 0.02564510017755983\n"
     ]
    }
   ],
   "source": [
    "result = fit_full_logit(train_selected, test_selected, dep_var=\"is_fraud\")\n",
    "\n",
    "print(\"Train AUC:\", result[\"train_auc\"])\n",
    "print(\"Test AUC:\", result[\"test_auc\"])\n",
    "\n",
    "print(\"Train PR-AUC:\", result[\"train_prauc\"])\n",
    "print(\"Test PR-AUC:\", result[\"test_prauc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transaction_id',\n",
       " 'client_id_x',\n",
       " 'amount',\n",
       " 'merchant_id',\n",
       " 'zip',\n",
       " 'mcc_code',\n",
       " 'errors_missing_flag',\n",
       " 'use_chip_Swipe Transaction',\n",
       " 'current_age',\n",
       " 'retirement_age',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'yearly_income',\n",
       " 'credit_score',\n",
       " 'num_credit_cards',\n",
       " 'cvv',\n",
       " 'num_cards_issued',\n",
       " 'credit_limit',\n",
       " 'card_type_Debit',\n",
       " 'card_brand_Mastercard',\n",
       " 'has_chip_YES']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "virtual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
