{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "01_import dataset\n",
        "==="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "#https://drive.google.com/drive/folders/18qV82fNY3IIWu3BRoGqm_LNgJzE8Akbr?usp=drive_link\n",
        "#base_dir = \"/Users/Andypon/10_交大研究所/1141_01_機器學習與金融科技/data\"\n",
        "base_dir= '/Users/andyw.p.chen/Documents/Project/datasets'\n",
        "#base_dir=  \"c:\\Users\\user\\Downloads\\datasets\"\n",
        "\n",
        "def load_json_to_df(filename: str) -> pd.DataFrame:\n",
        "    file_path = os.path.join(base_dir, filename)\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # 如果是 { \"target\": {id: value, ...} }\n",
        "    if isinstance(data, dict) and len(data) == 1 and isinstance(next(iter(data.values())), dict):\n",
        "        key, inner = next(iter(data.items()))\n",
        "        return pd.DataFrame(list(inner.items()), columns=[\"id\", key])\n",
        "\n",
        "    # dict of scalar\n",
        "    if isinstance(data, dict):\n",
        "        return pd.DataFrame([{\"code\": k, \"desc\": v} for k, v in data.items()])\n",
        "\n",
        "    # list of dict\n",
        "    elif isinstance(data, list):\n",
        "        return pd.DataFrame(data)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported JSON structure in {filename}: {type(data)}\")\n",
        "\n",
        "\n",
        "def load_csv_to_df(filename: str) -> pd.DataFrame:\n",
        "    \"\"\"讀取 CSV 並轉為 DataFrame。\"\"\"\n",
        "    return pd.read_csv(os.path.join(base_dir, filename))\n",
        "\n",
        "# JSON 資料\n",
        "##mcc_codes_df = load_json_to_df(\"mcc_codes.json\")\n",
        "train_fraud_labels_df = load_json_to_df(\"train_fraud_labels.json\")\n",
        "\n",
        "# CSV 資料\n",
        "cards_df = load_csv_to_df(\"cards_data.csv\")\n",
        "transactions_df = load_csv_to_df(\"transactions_data.csv\")\n",
        "users_df = load_csv_to_df(\"users_data.csv\")\n",
        "\n",
        "# 簡單檢查\n",
        "#print(mcc_codes_df.head())\n",
        "#print(train_fraud_labels_df.head())\n",
        "#print(cards_df.head())\n",
        "#print(transactions_df.head())\n",
        "#print(users_df.apthead())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "02_rename variable in each data set\n",
        "==="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_fraud_labels_df = train_fraud_labels_df.rename(columns={'id': 'transactions_id'})\n",
        "train_fraud_labels_df = train_fraud_labels_df.rename(columns={'target': 'is_fraud'})\n",
        "\n",
        "cards_df = cards_df.rename(columns={'id':'card_id'})\n",
        "\n",
        "users_df = users_df.rename(columns={'id':'client_id'})\n",
        "\n",
        "transactions_df = transactions_df.rename(columns={'mcc': 'mcc_code'})\n",
        "transactions_df = transactions_df.rename(columns={'id': 'transaction_id'})\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "03_變數型態統一及缺失值處理\n",
        "==="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_missing_flags(df: pd.DataFrame, cols: list) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    在 DataFrame 中對指定欄位建立 missing flag 欄位\n",
        "    flag=1 表示缺失值，flag=0 表示非缺失值\n",
        "    \n",
        "    參數\n",
        "    ----\n",
        "    df : pd.DataFrame\n",
        "        輸入的資料框\n",
        "    cols : list\n",
        "        要檢查的欄位名稱清單\n",
        "    \n",
        "    回傳\n",
        "    ----\n",
        "    pd.DataFrame : 新的資料框 (含新增的 flag 欄位)\n",
        "    \"\"\"\n",
        "    for col in cols:\n",
        "        df[f\"{col}_missing_flag\"] = df[col].isna().astype(int)\n",
        "    return df\n",
        "\n",
        "transactions_df = add_missing_flags(transactions_df, [\"merchant_state\", \"zip\", \"errors\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "##train_fraud_labels_df##\n",
        "train_fraud_labels_df[\"is_fraud\"]=train_fraud_labels_df[\"is_fraud\"].astype(\"category\") \n",
        "train_fraud_labels_df[\"transactions_id\"]=train_fraud_labels_df[\"transactions_id\"].astype(int) #合併資料需要\n",
        "\n",
        "##cards_df##\n",
        "cards_df[\"card_brand\"]=cards_df[\"card_brand\"].astype(\"category\") \n",
        "cards_df[\"card_type\"]=cards_df[\"card_type\"].astype(\"category\")\n",
        "#####不要load這行 cards_df[\"expires\"]=pd.to_datetime(cards_df[\"expires\"], format=\"%m/%Y\")\n",
        "cards_df[\"expires\"] = pd.to_datetime(cards_df[\"expires\"], format=\"%m/%Y\").dt.to_period(\"M\")\n",
        "cards_df[\"has_chip\"]=cards_df[\"has_chip\"].astype(\"category\")\n",
        "\n",
        "cards_df['credit_limit'] = cards_df['credit_limit'].replace(r'[\\$,]', '', regex=True).astype(int)\n",
        "#####不要load這行 cards_df[\"acct_open_date\"]=pd.to_datetime(cards_df[\"acct_open_date\"], format=\"%m/%Y\")\n",
        "cards_df[\"acct_open_date\"] = pd.to_datetime(cards_df[\"acct_open_date\"], format=\"%m/%Y\").dt.to_period(\"M\")\n",
        "#####不要load這行 cards_df[\"year_pin_last_changed\"]=pd.to_datetime(cards_df[\"year_pin_last_changed\"], format=\"%Y\")\n",
        "cards_df[\"year_pin_last_changed\"] = pd.to_datetime(cards_df[\"year_pin_last_changed\"], format=\"%Y\").dt.to_period(\"Y\")\n",
        "cards_df[\"card_on_dark_web\"]=cards_df[\"card_on_dark_web\"].astype(\"category\") \n",
        "\n",
        "##users_df##\n",
        "users_df[\"birth_year\"] = pd.to_datetime(users_df[\"birth_year\"], format=\"%Y\").dt.to_period(\"Y\")\n",
        "users_df[\"birth_month\"] = pd.to_datetime(users_df[\"birth_month\"], format=\"%m\").dt.to_period(\"M\")\n",
        "users_df[\"gender\"]=users_df[\"gender\"].astype(\"category\") \n",
        "users_df['per_capita_income'] = users_df['per_capita_income'].replace(r'[\\$,]', '', regex=True).astype(int)\n",
        "users_df['yearly_income'] = users_df['yearly_income'].replace(r'[\\$,]', '', regex=True).astype(int)\n",
        "users_df['total_debt'] = users_df['total_debt'].replace(r'[\\$,]', '', regex=True).astype(int)\n",
        "\n",
        "##transactions_df##\n",
        "transactions_df[\"date\"] = pd.to_datetime(transactions_df[\"date\"])\n",
        "#浮點數轉整數原因確定？\n",
        "transactions_df['amount'] = transactions_df['amount'].replace(r'[\\$,]', '', regex=True).astype(float).astype(int)\n",
        "##負數取log調成1\n",
        "#transactions_df['amount'] = transactions_df['amount'].replace(r'[\\$,]', '', regex=True).astype(float)\n",
        "\n",
        "transactions_df[\"use_chip\"]=transactions_df[\"use_chip\"].astype(\"category\") \n",
        "\n",
        "transactions_df.loc[\n",
        "    transactions_df['merchant_city'].str.lower() == 'online',\n",
        "    'merchant_state'\n",
        "] = 'online'\n",
        "\n",
        "transactions_df.loc[\n",
        "    transactions_df['merchant_city'].str.lower() == 'online',\n",
        "    'zip'\n",
        "] = -1\n",
        "## 我沒有全部改，這樣完之後仍有89006筆Missing，剩下都是在國外\n",
        "transactions_df['zip'] = transactions_df['zip'].fillna(-999)\n",
        "transactions_df[\"zip\"]=transactions_df[\"zip\"].astype(\"int64\")\n",
        "\n",
        "transactions_df['errors'] = transactions_df['errors'].astype('category')\n",
        "transactions_df['errors'] = transactions_df['errors'].cat.add_categories('No_error').fillna('No_error')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "#cars one hot encoding\n",
        "##統一類別變數轉dummy variable(要注意共線性問題，應刪掉其中之一)\n",
        "\n",
        "#card_type 原始種類：Debit_57%, Credit_33%, Debit(Prepaid)_9%\n",
        "#card_brand 原始種類：MasterCard_52%, Visa_38%, Amex_7%, Discovery_3%\n",
        "#has_chip 原始種類：Yes_89%, No_11%\n",
        "#card_on_dark_web 原始種類：No_0%\n",
        "cols_to_encode = ['card_type', 'card_brand', 'has_chip']\n",
        "cards_df[cols_to_encode] = cards_df[cols_to_encode].astype('category')\n",
        "dummies_cards = pd.get_dummies(\n",
        "    cards_df[cols_to_encode], \n",
        "    prefix=cols_to_encode, \n",
        "    dtype='uint8'\n",
        "    )\n",
        "cards_df = pd.concat([cards_df, dummies_cards], axis=1)\n",
        "\n",
        "#use_chip 原始種類：Swiped_52%, Chipe_36%, Online_12%\n",
        "dummies_use = pd.get_dummies(transactions_df['use_chip'], prefix='use_chip', dtype='uint8')\n",
        "transactions_df = pd.concat([transactions_df, dummies_use], axis=1)\n",
        "\n",
        "#gender 原始種類：Female_51%, Male_49%\n",
        "dummies_gender = pd.get_dummies(users_df['gender'], prefix='gender', dtype='uint8')\n",
        "users_df = pd.concat([users_df, dummies_gender], axis=1)\n",
        "\n",
        "\n",
        "cards_df.drop(columns=[\"has_chip_NO\",\"has_chip\"], inplace=True)\n",
        "transactions_df.drop(columns=[\"use_chip\"], inplace=True)\n",
        "users_df.drop(columns=[\"gender_Female\"], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "##不用執行～～(本來試圖建立對照表將Missing的zip補上)\n",
        "\n",
        "##檢查89006筆Missing的zip\n",
        "c_missing_zip = transactions_df[transactions_df[\"zip\"].isna()]\n",
        "c_mexico_zip = transactions_df[transactions_df[\"merchant_state\"]==\"Mexico\"]\n",
        "#c_mcc_mv_zip = c_missing_zip[\n",
        "#    (c_missing_zip[\"mcc_code\"] > 5400) & (c_missing_zip[\"mcc_code\"] < 5700)\n",
        "#]\n",
        "\n",
        "\n",
        "\n",
        "# 先建立 mapping table：一組 state+city 可能對應多個 zip\n",
        "mapping_df = (\n",
        "    transactions_df\n",
        "    .dropna(subset=[\"zip\"])                                   # 只要 zip 有值的 row\n",
        "    .drop_duplicates(subset=[\"merchant_state\", \"merchant_city\", \"zip\"]) \n",
        "    [[\"merchant_state\", \"merchant_city\", \"zip\"]]              # 只留下需要的欄位\n",
        ")\n",
        "\n",
        "print(mapping_df.head())\n",
        "\n",
        "\n",
        "# 假設 df 已經存在\n",
        "# 建立新的欄位 F，B 與 C 合併\n",
        "c_missing_zip[\"fullname\"] = c_missing_zip[\"merchant_city\"].astype(str) + c_missing_zip[\"merchant_state\"].astype(str)\n",
        "# 建立新的 DataFrame，只取 A, D, F\n",
        "df_small = c_missing_zip[[\"transaction_id\", \"fullname\",\"zip\"]]\n",
        "\n",
        "mapping_df[\"mfullname\"] = mapping_df[\"merchant_city\"].astype(str) + mapping_df[\"merchant_state\"].astype(str)\n",
        "\n",
        "# 先建立一個 lookup 字典\n",
        "lookup_dict = dict(zip(mapping_df[\"mfullname\"], mapping_df[\"zip\"]))\n",
        "\n",
        "# 用 map 當作 vlookup\n",
        "df_small[\"zip\"] = df_small[\"zip\"].fillna(df_small[\"fullname\"].map(lookup_dict))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "05_data資料整合\n",
        "==="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "#transactions_df.loc[transactions_df[\"transaction_id\"] == 10649266] #transaction_id vs id\n",
        "\n",
        "#原始資料筆數：13305915\n",
        "### transactions_df+train_fraud_labels_df      left 會有4390952 missing values\n",
        "merged = pd.merge(transactions_df, train_fraud_labels_df, left_on=\"transaction_id\", right_on=\"transactions_id\", how=\"outer\")\n",
        "### transactions_df train_fraud_labels_df(8914963) + users_df 對過去不會有missing values\n",
        "merged = pd.merge(merged,users_df , left_on=\"client_id\", right_on=\"client_id\", how=\"left\")\n",
        "### transactions_df train_fraud_labels_df users_df + cards_df 對過去不會有missing values\n",
        "merged = pd.merge(merged,cards_df , left_on=\"card_id\", right_on=\"card_id\", how=\"left\")\n",
        "\n",
        "#刪掉重複的columns\n",
        "merged.drop(columns=[\"transactions_id\"], inplace=True)\n",
        "merged.drop(columns=[\"client_id_y\"], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "del transactions_df, users_df, cards_df, train_fraud_labels_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged[\"is_fraud\"] = merged[\"is_fraud\"].astype(str)\n",
        "merged.loc[merged['is_fraud'].str.lower() == 'no','is_fraud'] = '0'\n",
        "merged.loc[merged['is_fraud'].str.lower() == 'yes','is_fraud'] = '1'\n",
        "merged[\"is_fraud\"] = pd.to_numeric(merged[\"is_fraud\"], errors=\"coerce\").astype(\"Int64\")\n",
        "\n",
        "merged = add_missing_flags(merged, [\"is_fraud\"])\n",
        "\n",
        "#merged.to_csv(\"merged.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "del cols_to_encode, dummies_cards, dummies_use, dummies_gender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "#merged[[\"card_id\",\"card_number\"]]\n",
        "import numpy as np\n",
        "\n",
        "merged['amount'] = np.where(merged['amount'] < 0, 0, merged['amount'])  # 負數變 0\n",
        "merged['amount'] = np.log(merged['amount'] + 1)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "06_EDA_Exploratory-Data-Analysis\n",
        "=="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "06-1_資料型態\n",
        "=="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "merged資料：8914963x37"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "06-2_資料統計指標\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged.describe(include='all')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "06-3_類別型資料frequency barchart\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "cat_cols = merged.select_dtypes(include=[\"category\"]).columns\n",
        "\n",
        "n_rows, n_cols = 4, 2\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 50))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, col in enumerate(cat_cols):\n",
        "    ax = axes[i]\n",
        "    sns.countplot(data=merged, x=col, order=merged[col].value_counts().index, ax=ax)\n",
        "    ax.set_title(f\"Bar chart of {col}\")\n",
        "    ax.set_xlabel(col)\n",
        "    ax.set_ylabel(\"Count\")\n",
        "    if col == \"errors\":\n",
        "        ax.tick_params(axis='x', rotation=90)  # X軸標籤旋轉\n",
        "    else:\n",
        "        ax.tick_params(axis='x', rotation=0)  # X軸標籤旋轉\n",
        "    \n",
        "    # 在長條圖上加數字\n",
        "    for p in ax.patches:\n",
        "        height = p.get_height()\n",
        "        ax.text(x=p.get_x() + p.get_width()/2,\n",
        "                y=height + 0.05,\n",
        "                s=int(height),\n",
        "                ha='center')\n",
        "\n",
        "# 移除多餘空白子圖\n",
        "for j in range(i+1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "06-4_數值型資料histogram\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 設定 subplot 格式\n",
        "n_cols = 4   # 每列放4張圖\n",
        "n_rows = 6   # 每行放6列 (共 4x6=24)\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(20,15))  # 調整大小\n",
        "axes = axes.flatten()  # 攤平成一維方便迭代\n",
        "num_cols = merged.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "for i, col in enumerate(num_cols):\n",
        "    sns.histplot(data=merged, x=col, bins=30, kde=True, ax=axes[i])\n",
        "    axes[i].set_title(col)\n",
        "\n",
        "# 把多餘的 subplot 關掉（避免空白框）\n",
        "for j in range(i+1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "06-5_類別型資料box plot\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 抓出數值型欄位\n",
        "num_cols = merged.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# 建立 3x8 subplot\n",
        "fig, axes = plt.subplots(8, 3, figsize=(30, 50))  # 依照需求調整 figsize\n",
        "axes = axes.flatten()  # 攤平成一維 array，方便迴圈\n",
        "\n",
        "# 逐一畫圖\n",
        "for i, col in enumerate(num_cols):\n",
        "    sns.boxplot(y=merged[col], ax=axes[i])  # 每個 subplot 畫一個 boxplot\n",
        "    axes[i].set_title(col, fontsize=10)\n",
        "\n",
        "# 如果欄位數小於 3x8，隱藏多餘的子圖\n",
        "for j in range(len(num_cols), len(axes)):\n",
        "    axes[j].axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "06-7_數值型資料pair wise scatterplot(畫不出來？)\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_cols = merged.select_dtypes(include=['int64', 'float64']).columns\n",
        "sns.pairplot(merged[num_cols])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "06-8_針對詐騙標籤轉成dummy variable\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cols_to_encode = ['is_fraud']\n",
        "merged[cols_to_encode] = merged[cols_to_encode].astype('category')\n",
        "dummies_cards = pd.get_dummies(\n",
        "    merged[cols_to_encode], \n",
        "    prefix=cols_to_encode, \n",
        "    dtype='uint8'\n",
        "    )\n",
        "merged = pd.concat([merged, dummies_cards], axis=1)\n",
        "merged.drop(columns=[\"is_fraud_No\",\"is_fraud\"], inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged[\"is_fraud_Yes\"]=merged[\"is_fraud_Yes\"].astype(\"int64\")\n",
        "target = 'is_fraud_Yes'  # 假設這是目標\n",
        "num_cols = merged.select_dtypes(include=['int64','float64']).columns.drop(target)\n",
        "\n",
        "for col in num_cols:\n",
        "    plt.figure(figsize=(5,4))\n",
        "    plt.scatter(merged[col], merged[target], alpha=0.3)  # alpha降低透明度，避免太擠\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel(target)\n",
        "    plt.title(f\"{target} vs {col}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "06-9_其他觀察 詐騙與否跟時間的關係\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 確保 date 是 datetime 格式\n",
        "merged[\"date\"] = pd.to_datetime(merged[\"date\"])\n",
        "\n",
        "# 按天統計詐騙事件數\n",
        "fraud_per_day = merged.groupby(merged[\"date\"].dt.date)[\"is_fraud_Yes\"].sum()\n",
        "\n",
        "# 畫折線圖\n",
        "plt.figure(figsize=(12,5))\n",
        "fraud_per_day.plot(kind=\"line\", marker=\"o\")\n",
        "plt.title(\"Daily Fraud Counts 日期 vs 詐騙次數\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Number of Frauds\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# 按小時\n",
        "merged[\"hour\"] = merged[\"date\"].dt.hour\n",
        "hourly_fraud = merged.groupby(\"hour\")[\"is_fraud_Yes\"].sum()\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "hourly_fraud.plot(kind=\"bar\")\n",
        "plt.title(\"Fraud Counts by Hour of Day\")\n",
        "plt.xlabel(\"Hour\")\n",
        "plt.ylabel(\"Number of Frauds\")\n",
        "plt.show()\n",
        "\n",
        "# 按星期幾\n",
        "merged[\"weekday\"] = merged[\"date\"].dt.day_name()\n",
        "weekday_fraud = merged.groupby(\"weekday\")[\"is_fraud_Yes\"].sum().reindex(\n",
        "    [\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"]\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "weekday_fraud.plot(kind=\"bar\")\n",
        "plt.title(\"Fraud Counts by Weekday\")\n",
        "plt.ylabel(\"Number of Frauds\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 想確認原始交易分布與詐騙無關\n",
        "# 取出小時\n",
        "merged[\"hour\"] = merged[\"date\"].dt.hour\n",
        "\n",
        "# 按小時計算交易數\n",
        "transactions_per_hour = merged[\"hour\"].value_counts().sort_index()\n",
        "\n",
        "# 畫長條圖\n",
        "plt.figure(figsize=(12,5))\n",
        "transactions_per_hour.plot(kind=\"bar\")\n",
        "plt.title(\"Transaction Distribution by Hour of Day\")\n",
        "plt.xlabel(\"Hour of Day\")\n",
        "plt.ylabel(\"Number of Transactions\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "06-10_correlation and heatmap\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "numeric_df = merged.select_dtypes(include=['int64', 'float64'])\n",
        "corr = numeric_df.corr()\n",
        "print(corr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,20))\n",
        "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", center=0)\n",
        "plt.title(\"Correlation Heatmap of Numeric Variables\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 原始資料 correlation ---\n",
        "corr_raw = numeric_df.corr()\n",
        "\n",
        "# --- 標準化後 correlation ---\n",
        "scaler = StandardScaler()\n",
        "num_scaled = scaler.fit_transform(numeric_df)   # 轉換成 Numpy array\n",
        "num_df_scaled = pd.DataFrame(num_scaled, columns=numeric_df.columns)\n",
        "corr_scaled = num_df_scaled.corr()\n",
        "\n",
        "# --- 繪圖 (上下對照) ---\n",
        "fig, axes = plt.subplots(2, 1, figsize=(12, 14))\n",
        "\n",
        "sns.heatmap(corr_raw, annot=True, fmt=\".2f\", cmap=\"coolwarm\", center=0, ax=axes[0])\n",
        "axes[0].set_title(\"Correlation Heatmap (Raw Data)\")\n",
        "\n",
        "sns.heatmap(corr_scaled, annot=True, fmt=\".2f\", cmap=\"coolwarm\", center=0, ax=axes[1])\n",
        "axes[1].set_title(\"Correlation Heatmap (Standardized Data)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "07_categoracal 轉 dummy分析\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "WVTi5S6XrUyN",
        "outputId": "7c85d826-19f0-4213-a5de-613565d7244e"
      },
      "outputs": [],
      "source": [
        "info_df = pd.DataFrame({\n",
        "    \"column\": merged.columns,\n",
        "    \"dtype\": merged.dtypes.astype(str)\n",
        "})\n",
        "info_df.to_csv(\"info.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "08_Benchmark model\n",
        "==\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_cols = merged.select_dtypes(include=['int64', 'float64','uint8']).columns\n",
        "df=merged[num_cols]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_cleaned = df.dropna()\n",
        "del df\n",
        "\n",
        "df_cleaned.drop(columns=[\"is_fraud_missing_flag\",\"card_type_Debit (Prepaid)\", \"card_brand_Discover\", \"use_chip_Online Transaction\"], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, test_df = train_test_split(df_cleaned, test_size=0.2, random_state=888)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "is_fraud\n",
            "0    7121379\n",
            "1      10591\n",
            "Name: count, dtype: Int64\n",
            "is_fraud\n",
            "0    1780252\n",
            "1       2741\n",
            "Name: count, dtype: Int64\n"
          ]
        }
      ],
      "source": [
        "del df_cleaned, merged\n",
        "trainp = train_df['is_fraud'].value_counts(normalize=False)\n",
        "print(trainp)\n",
        "testp = test_df['is_fraud'].value_counts(normalize=False)\n",
        "print(testp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/regression/linear_model.py:1782: RuntimeWarning: divide by zero encountered in scalar divide\n",
            "  return 1 - self.ssr/self.centered_tss\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                       features  VIF Factor\n",
            "24                  card_number   22.245529\n",
            "18            per_capita_income   12.894211\n",
            "19                yearly_income   12.618209\n",
            "30              card_brand_Amex    4.407098\n",
            "9              zip_missing_flag    2.106343\n",
            "8   merchant_state_missing_flag    1.987286\n",
            "6                           zip    1.919523\n",
            "11    use_chip_Chip Transaction    1.800294\n",
            "12   use_chip_Swipe Transaction    1.791025\n",
            "17                    longitude    1.745601\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "def calculate_vif(df):\n",
        "    # 1. 保留數值欄位\n",
        "    df_num = df.select_dtypes(include=[np.number]).copy()\n",
        "\n",
        "    # 2. 強制轉成 float64，避免 Int64 / uint8 / object 問題\n",
        "    df_num = df_num.astype(np.float64)\n",
        "\n",
        "    # 3. 檢查 inf / NaN\n",
        "    if not np.isfinite(df_num.values).all():\n",
        "        raise ValueError(\"Data contains NaN or infinite values, cannot compute VIF.\")\n",
        "\n",
        "    # 4. 加上截距\n",
        "    X = sm.add_constant(df_num)\n",
        "\n",
        "    # 5. 計算 VIF\n",
        "    vif = pd.DataFrame()\n",
        "    vif[\"features\"] = X.columns\n",
        "    vif[\"VIF Factor\"] = [variance_inflation_factor(X.values, i) \n",
        "                         for i in range(X.shape[1])]\n",
        "\n",
        "    return vif\n",
        "\n",
        "# 使用範例\n",
        "vif_result = calculate_vif(train_df)\n",
        "print(vif_result.sort_values(by=\"VIF Factor\", ascending=False).head(10))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4070 / 7131970\n"
          ]
        }
      ],
      "source": [
        "##第一次處理共線性\n",
        "train_df.drop(columns=[\"per_capita_income\"], inplace=True)\n",
        "##觀察card_number的異常處\n",
        "print(train_df[\"card_number\"].nunique(), \"/\", len(train_df))\n",
        "##第一次處理card_number\n",
        "train_df.drop(columns=[\"card_number\"], inplace=True)\n",
        "#再重跑一次VIF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                       features   VIF Factor\n",
            "0                         const  3105.658128\n",
            "12   use_chip_Swipe Transaction   569.053663\n",
            "11    use_chip_Chip Transaction   524.231371\n",
            "8   merchant_state_missing_flag   250.962104\n",
            "9              zip_missing_flag    17.641670\n",
            "29        card_brand_Mastercard    11.382578\n",
            "30              card_brand_Visa    10.554165\n",
            "27              card_type_Debit     4.934019\n",
            "26             card_type_Credit     4.574125\n",
            "6                           zip     3.879782\n"
          ]
        }
      ],
      "source": [
        "vif_result = calculate_vif(train_df)\n",
        "print(vif_result.sort_values(by=\"VIF Factor\", ascending=False).head(10))\n",
        "##發現missing_flag的共線性問題，決定保留one hot encoding高vif值的變數"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df.drop(columns=[\"zip_missing_flag\",\"merchant_state_missing_flag\"], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                      features  VIF Factor\n",
            "0                        const  855.050961\n",
            "27       card_brand_Mastercard   11.382523\n",
            "28             card_brand_Visa   10.554143\n",
            "10  use_chip_Swipe Transaction    5.281126\n",
            "9    use_chip_Chip Transaction    5.178922\n",
            "25             card_type_Debit    4.933947\n",
            "24            card_type_Credit    4.573924\n",
            "6                          zip    3.645811\n",
            "26             card_brand_Amex    3.328266\n",
            "15                   longitude    2.709260\n"
          ]
        }
      ],
      "source": [
        "vif_result = calculate_vif(train_df)\n",
        "print(vif_result.sort_values(by=\"VIF Factor\", ascending=False).head(10))\n",
        "##移除missing_flag共線性問題，再次確認"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                      variable   coefficient  p_value\n",
            "0    use_chip_Chip Transaction -5.792166e-01   0.0000\n",
            "1              card_type_Debit -2.431305e-01   0.0000\n",
            "2             card_type_Credit  1.187650e-01   0.0000\n",
            "3                       amount  4.194989e-01   0.0000\n",
            "4                  merchant_id  5.815498e-06   0.0000\n",
            "5                          zip -1.023342e-04   0.0000\n",
            "6                     mcc_code -5.383161e-04   0.0000\n",
            "7          errors_missing_flag -1.034501e+00   0.0000\n",
            "8                   total_debt -1.507820e-06   0.0000\n",
            "9   use_chip_Swipe Transaction -2.282721e+00   0.0000\n",
            "10                 current_age  6.811940e-03   0.0000\n",
            "11                credit_limit -1.325726e-05   0.0000\n",
            "12                    latitude -8.201997e-03   0.0000\n",
            "13            num_credit_cards  1.108020e-01   0.0000\n",
            "14               yearly_income -4.518766e-06   0.0000\n",
            "15                has_chip_YES  1.266255e-01   0.0002\n",
            "16                credit_score  4.469540e-04   0.0027\n",
            "17                   longitude -1.721663e-03   0.0035\n",
            "18                     card_id -1.531231e-05   0.0081\n",
            "19             card_brand_Visa -4.786636e-02   0.0180\n",
            "20             card_brand_Amex  8.191560e-02   0.0324\n",
            "21                 gender_Male -3.298179e-02   0.0902\n",
            "22              transaction_id  2.666966e-09   0.1970\n",
            "23       card_brand_Mastercard -2.159213e-02   0.2679\n",
            "24              retirement_age  2.897547e-03   0.2859\n",
            "25                         cvv  3.560710e-05   0.2907\n",
            "26                 client_id_x -1.477671e-05   0.3766\n",
            "27            num_cards_issued -7.095726e-03   0.7070\n"
          ]
        }
      ],
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "# assume train_df is your dataframe and \"is_fraud\" is the dependent variable\n",
        "y = train_df[\"is_fraud\"]\n",
        "\n",
        "# exclude the dependent variable itself\n",
        "independent_vars = train_df.columns.drop(\"is_fraud\")\n",
        "\n",
        "results = []\n",
        "\n",
        "for var in independent_vars:\n",
        "    X = sm.add_constant(train_df[var])  # add intercept\n",
        "    model = sm.Logit(y, X)\n",
        "    try:\n",
        "        result = model.fit(disp=False)\n",
        "        coef = result.params[var]\n",
        "        pval = np.around(result.pvalues[var], 4)\n",
        "        results.append({\"variable\": var, \"coefficient\": coef, \"p_value\": pval})\n",
        "    except Exception as e:\n",
        "        results.append({\"variable\": var, \"coefficient\": None, \"p_value\": None})\n",
        "        print(f\"Skipped {var} due to error: {e}\")\n",
        "\n",
        "# convert to dataframe\n",
        "summary_df = pd.DataFrame(results)\n",
        "\n",
        "# optional: sort by p_value\n",
        "summary_df = summary_df.sort_values(\"p_value\", ascending=True).reset_index(drop=True)\n",
        "\n",
        "print(summary_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Table: Logistic Regression Stepwise Estimation — Adding amount\n",
            "\n",
            "Overall Model Fit: Goodness-of-Fit Measures\n",
            "\n",
            "                       Measure      Value Change_from_Base Change_pvalue\n",
            "-2 Log Likelihood (−2LL) value 156596.132         -2514.48           0.0\n",
            "              Cox and Snell R2      0.000                               \n",
            "                 Nagelkerke R2      0.016                               \n",
            "          Pseudo R2 (McFadden)      0.016                               \n",
            "            Hosmer-Lemeshow χ2   3046.494                            0.0\n",
            "\n",
            "\n",
            "Variables in the Equation\n",
            "\n",
            "Independent Variable       B  Std. Error       Wald  df  Sig.  Exp(B)\n",
            "              amount  0.0026      0.0000   4155.605   1   0.0  1.0026\n",
            "            Constant -6.6693      0.0104 408490.171   1   0.0  0.0013\n",
            "\n",
            "\n",
            "Variables Not in the Equation (candidates and LRT vs base)\n",
            "\n",
            "      Independent Variable  Score Statistic (LRT)  Significance\n",
            "                       zip              25168.126        0.0000\n",
            "                  mcc_code               1850.618        0.0000\n",
            "use_chip_Swipe Transaction               8580.841        0.0000\n",
            " use_chip_Chip Transaction                707.825        0.0000\n",
            "       errors_missing_flag                348.748        0.0000\n",
            "          num_credit_cards                330.237        0.0000\n",
            "               merchant_id                241.641        0.0000\n",
            "              credit_limit                223.331        0.0000\n",
            "           card_type_Debit                151.787        0.0000\n",
            "               current_age                125.967        0.0000\n",
            "             yearly_income                105.587        0.0000\n",
            "                total_debt                 59.954        0.0000\n",
            "          card_type_Credit                 32.764        0.0000\n",
            "                  latitude                 18.643        0.0000\n",
            "              has_chip_YES                 14.268        0.0002\n",
            "              credit_score                  9.045        0.0026\n",
            "                 longitude                  8.450        0.0037\n",
            "                   card_id                  6.996        0.0082\n",
            "           card_brand_Visa                  5.618        0.0178\n",
            "           card_brand_Amex                  4.471        0.0345\n",
            "               gender_Male                  2.872        0.0901\n",
            "            transaction_id                  1.665        0.1970\n",
            "     card_brand_Mastercard                  1.227        0.2680\n",
            "            retirement_age                  1.140        0.2857\n",
            "                       cvv                  1.116        0.2907\n",
            "               client_id_x                  0.782        0.3767\n",
            "          num_cards_issued                  0.141        0.7070\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/yb/xnfk9z6x34527z3924bcjqcr0000gn/T/ipykernel_1455/99504724.py:17: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  grouped = df.groupby(\"group\")\n"
          ]
        }
      ],
      "source": [
        "#(寫錯了不要使用)放入第一個變數會怎樣\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from scipy.stats import chi2\n",
        "from sklearn.utils import check_array\n",
        "\n",
        "def hosmer_lemeshow_test(y_true, y_prob, g=10):\n",
        "    \"\"\"\n",
        "    Hosmer-Lemeshow test (group into g quantiles by predicted prob).\n",
        "    Returns (chi2, pvalue, table_df)\n",
        "    \"\"\"\n",
        "    # create dataframe\n",
        "    df = pd.DataFrame({\"y\": np.asarray(y_true), \"yhat\": np.asarray(y_prob)})\n",
        "    # create g groups by quantile of predicted prob\n",
        "    df[\"group\"] = pd.qcut(df[\"yhat\"], q=g, duplicates=\"drop\")\n",
        "    grouped = df.groupby(\"group\")\n",
        "    \n",
        "    obs = grouped[\"y\"].sum()\n",
        "    n = grouped.size()\n",
        "    exp = grouped[\"yhat\"].sum()\n",
        "    \n",
        "    # HL chi2: sum ( (O - E)^2 / (E*(1 - E/n)) )  -- alternative formulation:\n",
        "    # Common simple formula: sum((O - E)^2 / (E*(1 - E/n_i))) is unstable;\n",
        "    # We'll use classical: sum((O - E)^2 / (E*(1 - E/n_i))) where E is expected count in group\n",
        "    # But many use: sum((O - E)^2 / (E*(1 - E/n_i))) ; here we fallback to standard HL:\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        term = (obs - exp) ** 2 / (exp * (1 - exp / n))\n",
        "        term = term.replace([np.inf, -np.inf], 0).fillna(0)\n",
        "    chi2_stat = term.sum()\n",
        "    df_hl = max(1, len(n) - 2)\n",
        "    pvalue = chi2.sf(chi2_stat, df_hl)\n",
        "    \n",
        "    table = pd.DataFrame({\n",
        "        \"group\": n.index.astype(str),\n",
        "        \"n_obs\": n.values,\n",
        "        \"y_obs\": obs.values,\n",
        "        \"y_exp\": exp.values\n",
        "    })\n",
        "    return chi2_stat, pvalue, table\n",
        "\n",
        "def table_for_first_step(train_df, target_col=\"is_fraud\", g_hl=10, max_vars_display=None):\n",
        "    \"\"\"\n",
        "    Automatically pick the best single variable to add (by LRT vs base) and\n",
        "    produce three tables in the style of the textbook for the first step:\n",
        "      - Overall Model Fit (with change from base)\n",
        "      - Variables in the Equation (the added variable)\n",
        "      - Variables Not in the Equation (LRT for other candidates)\n",
        "    Returns: dict with dataframes: goodness_df, in_eq_df, not_in_eq_df\n",
        "    \"\"\"\n",
        "    train_df = train_df.reset_index(drop=True)\n",
        "    y = train_df[target_col].astype(int).reset_index(drop=True)\n",
        "    candidates = list(train_df.columns.drop(target_col))\n",
        "    n = len(train_df)\n",
        "    \n",
        "    # Base model (intercept only)\n",
        "    X_base = sm.add_constant(pd.DataFrame({\"intercept\": np.ones(n)}))\n",
        "    base_model = sm.Logit(y, X_base).fit(disp=False)\n",
        "    ll_base = base_model.llf\n",
        "    minus2ll_base = -2 * ll_base\n",
        "    \n",
        "    # For each candidate, fit model with that single variable and compute LRT\n",
        "    lrt_list = []\n",
        "    for var in candidates:\n",
        "        try:\n",
        "            Xv = sm.add_constant(train_df[[var]])\n",
        "            mv = sm.Logit(y, Xv).fit(disp=False)\n",
        "            ll_v = mv.llf\n",
        "            lr_stat = -2 * (ll_base - ll_v)   # change in -2LL\n",
        "            pval = chi2.sf(lr_stat, df=1)\n",
        "            lrt_list.append({\"variable\": var, \"lr_stat\": lr_stat, \"p_value\": pval, \"ll_v\": ll_v})\n",
        "        except Exception as e:\n",
        "            lrt_list.append({\"variable\": var, \"lr_stat\": None, \"p_value\": None, \"ll_v\": None})\n",
        "            # continue even on errors\n",
        "    \n",
        "    lrt_df = pd.DataFrame(lrt_list).sort_values(\"p_value\", na_position=\"last\").reset_index(drop=True)\n",
        "    \n",
        "    # pick best variable (smallest p-value, and lr_stat not null)\n",
        "    best_row = lrt_df.dropna(subset=[\"p_value\"]).sort_values(\"p_value\").iloc[0]  ###原本只有1個\n",
        "    best_var = best_row[\"variable\"]                                              ###原本只有1個\n",
        "    ###k = 2  # 想放幾個變數就改這裡\n",
        "    ###top_k_vars = lrt_df.dropna(subset=[\"p_value\"]).sort_values(\"p_value\").head(k)[\"variable\"].tolist()\n",
        "    \n",
        "    # Fit model with the chosen variable\n",
        "    X_best = sm.add_constant(train_df[[best_var]])  ###原本只有1個\n",
        "    ###X_best = sm.add_constant(train_df[top_k_vars])\n",
        "    ###best_var = \", \".join(top_k_vars)\n",
        "\n",
        "    best_model = sm.Logit(y, X_best).fit(disp=False)\n",
        "    ll_best = best_model.llf\n",
        "    minus2ll_best = -2 * ll_best\n",
        "    change_from_base = minus2ll_best - minus2ll_base  # how -2LL changed (positive if decreased in LL)\n",
        "    # significance of change (LRT)\n",
        "    lr_stat_best = -2 * (ll_base - ll_best)\n",
        "    p_change = chi2.sf(lr_stat_best, df=1)\n",
        "    \n",
        "    # R-squared variants\n",
        "    ll_null = ll_base\n",
        "    ll_model = ll_best\n",
        "    # McFadden\n",
        "    pseudo_r2 = 1 - (ll_model / ll_null)\n",
        "    # Cox & Snell\n",
        "    try:\n",
        "        r_cs = 1 - np.exp((2.0 / n) * (ll_null - ll_model))\n",
        "    except:\n",
        "        r_cs = np.nan\n",
        "    # Nagelkerke (adjusted Cox & Snell)\n",
        "    try:\n",
        "        r_max = 1 - np.exp((2.0 / n) * ll_null)\n",
        "        nagelkerke = r_cs / r_max if r_max != 0 else np.nan\n",
        "    except:\n",
        "        nagelkerke = np.nan\n",
        "    \n",
        "    # Hosmer-Lemeshow\n",
        "    preds = best_model.predict(X_best)\n",
        "    hl_chi2, hl_p, hl_table = hosmer_lemeshow_test(y, preds, g=g_hl)\n",
        "    \n",
        "    # Build Goodness-of-Fit DataFrame (layout like textbook)\n",
        "    goodness_rows = [\n",
        "        {\"Measure\": \"-2 Log Likelihood (−2LL) value\", \"Value\": round(minus2ll_best, 3),\n",
        "         \"Change_from_Base\": round(change_from_base, 3), \"Change_pvalue\": round(p_change, 4)},\n",
        "        {\"Measure\": \"Cox and Snell R2\", \"Value\": round(r_cs, 3), \"Change_from_Base\": \"\", \"Change_pvalue\": \"\"},\n",
        "        {\"Measure\": \"Nagelkerke R2\", \"Value\": round(nagelkerke, 3), \"Change_from_Base\": \"\", \"Change_pvalue\": \"\"},\n",
        "        {\"Measure\": \"Pseudo R2 (McFadden)\", \"Value\": round(pseudo_r2, 3), \"Change_from_Base\": \"\", \"Change_pvalue\": \"\"},\n",
        "        {\"Measure\": \"Hosmer-Lemeshow χ2\", \"Value\": round(hl_chi2, 3), \"Change_from_Base\": \"\", \"Change_pvalue\": round(hl_p, 4)}\n",
        "    ]\n",
        "    goodness_df = pd.DataFrame(goodness_rows)\n",
        "    \n",
        "    # Variables in the Equation (the chosen variable)\n",
        "    params = best_model.params\n",
        "    bse = best_model.bse\n",
        "    z = params / bse\n",
        "    wald = (z**2)\n",
        "    pvals = best_model.pvalues\n",
        "    expb = np.exp(params)\n",
        "    in_eq = []\n",
        "    # list chosen var then constant\n",
        "    for var in [best_var, \"const\"]:\n",
        "        if var in params.index:\n",
        "            in_eq.append({\n",
        "                \"Independent Variable\": var if var != \"const\" else \"Constant\",\n",
        "                \"B\": round(params[var], 4),\n",
        "                \"Std. Error\": round(bse[var], 4),\n",
        "                \"Wald\": round(wald[var], 3),\n",
        "                \"df\": 1,\n",
        "                \"Sig.\": round(pvals[var], 4),\n",
        "                \"Exp(B)\": round(expb[var], 4)\n",
        "            })\n",
        "    in_eq_df = pd.DataFrame(in_eq)\n",
        "    \n",
        "    # Variables Not in the Equation: show LRT for all other candidates (vs base model)\n",
        "    not_in = []\n",
        "    for _, row in lrt_df.iterrows():\n",
        "        var = row[\"variable\"]\n",
        "        if var == best_var:\n",
        "            continue\n",
        "        not_in.append({\n",
        "            \"Independent Variable\": var,\n",
        "            \"Score Statistic (LRT)\": None if pd.isna(row[\"lr_stat\"]) else round(row[\"lr_stat\"], 3),\n",
        "            \"Significance\": None if pd.isna(row[\"p_value\"]) else round(row[\"p_value\"], 4)\n",
        "        })\n",
        "    not_in_eq_df = pd.DataFrame(not_in)\n",
        "    \n",
        "    # Optionally truncate long lists\n",
        "    if max_vars_display is not None:\n",
        "        not_in_eq_df = not_in_eq_df.head(max_vars_display)\n",
        "    \n",
        "    # Print summary in textbook style\n",
        "    print(\"Table: Logistic Regression Stepwise Estimation — Adding\", best_var)\n",
        "    print(\"\\nOverall Model Fit: Goodness-of-Fit Measures\\n\")\n",
        "    print(goodness_df.to_string(index=False))\n",
        "    print(\"\\n\\nVariables in the Equation\\n\")\n",
        "    print(in_eq_df.to_string(index=False))\n",
        "    print(\"\\n\\nVariables Not in the Equation (candidates and LRT vs base)\\n\")\n",
        "    print(not_in_eq_df.to_string(index=False))\n",
        "    \n",
        "    return {\n",
        "        \"best_var\": best_var,\n",
        "        \"goodness_df\": goodness_df,\n",
        "        \"in_eq_df\": in_eq_df,\n",
        "        \"not_in_eq_df\": not_in_eq_df,\n",
        "        \"hl_table\": hl_table,\n",
        "        \"best_model\": best_model\n",
        "    }\n",
        "\n",
        "# ===== Example usage =====\n",
        "results = table_for_first_step(train_df, target_col=\"is_fraud\")\n",
        "good_df, in_df, notin_df = results[\"goodness_df\"], results[\"in_eq_df\"], results[\"not_in_eq_df\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#(寫錯了不要使用)\n",
        "from scipy.stats import chi2\n",
        "def table_for_first_step(train_df, target_col=\"is_fraud\", g_hl=10, max_vars_display=None, k=1):\n",
        "    \"\"\"\n",
        "    Automatically pick the best k variables (default=1) to add (by LRT vs base)\n",
        "    and produce tables like textbook.\n",
        "    \"\"\"\n",
        "    train_df = train_df.reset_index(drop=True)\n",
        "    y = train_df[target_col].astype(int).reset_index(drop=True)\n",
        "    candidates = list(train_df.columns.drop(target_col))\n",
        "    n = len(train_df)\n",
        "    \n",
        "    # Base model\n",
        "    X_base = sm.add_constant(pd.DataFrame({\"intercept\": np.ones(n)}))\n",
        "    base_model = sm.Logit(y, X_base).fit(disp=False)\n",
        "    ll_base = base_model.llf\n",
        "    minus2ll_base = -2 * ll_base\n",
        "\n",
        "    # Fit single-variable models for all candidates\n",
        "    lrt_list = []\n",
        "    for var in candidates:\n",
        "        try:\n",
        "            Xv = sm.add_constant(train_df[[var]])\n",
        "            mv = sm.Logit(y, Xv).fit(disp=False)\n",
        "            ll_v = mv.llf\n",
        "            lr_stat = -2 * (ll_base - ll_v)\n",
        "            pval = chi2.sf(lr_stat, df=1)\n",
        "            lrt_list.append({\"variable\": var, \"lr_stat\": lr_stat, \"p_value\": pval, \"ll_v\": ll_v})\n",
        "        except Exception:\n",
        "            lrt_list.append({\"variable\": var, \"lr_stat\": None, \"p_value\": None, \"ll_v\": None})\n",
        "    \n",
        "    lrt_df = pd.DataFrame(lrt_list).sort_values(\"p_value\", na_position=\"last\").reset_index(drop=True)\n",
        "\n",
        "    # ✅ pick top k variables\n",
        "    top_k_vars = lrt_df.dropna(subset=[\"p_value\"]).sort_values(\"p_value\").head(k)[\"variable\"].tolist()\n",
        "    best_var_display = \", \".join(top_k_vars)\n",
        "\n",
        "    # Fit model with chosen variables\n",
        "    X_best = sm.add_constant(train_df[top_k_vars])\n",
        "    best_model = sm.Logit(y, X_best).fit(disp=False)\n",
        "    ll_best = best_model.llf\n",
        "    minus2ll_best = -2 * ll_best\n",
        "\n",
        "    # Model fit improvements\n",
        "    change_from_base = minus2ll_best - minus2ll_base\n",
        "    df_lr = len(best_model.params) - len(base_model.params)\n",
        "    lr_stat_best = -2 * (ll_base - ll_best)\n",
        "    p_change = chi2.sf(lr_stat_best, df=df_lr)\n",
        "\n",
        "    # R2 variants\n",
        "    ll_null = ll_base\n",
        "    ll_model = ll_best\n",
        "    pseudo_r2 = 1 - (ll_model / ll_null)\n",
        "    r_cs = 1 - np.exp((2.0 / n) * (ll_null - ll_model))\n",
        "    r_max = 1 - np.exp((2.0 / n) * ll_null)\n",
        "    nagelkerke = r_cs / r_max if r_max != 0 else np.nan\n",
        "\n",
        "    # Hosmer-Lemeshow\n",
        "    preds = best_model.predict(X_best)\n",
        "    hl_chi2, hl_p, hl_table = hosmer_lemeshow_test(y, preds, g=g_hl)\n",
        "\n",
        "    # ✅ Table 1: Goodness of Fit\n",
        "    goodness_rows = [\n",
        "        {\"Measure\": \"-2 Log Likelihood (−2LL) value\", \"Value\": round(minus2ll_best, 3),\n",
        "         \"Change_from_Base\": round(change_from_base, 3), \"Change_pvalue\": round(p_change, 4)},\n",
        "        {\"Measure\": \"Cox and Snell R2\", \"Value\": round(r_cs, 3), \"Change_from_Base\": \"\", \"Change_pvalue\": \"\"},\n",
        "        {\"Measure\": \"Nagelkerke R2\", \"Value\": round(nagelkerke, 3), \"Change_from_Base\": \"\", \"Change_pvalue\": \"\"},\n",
        "        {\"Measure\": \"Pseudo R2 (McFadden)\", \"Value\": round(pseudo_r2, 3), \"Change_from_Base\": \"\", \"Change_pvalue\": \"\"},\n",
        "        {\"Measure\": \"Hosmer-Lemeshow χ2\", \"Value\": round(hl_chi2, 3), \"Change_from_Base\": \"\", \"Change_pvalue\": round(hl_p, 4)}\n",
        "    ]\n",
        "    goodness_df = pd.DataFrame(goodness_rows)\n",
        "\n",
        "    # ✅ Table 2: Variables in the Equation\n",
        "    params = best_model.params\n",
        "    bse = best_model.bse\n",
        "    z = params / bse\n",
        "    wald = (z ** 2)\n",
        "    pvals = best_model.pvalues\n",
        "    expb = np.exp(params)\n",
        "    in_eq = []\n",
        "    for var in top_k_vars + [\"const\"]:\n",
        "        if var in params.index:\n",
        "            in_eq.append({\n",
        "                \"Independent Variable\": var if var != \"const\" else \"Constant\",\n",
        "                \"B\": round(params[var], 4),\n",
        "                \"Std. Error\": round(bse[var], 4),\n",
        "                \"Wald\": round(wald[var], 3),\n",
        "                \"df\": 1,\n",
        "                \"Sig.\": round(pvals[var], 4),\n",
        "                \"Exp(B)\": round(expb[var], 4)\n",
        "            })\n",
        "    in_eq_df = pd.DataFrame(in_eq)\n",
        "\n",
        "    # ✅ Table 3: Variables Not in the Equation\n",
        "    not_in = []\n",
        "    for _, row in lrt_df.iterrows():\n",
        "        var = row[\"variable\"]\n",
        "        if var in top_k_vars:\n",
        "            continue\n",
        "        not_in.append({\n",
        "            \"Independent Variable\": var,\n",
        "            \"Score Statistic (LRT)\": None if pd.isna(row[\"lr_stat\"]) else round(row[\"lr_stat\"], 3),\n",
        "            \"Significance\": None if pd.isna(row[\"p_value\"]) else round(row[\"p_value\"], 4)\n",
        "        })\n",
        "    not_in_eq_df = pd.DataFrame(not_in)\n",
        "\n",
        "    if max_vars_display is not None:\n",
        "        not_in_eq_df = not_in_eq_df.head(max_vars_display)\n",
        "\n",
        "    # print results\n",
        "    print(f\"Table: Logistic Regression Stepwise Estimation — Adding {best_var_display}\")\n",
        "    print(\"\\nOverall Model Fit: Goodness-of-Fit Measures\\n\")\n",
        "    print(goodness_df.to_string(index=False))\n",
        "    print(\"\\n\\nVariables in the Equation\\n\")\n",
        "    print(in_eq_df.to_string(index=False))\n",
        "    print(\"\\n\\nVariables Not in the Equation (candidates and LRT vs base)\\n\")\n",
        "    print(not_in_eq_df.to_string(index=False))\n",
        "\n",
        "    return {\n",
        "        \"best_vars\": top_k_vars,\n",
        "        \"goodness_df\": goodness_df,\n",
        "        \"in_eq_df\": in_eq_df,\n",
        "        \"not_in_eq_df\": not_in_eq_df,\n",
        "        \"hl_table\": hl_table,\n",
        "        \"best_model\": best_model\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Matrix — Training Sample\n",
            "Actual Group  Predicted Fraud (1)  Predicted Normal (0)   Total  % Correct\n",
            "   Fraud (1)                   16                 10575   10591        0.2\n",
            "  Normal (0)                   54               7121325 7121379      100.0\n",
            "       Total                   70               7131900 7131970       99.9\n",
            "\n",
            "Classification Matrix — Holdout (Test) Sample\n",
            "Actual Group  Predicted Fraud (1)  Predicted Normal (0)   Total  % Correct\n",
            "   Fraud (1)                    4                  2737    2741        0.1\n",
            "  Normal (0)                   15               1780237 1780252      100.0\n",
            "       Total                   19               1782974 1782993       99.8\n"
          ]
        }
      ],
      "source": [
        "#(寫錯了先不要用)\n",
        "#在 train 與 test 上做預測並分類，並產出分類矩陣\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def classification_table(model, df, target_col=\"is_fraud\", cutoff=0.5):\n",
        "    \"\"\"\n",
        "    建立分類矩陣 (confusion matrix) 與正確率統計\n",
        "    \"\"\"\n",
        "    y_true = df[target_col].astype(int)\n",
        "    X = sm.add_constant(df[model.params.index.drop(\"const\")])\n",
        "    y_pred_prob = model.predict(X)\n",
        "    y_pred = (y_pred_prob >= cutoff).astype(int)\n",
        "    \n",
        "    cm = confusion_matrix(y_true, y_pred, labels=[1, 0])\n",
        "    TP, FN, FP, TN = cm.ravel()  # 注意順序是 [ [TP, FN], [FP, TN] ]\n",
        "    \n",
        "    fraud_total = TP + FN\n",
        "    normal_total = FP + TN\n",
        "    \n",
        "    fraud_correct = TP / fraud_total if fraud_total > 0 else 0\n",
        "    normal_correct = TN / normal_total if normal_total > 0 else 0\n",
        "    overall_correct = (TP + TN) / (fraud_total + normal_total)\n",
        "    \n",
        "    table = pd.DataFrame({\n",
        "        \"Actual Group\": [\"Fraud (1)\", \"Normal (0)\", \"Total\"],\n",
        "        \"Predicted Fraud (1)\": [TP, FP, TP+FP],\n",
        "        \"Predicted Normal (0)\": [FN, TN, FN+TN],\n",
        "        \"Total\": [fraud_total, normal_total, fraud_total + normal_total],\n",
        "        \"% Correct\": [round(fraud_correct*100, 1), round(normal_correct*100, 1), round(overall_correct*100, 1)]\n",
        "    })\n",
        "    \n",
        "    return table\n",
        "\n",
        "# 產出訓練集和測試集的分類矩陣\n",
        "best_model = results[\"best_model\"]\n",
        "\n",
        "train_table = classification_table(best_model, train_df, target_col=\"is_fraud\")\n",
        "test_table = classification_table(best_model, test_df, target_col=\"is_fraud\")\n",
        "\n",
        "print(\"Classification Matrix — Training Sample\")\n",
        "print(train_table.to_string(index=False))\n",
        "\n",
        "print(\"\\nClassification Matrix — Holdout (Test) Sample\")\n",
        "print(test_table.to_string(index=False))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0: Base model estimated. -2LL = 159110.612\n",
            "Step 1: Added amount, p = 0.0000, -2LL = 156093.371\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[32m     57\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m step_df, best_model\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# 🚀 執行\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m step_df, final_model = \u001b[43mstepwise_logit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mis_fraud\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m step_df\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mstepwise_logit\u001b[39m\u001b[34m(train_df, target_col, entry_threshold)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     28\u001b[39m     X_temp = sm.add_constant(train_df[included_vars + [var]])\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     model_temp = \u001b[43msm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLogit\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_temp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdisp\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m     pval = model_temp.pvalues[var]\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pval < best_pval:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/discrete/discrete_model.py:2601\u001b[39m, in \u001b[36mLogit.fit\u001b[39m\u001b[34m(self, start_params, method, maxiter, full_output, disp, callback, **kwargs)\u001b[39m\n\u001b[32m   2598\u001b[39m \u001b[38;5;129m@Appender\u001b[39m(DiscreteModel.fit.\u001b[34m__doc__\u001b[39m)\n\u001b[32m   2599\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, start_params=\u001b[38;5;28;01mNone\u001b[39;00m, method=\u001b[33m'\u001b[39m\u001b[33mnewton\u001b[39m\u001b[33m'\u001b[39m, maxiter=\u001b[32m35\u001b[39m,\n\u001b[32m   2600\u001b[39m         full_output=\u001b[32m1\u001b[39m, disp=\u001b[32m1\u001b[39m, callback=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m-> \u001b[39m\u001b[32m2601\u001b[39m     bnryfit = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2602\u001b[39m \u001b[43m                          \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2603\u001b[39m \u001b[43m                          \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2604\u001b[39m \u001b[43m                          \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2605\u001b[39m \u001b[43m                          \u001b[49m\u001b[43mdisp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2606\u001b[39m \u001b[43m                          \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2607\u001b[39m \u001b[43m                          \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2609\u001b[39m     discretefit = LogitResults(\u001b[38;5;28mself\u001b[39m, bnryfit)\n\u001b[32m   2610\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m BinaryResultsWrapper(discretefit)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/discrete/discrete_model.py:243\u001b[39m, in \u001b[36mDiscreteModel.fit\u001b[39m\u001b[34m(self, start_params, method, maxiter, full_output, disp, callback, **kwargs)\u001b[39m\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# TODO: make a function factory to have multiple call-backs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m mlefit = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mdisp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m                     \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m mlefit\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/base/model.py:566\u001b[39m, in \u001b[36mLikelihoodModel.fit\u001b[39m\u001b[34m(self, start_params, method, maxiter, full_output, disp, fargs, callback, retall, skip_hessian, **kwargs)\u001b[39m\n\u001b[32m    563\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_t\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    565\u001b[39m optimizer = Optimizer()\n\u001b[32m--> \u001b[39m\u001b[32m566\u001b[39m xopt, retvals, optim_settings = \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[43m                                               \u001b[49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[43m                                               \u001b[49m\u001b[43mhessian\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[43m                                               \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[43m                                               \u001b[49m\u001b[43mdisp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    571\u001b[39m \u001b[43m                                               \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m                                               \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m                                               \u001b[49m\u001b[43mretall\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretall\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[43m                                               \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[38;5;66;03m# Restore cov_type, cov_kwds and use_t\u001b[39;00m\n\u001b[32m    576\u001b[39m optim_settings.update(kwds)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/base/optimizer.py:245\u001b[39m, in \u001b[36mOptimizer._fit\u001b[39m\u001b[34m(self, objective, gradient, start_params, fargs, kwargs, hessian, method, maxiter, full_output, disp, callback, retall)\u001b[39m\n\u001b[32m    242\u001b[39m     fit_funcs.update(extra_fit_funcs)\n\u001b[32m    244\u001b[39m func = fit_funcs[method]\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m xopt, retvals = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mdisp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mretall\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mhess\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhessian\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    250\u001b[39m optim_settings = {\u001b[33m'\u001b[39m\u001b[33moptimizer\u001b[39m\u001b[33m'\u001b[39m: method, \u001b[33m'\u001b[39m\u001b[33mstart_params\u001b[39m\u001b[33m'\u001b[39m: start_params,\n\u001b[32m    251\u001b[39m                   \u001b[33m'\u001b[39m\u001b[33mmaxiter\u001b[39m\u001b[33m'\u001b[39m: maxiter, \u001b[33m'\u001b[39m\u001b[33mfull_output\u001b[39m\u001b[33m'\u001b[39m: full_output,\n\u001b[32m    252\u001b[39m                   \u001b[33m'\u001b[39m\u001b[33mdisp\u001b[39m\u001b[33m'\u001b[39m: disp, \u001b[33m'\u001b[39m\u001b[33mfargs\u001b[39m\u001b[33m'\u001b[39m: fargs, \u001b[33m'\u001b[39m\u001b[33mcallback\u001b[39m\u001b[33m'\u001b[39m: callback,\n\u001b[32m    253\u001b[39m                   \u001b[33m'\u001b[39m\u001b[33mretall\u001b[39m\u001b[33m'\u001b[39m: retall, \u001b[33m\"\u001b[39m\u001b[33mextra_fit_funcs\u001b[39m\u001b[33m\"\u001b[39m: extra_fit_funcs}\n\u001b[32m    254\u001b[39m optim_settings.update(kwargs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/base/optimizer.py:454\u001b[39m, in \u001b[36m_fit_newton\u001b[39m\u001b[34m(f, score, start_params, fargs, kwargs, disp, maxiter, callback, retall, full_output, hess, ridge_factor)\u001b[39m\n\u001b[32m    452\u001b[39m         history.append(newparams)\n\u001b[32m    453\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m callback \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m454\u001b[39m         \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnewparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    455\u001b[39m     iterations += \u001b[32m1\u001b[39m\n\u001b[32m    456\u001b[39m fval = f(newparams, *fargs)  \u001b[38;5;66;03m# this is the negative likelihood\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/discrete/discrete_model.py:218\u001b[39m, in \u001b[36mDiscreteModel._check_perfect_pred\u001b[39m\u001b[34m(self, params, *args)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_check_perfect_pred\u001b[39m(\u001b[38;5;28mself\u001b[39m, params, *args):\n\u001b[32m    217\u001b[39m     endog = \u001b[38;5;28mself\u001b[39m.endog\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     fittedvalues = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m np.allclose(fittedvalues - endog, \u001b[32m0\u001b[39m):\n\u001b[32m    220\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raise_on_perfect_prediction:\n\u001b[32m    221\u001b[39m             \u001b[38;5;66;03m# backwards compatibility for attr raise_on_perfect_prediction\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/discrete/discrete_model.py:546\u001b[39m, in \u001b[36mBinaryModel.predict\u001b[39m\u001b[34m(self, params, exog, which, linear, offset)\u001b[39m\n\u001b[32m    543\u001b[39m linpred = np.dot(exog, params) + offset\n\u001b[32m    545\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m which == \u001b[33m\"\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m546\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlinpred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m which == \u001b[33m\"\u001b[39m\u001b[33mlinear\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m linpred\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/discrete/discrete_model.py:2385\u001b[39m, in \u001b[36mLogit.cdf\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m   2364\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2365\u001b[39m \u001b[33;03mThe logistic cumulative distribution function\u001b[39;00m\n\u001b[32m   2366\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2382\u001b[39m \u001b[33;03m          \\\\frac{e^{x^{\\\\prime}\\\\beta}}{1+e^{x^{\\\\prime}\\\\beta}}\u001b[39;00m\n\u001b[32m   2383\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2384\u001b[39m X = np.asarray(X)\n\u001b[32m-> \u001b[39m\u001b[32m2385\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m1\u001b[39m/(\u001b[32m1\u001b[39m+\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "#Stepwise Selection all variables\n",
        "\n",
        "def stepwise_logit(train_df, target_col=\"is_fraud\", entry_threshold=0.05):\n",
        "    # ✅ 確保索引對齊\n",
        "    train_df = train_df.reset_index(drop=True)\n",
        "    y = train_df[target_col].reset_index(drop=True)\n",
        "    \n",
        "    candidate_vars = list(train_df.columns.drop(target_col))\n",
        "    included_vars = []\n",
        "    step_results = []\n",
        "    \n",
        "    # Base model (only intercept)\n",
        "    X_base = sm.add_constant(pd.DataFrame({\"intercept\": [1]*len(train_df)}))\n",
        "    base_model = sm.Logit(y, X_base).fit(disp=False)\n",
        "    base_ll = -2 * base_model.llf\n",
        "    step_results.append({\"Step\": 0, \"Variable Entered\": None, \"-2 Log Likelihood\": base_ll})\n",
        "    \n",
        "    print(f\"Step 0: Base model estimated. -2LL = {base_ll:.3f}\")\n",
        "    \n",
        "    step = 1\n",
        "    while True:\n",
        "        best_pval = 1\n",
        "        best_var = None\n",
        "        best_model = None\n",
        "        \n",
        "        for var in candidate_vars:\n",
        "            try:\n",
        "                X_temp = sm.add_constant(train_df[included_vars + [var]])\n",
        "                model_temp = sm.Logit(y, X_temp).fit(disp=False)\n",
        "                pval = model_temp.pvalues[var]\n",
        "                \n",
        "                if pval < best_pval:\n",
        "                    best_pval = pval\n",
        "                    best_var = var\n",
        "                    best_model = model_temp\n",
        "                    \n",
        "            except Exception:\n",
        "                continue\n",
        "        \n",
        "        if best_var is None or best_pval > entry_threshold:\n",
        "            print(\"\\n✅ No more variables meet the entry threshold. Stepwise selection finished.\")\n",
        "            break\n",
        "        \n",
        "        included_vars.append(best_var)\n",
        "        candidate_vars.remove(best_var)\n",
        "        \n",
        "        ll = -2 * best_model.llf\n",
        "        step_results.append({\"Step\": step, \"Variable Entered\": best_var, \"-2 Log Likelihood\": ll})\n",
        "        \n",
        "        print(f\"Step {step}: Added {best_var}, p = {best_pval:.4f}, -2LL = {ll:.3f}\")\n",
        "        step += 1\n",
        "    \n",
        "    print(\"\\nFinal model summary:\")\n",
        "    print(best_model.summary())\n",
        "    \n",
        "    step_df = pd.DataFrame(step_results)\n",
        "    return step_df, best_model\n",
        "\n",
        "# 🚀 執行\n",
        "step_df, final_model = stepwise_logit(train_df, target_col=\"is_fraud\")\n",
        "step_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "import statsmodels.api as sm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "def stepwise_logit_with_k_tables(train_df, test_df, dep_var=\"is_fraud\", k=314657018,\n",
        "                                 threshold_in=0.05, threshold_out=0.10):\n",
        "    \"\"\"\n",
        "    Stepwise logistic regression (forward + backward) with flexible k control,\n",
        "    and 3 formatted output tables like table_for_first_step().\n",
        "    \"\"\"\n",
        "\n",
        "    y_train = train_df[dep_var]\n",
        "    X_train = train_df.drop(columns=[dep_var])\n",
        "    y_test = test_df[dep_var]\n",
        "    X_test = test_df.drop(columns=[dep_var])\n",
        "\n",
        "    included = []\n",
        "    step = 0\n",
        "    full_mode = (k == 314657018)\n",
        "    base_model = None\n",
        "\n",
        "    while True:\n",
        "        step += 1\n",
        "        changed = False\n",
        "\n",
        "        # ---------- Forward Step ----------\n",
        "        excluded = list(set(X_train.columns) - set(included))\n",
        "        new_pvals = pd.Series(index=excluded, dtype=float)\n",
        "        for new_var in excluded:\n",
        "            try:\n",
        "                model = sm.Logit(y_train, sm.add_constant(X_train[included + [new_var]])).fit(disp=False)\n",
        "                new_pvals[new_var] = model.pvalues[new_var]\n",
        "            except Exception:\n",
        "                new_pvals[new_var] = np.nan\n",
        "\n",
        "        if new_pvals.empty:\n",
        "            break\n",
        "\n",
        "        best_pval = new_pvals.min()\n",
        "        if best_pval < threshold_in:\n",
        "            best_var = new_pvals.idxmin()\n",
        "            included.append(best_var)\n",
        "            changed = True\n",
        "\n",
        "        # ---------- Backward Step ----------\n",
        "        if included:\n",
        "            model = sm.Logit(y_train, sm.add_constant(X_train[included])).fit(disp=False)\n",
        "            pvalues = model.pvalues.iloc[1:]\n",
        "            worst_pval = pvalues.max()\n",
        "            if worst_pval > threshold_out:\n",
        "                worst_var = pvalues.idxmax()\n",
        "                included.remove(worst_var)\n",
        "                changed = True\n",
        "\n",
        "        # ---------- 結束條件 ----------\n",
        "        if not changed:\n",
        "            break\n",
        "        if not full_mode and len(included) >= k:\n",
        "            break\n",
        "\n",
        "    # ========= Final Model =========\n",
        "    final_model = sm.Logit(y_train, sm.add_constant(X_train[included])).fit(disp=False)\n",
        "    ll_full = final_model.llf\n",
        "    ll_null = sm.Logit(y_train, sm.add_constant(np.ones(len(y_train)))).fit(disp=False).llf\n",
        "\n",
        "    # 1️⃣ Overall Model Fit\n",
        "    ll_diff = -2 * (ll_null - ll_full)\n",
        "    df_diff = len(final_model.params) - 1\n",
        "    p_value = stats.chi2.sf(ll_diff, df_diff)\n",
        "\n",
        "    overall_fit = pd.DataFrame({\n",
        "        \"Measure\": [\n",
        "            \"-2 Log Likelihood (−2LL) value\",\n",
        "            \"Cox and Snell R2\",\n",
        "            \"Nagelkerke R2\",\n",
        "            \"Pseudo R2 (McFadden)\",\n",
        "            \"Hosmer-Lemeshow χ2\"\n",
        "        ],\n",
        "        \"Value\": [\n",
        "            round(-2 * ll_full, 3),\n",
        "            round(1 - np.exp((2 / len(y_train)) * (ll_null - ll_full)), 3),\n",
        "            round((1 - np.exp((2 / len(y_train)) * (ll_null - ll_full))) / (1 - np.exp(2 * ll_null / len(y_train))), 3),\n",
        "            round(1 - (ll_full / ll_null), 3),\n",
        "            round(ll_diff, 3)\n",
        "        ],\n",
        "        \"Change_from_Base\": [\n",
        "            round(-2 * (ll_null - ll_full), 3),\n",
        "            \"\", \"\", \"\", \"\"\n",
        "        ],\n",
        "        \"Change_pvalue\": [\n",
        "            round(p_value, 4),\n",
        "            \"\", \"\", \"\", \"\"\n",
        "        ]\n",
        "    })\n",
        "\n",
        "    # 2️⃣ Variables in the Equation\n",
        "    coef_df = pd.DataFrame({\n",
        "        \"Independent Variable\": final_model.params.index,\n",
        "        \"B\": final_model.params.values,\n",
        "        \"Std. Error\": final_model.bse.values,\n",
        "        \"Wald\": (final_model.params / final_model.bse) ** 2,\n",
        "        \"df\": 1,\n",
        "        \"Sig.\": final_model.pvalues.values,\n",
        "        \"Exp(B)\": np.exp(final_model.params.values)\n",
        "    })\n",
        "    coef_df = coef_df.reset_index(drop=True)\n",
        "\n",
        "    # 3️⃣ Variables Not in the Equation\n",
        "    excluded_vars = [v for v in X_train.columns if v not in included]\n",
        "    not_in_eq = []\n",
        "    for var in excluded_vars:\n",
        "        try:\n",
        "            temp_model = sm.Logit(y_train, sm.add_constant(X_train[included + [var]])).fit(disp=False)\n",
        "            lr_stat = -2 * (final_model.llf - temp_model.llf)\n",
        "            p_val = stats.chi2.sf(lr_stat, 1)\n",
        "            not_in_eq.append({\"Independent Variable\": var,\n",
        "                              \"Score Statistic (LRT)\": round(lr_stat, 3),\n",
        "                              \"Significance\": round(p_val, 4)})\n",
        "        except Exception:\n",
        "            not_in_eq.append({\"Independent Variable\": var,\n",
        "                              \"Score Statistic (LRT)\": None,\n",
        "                              \"Significance\": None})\n",
        "\n",
        "    not_in_eq_df = pd.DataFrame(not_in_eq)\n",
        "\n",
        "    # 額外：Train / Test Accuracy\n",
        "    train_pred = (final_model.predict(sm.add_constant(X_train[included])) > 0.5).astype(int)\n",
        "    test_pred = (final_model.predict(sm.add_constant(X_test[included])) > 0.5).astype(int)\n",
        "    train_acc = (train_pred == y_train).mean()\n",
        "    test_acc = (test_pred == y_test).mean()\n",
        "\n",
        "    print(f\"\\n✅ Stepwise completed with {len(included)} variables: {included}\")\n",
        "    print(f\"Train Accuracy: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "    return overall_fit, coef_df, not_in_eq_df, final_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Stepwise completed with 1 variables: ['use_chip_Swipe Transaction']\n",
            "Train Accuracy: 0.9985, Test Accuracy: 0.9985\n",
            "=== Overall Model Fit ===\n",
            "                          Measure       Value Change_from_Base Change_pvalue\n",
            "0  -2 Log Likelihood (−2LL) value  150529.771         8580.841           0.0\n",
            "1                Cox and Snell R2       0.001                               \n",
            "2                   Nagelkerke R2       0.055                               \n",
            "3            Pseudo R2 (McFadden)       0.054                               \n",
            "4              Hosmer-Lemeshow χ2    8580.841                               \n",
            "\n",
            "=== Variables in the Equation ===\n",
            "         Independent Variable         B  Std. Error           Wald  df  Sig.  \\\n",
            "0                       const -5.874597    0.010263  327657.488906   1   0.0   \n",
            "1  use_chip_Swipe Transaction -2.282721    0.032252    5009.563547   1   0.0   \n",
            "\n",
            "     Exp(B)  \n",
            "0  0.002810  \n",
            "1  0.102006  \n",
            "\n",
            "=== Variables Not in the Equation ===\n",
            "         Independent Variable  Score Statistic (LRT)  Significance\n",
            "0              transaction_id               4323.514        0.0000\n",
            "1                 client_id_x                  3.485        0.0619\n",
            "2                     card_id                 16.687        0.0000\n",
            "3                      amount               2703.885        0.0000\n",
            "4                 merchant_id                364.660        0.0000\n",
            "5                         zip              18079.845        0.0000\n",
            "6                    mcc_code               1107.672        0.0000\n",
            "7         errors_missing_flag                318.314        0.0000\n",
            "8   use_chip_Chip Transaction              10040.388        0.0000\n",
            "9                 current_age                151.630        0.0000\n",
            "10             retirement_age                  0.346        0.5565\n",
            "11                   latitude                 14.845        0.0001\n",
            "12                  longitude                 15.157        0.0001\n",
            "13              yearly_income                108.238        0.0000\n",
            "14                 total_debt                 66.711        0.0000\n",
            "15               credit_score                 18.433        0.0000\n",
            "16           num_credit_cards                365.597        0.0000\n",
            "17                gender_Male                  0.261        0.6096\n",
            "18                        cvv                  2.592        0.1074\n",
            "19           num_cards_issued                  0.259        0.6106\n",
            "20               credit_limit                258.321        0.0000\n",
            "21           card_type_Credit                 39.927        0.0000\n",
            "22            card_type_Debit                161.077        0.0000\n",
            "23            card_brand_Amex                  3.356        0.0670\n",
            "24      card_brand_Mastercard                  0.658        0.4172\n",
            "25            card_brand_Visa                  7.079        0.0078\n",
            "26               has_chip_YES                636.290        0.0000\n",
            "\n",
            "================================================\n",
            "\n",
            "=== Accuracy in Training and Testing dataset ===\n",
            "\n",
            "=== Classification Matrix — Training Sample ===\n",
            "Actual Group  Predicted Fraud (1)  Predicted Normal (0)   Total  % Correct\n",
            "   Fraud (1)                    0                 10591   10591        0.0\n",
            "  Normal (0)                    0               7121379 7121379      100.0\n",
            "       Total                    0               7131970 7131970       99.9\n",
            "\n",
            "=== Classification Matrix — Holdout (Test) Sample ===\n",
            "Actual Group  Predicted Fraud (1)  Predicted Normal (0)   Total  % Correct\n",
            "   Fraud (1)                    0                  2741    2741        0.0\n",
            "  Normal (0)                    0               1780252 1780252      100.0\n",
            "       Total                    0               1782993 1782993       99.8\n"
          ]
        }
      ],
      "source": [
        "overall_fit, coef_df, not_in_eq_df, model_0 = stepwise_logit_with_k_tables(\n",
        "    train_df, test_df, dep_var=\"is_fraud\", k=0\n",
        ")\n",
        "\n",
        "\n",
        "print(\"=== Overall Model Fit ===\")\n",
        "print(overall_fit)\n",
        "\n",
        "print(\"\\n=== Variables in the Equation ===\")\n",
        "print(coef_df)\n",
        "\n",
        "print(\"\\n=== Variables Not in the Equation ===\")\n",
        "print(not_in_eq_df)\n",
        "\n",
        "print(\"\\n================================================\")\n",
        "print(\"\\n=== Accuracy in Training and Testing dataset ===\")\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def classification_table(model, df, target_col=\"is_fraud\", cutoff=0.5):\n",
        "    y_true = df[target_col].astype(int)\n",
        "    X = sm.add_constant(df[model.params.index.drop(\"const\")])\n",
        "    y_pred_prob = model.predict(X)\n",
        "    y_pred = (y_pred_prob >= cutoff).astype(int)\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=[1, 0])\n",
        "    TP, FN, FP, TN = cm.ravel()\n",
        "\n",
        "    fraud_total = TP + FN\n",
        "    normal_total = FP + TN\n",
        "\n",
        "    fraud_correct = TP / fraud_total if fraud_total > 0 else 0\n",
        "    normal_correct = TN / normal_total if normal_total > 0 else 0\n",
        "    overall_correct = (TP + TN) / (fraud_total + normal_total)\n",
        "\n",
        "    table = pd.DataFrame({\n",
        "        \"Actual Group\": [\"Fraud (1)\", \"Normal (0)\", \"Total\"],\n",
        "        \"Predicted Fraud (1)\": [TP, FP, TP + FP],\n",
        "        \"Predicted Normal (0)\": [FN, TN, FN + TN],\n",
        "        \"Total\": [fraud_total, normal_total, fraud_total + normal_total],\n",
        "        \"% Correct\": [\n",
        "            round(fraud_correct * 100, 1),\n",
        "            round(normal_correct * 100, 1),\n",
        "            round(overall_correct * 100, 1),\n",
        "        ],\n",
        "    })\n",
        "    return table\n",
        "\n",
        "# Step 3. 產出訓練集和測試集的結果表\n",
        "train_table = classification_table(model_0, train_df, target_col=\"is_fraud\")\n",
        "test_table = classification_table(model_0, test_df, target_col=\"is_fraud\")\n",
        "\n",
        "print(\"\\n=== Classification Matrix — Training Sample ===\")\n",
        "print(train_table.to_string(index=False))\n",
        "print(\"\\n=== Classification Matrix — Holdout (Test) Sample ===\")\n",
        "print(test_table.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Stepwise completed with 2 variables: ['use_chip_Swipe Transaction', 'transaction_id']\n",
            "Train Accuracy: 0.9985, Test Accuracy: 0.9985\n",
            "=== Overall Model Fit ===\n",
            "                          Measure       Value Change_from_Base Change_pvalue\n",
            "0  -2 Log Likelihood (−2LL) value  146206.258        12904.354           0.0\n",
            "1                Cox and Snell R2       0.002                               \n",
            "2                   Nagelkerke R2       0.082                               \n",
            "3            Pseudo R2 (McFadden)       0.081                               \n",
            "4              Hosmer-Lemeshow χ2   12904.354                               \n",
            "\n",
            "=== Variables in the Equation ===\n",
            "         Independent Variable             B    Std. Error         Wald  df  \\\n",
            "0                       const -3.096791e+00  3.850850e-02  6467.116055   1   \n",
            "1  use_chip_Swipe Transaction -3.184227e+00  3.433942e-02  8598.491221   1   \n",
            "2              transaction_id -1.586421e-07  2.294354e-09  4780.970705   1   \n",
            "\n",
            "   Sig.    Exp(B)  \n",
            "0   0.0  0.045194  \n",
            "1   0.0  0.041410  \n",
            "2   0.0  1.000000  \n",
            "\n",
            "=== Variables Not in the Equation ===\n",
            "         Independent Variable  Score Statistic (LRT)  Significance\n",
            "0                 client_id_x                 12.080        0.0005\n",
            "1                     card_id                 15.087        0.0001\n",
            "2                      amount               2218.202        0.0000\n",
            "3                 merchant_id                548.977        0.0000\n",
            "4                         zip              14052.233        0.0000\n",
            "5                    mcc_code                394.861        0.0000\n",
            "6         errors_missing_flag                279.915        0.0000\n",
            "7   use_chip_Chip Transaction               6059.646        0.0000\n",
            "8                 current_age                118.375        0.0000\n",
            "9              retirement_age                  0.259        0.6109\n",
            "10                   latitude                  3.187        0.0742\n",
            "11                  longitude                 22.410        0.0000\n",
            "12              yearly_income                133.086        0.0000\n",
            "13                 total_debt                 68.882        0.0000\n",
            "14               credit_score                 29.208        0.0000\n",
            "15           num_credit_cards                354.191        0.0000\n",
            "16                gender_Male                  0.047        0.8285\n",
            "17                        cvv                  3.584        0.0583\n",
            "18           num_cards_issued                  0.001        0.9796\n",
            "19               credit_limit                278.641        0.0000\n",
            "20           card_type_Credit                 34.620        0.0000\n",
            "21            card_type_Debit                144.276        0.0000\n",
            "22            card_brand_Amex                  0.625        0.4292\n",
            "23      card_brand_Mastercard                  0.517        0.4722\n",
            "24            card_brand_Visa                  4.977        0.0257\n",
            "25               has_chip_YES                403.149        0.0000\n",
            "\n",
            "================================================\n",
            "\n",
            "=== Accuracy in Training and Testing dataset ===\n",
            "\n",
            "=== Classification Matrix — Training Sample ===\n",
            "Actual Group  Predicted Fraud (1)  Predicted Normal (0)   Total  % Correct\n",
            "   Fraud (1)                    0                 10591   10591        0.0\n",
            "  Normal (0)                    0               7121379 7121379      100.0\n",
            "       Total                    0               7131970 7131970       99.9\n",
            "\n",
            "=== Classification Matrix — Holdout (Test) Sample ===\n",
            "Actual Group  Predicted Fraud (1)  Predicted Normal (0)   Total  % Correct\n",
            "   Fraud (1)                    0                  2741    2741        0.0\n",
            "  Normal (0)                    0               1780252 1780252      100.0\n",
            "       Total                    0               1782993 1782993       99.8\n"
          ]
        }
      ],
      "source": [
        "overall_fit, coef_df, not_in_eq_df, model_2 = stepwise_logit_with_k_tables(\n",
        "    train_df, test_df, dep_var=\"is_fraud\", k=2\n",
        ")\n",
        "\n",
        "\n",
        "print(\"=== Overall Model Fit ===\")\n",
        "print(overall_fit)\n",
        "\n",
        "print(\"\\n=== Variables in the Equation ===\")\n",
        "print(coef_df)\n",
        "\n",
        "print(\"\\n=== Variables Not in the Equation ===\")\n",
        "print(not_in_eq_df)\n",
        "\n",
        "print(\"\\n================================================\")\n",
        "print(\"\\n=== Accuracy in Training and Testing dataset ===\")\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def classification_table(model, df, target_col=\"is_fraud\", cutoff=0.5):\n",
        "    y_true = df[target_col].astype(int)\n",
        "    X = sm.add_constant(df[model.params.index.drop(\"const\")])\n",
        "    y_pred_prob = model.predict(X)\n",
        "    y_pred = (y_pred_prob >= cutoff).astype(int)\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=[1, 0])\n",
        "    TP, FN, FP, TN = cm.ravel()\n",
        "\n",
        "    fraud_total = TP + FN\n",
        "    normal_total = FP + TN\n",
        "\n",
        "    fraud_correct = TP / fraud_total if fraud_total > 0 else 0\n",
        "    normal_correct = TN / normal_total if normal_total > 0 else 0\n",
        "    overall_correct = (TP + TN) / (fraud_total + normal_total)\n",
        "\n",
        "    table = pd.DataFrame({\n",
        "        \"Actual Group\": [\"Fraud (1)\", \"Normal (0)\", \"Total\"],\n",
        "        \"Predicted Fraud (1)\": [TP, FP, TP + FP],\n",
        "        \"Predicted Normal (0)\": [FN, TN, FN + TN],\n",
        "        \"Total\": [fraud_total, normal_total, fraud_total + normal_total],\n",
        "        \"% Correct\": [\n",
        "            round(fraud_correct * 100, 1),\n",
        "            round(normal_correct * 100, 1),\n",
        "            round(overall_correct * 100, 1),\n",
        "        ],\n",
        "    })\n",
        "    return table\n",
        "\n",
        "# Step 3. 產出訓練集和測試集的結果表\n",
        "train_table = classification_table(model_2, train_df, target_col=\"is_fraud\")\n",
        "test_table = classification_table(model_2, test_df, target_col=\"is_fraud\")\n",
        "\n",
        "print(\"\\n=== Classification Matrix — Training Sample ===\")\n",
        "print(train_table.to_string(index=False))\n",
        "print(\"\\n=== Classification Matrix — Holdout (Test) Sample ===\")\n",
        "print(test_table.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Stepwise completed with 3 variables: ['use_chip_Swipe Transaction', 'transaction_id', 'amount']\n",
            "Train Accuracy: 0.9985, Test Accuracy: 0.9985\n",
            "=== Overall Model Fit ===\n",
            "                          Measure       Value Change_from_Base Change_pvalue\n",
            "0  -2 Log Likelihood (−2LL) value  143988.055        15122.557           0.0\n",
            "1                Cox and Snell R2       0.002                               \n",
            "2                   Nagelkerke R2       0.096                               \n",
            "3            Pseudo R2 (McFadden)       0.095                               \n",
            "4              Hosmer-Lemeshow χ2   15122.557                               \n",
            "\n",
            "=== Variables in the Equation ===\n",
            "         Independent Variable             B    Std. Error         Wald  df  \\\n",
            "0                       const -4.622802e+00  5.174460e-02  7981.423846   1   \n",
            "1  use_chip_Swipe Transaction -3.073492e+00  3.428068e-02  8038.319156   1   \n",
            "2              transaction_id -1.479389e-07  2.284622e-09  4193.109368   1   \n",
            "3                      amount  3.805656e-01  8.491934e-03  2008.379956   1   \n",
            "\n",
            "   Sig.    Exp(B)  \n",
            "0   0.0  0.009825  \n",
            "1   0.0  0.046259  \n",
            "2   0.0  1.000000  \n",
            "3   0.0  1.463112  \n",
            "\n",
            "=== Variables Not in the Equation ===\n",
            "         Independent Variable  Score Statistic (LRT)  Significance\n",
            "0                 client_id_x                 10.455        0.0012\n",
            "1                     card_id                  9.308        0.0023\n",
            "2                 merchant_id                466.271        0.0000\n",
            "3                         zip              13029.175        0.0000\n",
            "4                    mcc_code                332.527        0.0000\n",
            "5         errors_missing_flag                210.458        0.0000\n",
            "6   use_chip_Chip Transaction               5259.780        0.0000\n",
            "7                 current_age                136.353        0.0000\n",
            "8              retirement_age                  0.571        0.4500\n",
            "9                    latitude                  4.787        0.0287\n",
            "10                  longitude                 30.386        0.0000\n",
            "11              yearly_income                308.137        0.0000\n",
            "12                 total_debt                115.269        0.0000\n",
            "13               credit_score                 36.609        0.0000\n",
            "14           num_credit_cards                386.879        0.0000\n",
            "15                gender_Male                  0.408        0.5229\n",
            "16                        cvv                  1.938        0.1639\n",
            "17           num_cards_issued                  0.403        0.5254\n",
            "18               credit_limit                426.715        0.0000\n",
            "19           card_type_Credit                  0.238        0.6255\n",
            "20            card_type_Debit                 73.339        0.0000\n",
            "21            card_brand_Amex                  2.531        0.1116\n",
            "22      card_brand_Mastercard                  2.881        0.0896\n",
            "23            card_brand_Visa                  8.879        0.0029\n",
            "24               has_chip_YES                342.918        0.0000\n",
            "\n",
            "================================================\n",
            "\n",
            "=== Accuracy in Training and Testing dataset ===\n",
            "\n",
            "=== Classification Matrix — Training Sample ===\n",
            "Actual Group  Predicted Fraud (1)  Predicted Normal (0)   Total  % Correct\n",
            "   Fraud (1)                    0                 10591   10591        0.0\n",
            "  Normal (0)                    0               7121379 7121379      100.0\n",
            "       Total                    0               7131970 7131970       99.9\n",
            "\n",
            "=== Classification Matrix — Holdout (Test) Sample ===\n",
            "Actual Group  Predicted Fraud (1)  Predicted Normal (0)   Total  % Correct\n",
            "   Fraud (1)                    0                  2741    2741        0.0\n",
            "  Normal (0)                    0               1780252 1780252      100.0\n",
            "       Total                    0               1782993 1782993       99.8\n"
          ]
        }
      ],
      "source": [
        "overall_fit, coef_df, not_in_eq_df, model_3 = stepwise_logit_with_k_tables(\n",
        "    train_df, test_df, dep_var=\"is_fraud\", k=3\n",
        ")\n",
        "\n",
        "\n",
        "print(\"=== Overall Model Fit ===\")\n",
        "print(overall_fit)\n",
        "\n",
        "print(\"\\n=== Variables in the Equation ===\")\n",
        "print(coef_df)\n",
        "\n",
        "print(\"\\n=== Variables Not in the Equation ===\")\n",
        "print(not_in_eq_df)\n",
        "\n",
        "print(\"\\n================================================\")\n",
        "print(\"\\n=== Accuracy in Training and Testing dataset ===\")\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def classification_table(model, df, target_col=\"is_fraud\", cutoff=0.5):\n",
        "    y_true = df[target_col].astype(int)\n",
        "    X = sm.add_constant(df[model.params.index.drop(\"const\")])\n",
        "    y_pred_prob = model.predict(X)\n",
        "    y_pred = (y_pred_prob >= cutoff).astype(int)\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=[1, 0])\n",
        "    TP, FN, FP, TN = cm.ravel()\n",
        "\n",
        "    fraud_total = TP + FN\n",
        "    normal_total = FP + TN\n",
        "\n",
        "    fraud_correct = TP / fraud_total if fraud_total > 0 else 0\n",
        "    normal_correct = TN / normal_total if normal_total > 0 else 0\n",
        "    overall_correct = (TP + TN) / (fraud_total + normal_total)\n",
        "\n",
        "    table = pd.DataFrame({\n",
        "        \"Actual Group\": [\"Fraud (1)\", \"Normal (0)\", \"Total\"],\n",
        "        \"Predicted Fraud (1)\": [TP, FP, TP + FP],\n",
        "        \"Predicted Normal (0)\": [FN, TN, FN + TN],\n",
        "        \"Total\": [fraud_total, normal_total, fraud_total + normal_total],\n",
        "        \"% Correct\": [\n",
        "            round(fraud_correct * 100, 1),\n",
        "            round(normal_correct * 100, 1),\n",
        "            round(overall_correct * 100, 1),\n",
        "        ],\n",
        "    })\n",
        "    return table\n",
        "\n",
        "# Step 3. 產出訓練集和測試集的結果表\n",
        "train_table = classification_table(model_3, train_df, target_col=\"is_fraud\")\n",
        "test_table = classification_table(model_3, test_df, target_col=\"is_fraud\")\n",
        "\n",
        "print(\"\\n=== Classification Matrix — Training Sample ===\")\n",
        "print(train_table.to_string(index=False))\n",
        "print(\"\\n=== Classification Matrix — Holdout (Test) Sample ===\")\n",
        "print(test_table.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Stepwise completed with 4 variables: ['use_chip_Swipe Transaction', 'transaction_id', 'amount', 'zip']\n",
            "Train Accuracy: 0.9985, Test Accuracy: 0.9985\n",
            "=== Overall Model Fit ===\n",
            "                          Measure       Value Change_from_Base Change_pvalue\n",
            "0  -2 Log Likelihood (−2LL) value  130958.880        28151.732           0.0\n",
            "1                Cox and Snell R2       0.004                               \n",
            "2                   Nagelkerke R2       0.179                               \n",
            "3            Pseudo R2 (McFadden)       0.177                               \n",
            "4              Hosmer-Lemeshow χ2   28151.732                               \n",
            "\n",
            "=== Variables in the Equation ===\n",
            "         Independent Variable             B    Std. Error          Wald  df  \\\n",
            "0                       const -5.318296e+00  5.093754e-02  10901.067736   1   \n",
            "1  use_chip_Swipe Transaction -1.325628e+00  3.619574e-02   1341.306837   1   \n",
            "2              transaction_id -3.626462e-08  2.223489e-09    266.008944   1   \n",
            "3                      amount  3.057868e-01  9.064978e-03   1137.899851   1   \n",
            "4                         zip -7.568753e-05  1.103757e-06   4702.204411   1   \n",
            "\n",
            "            Sig.    Exp(B)  \n",
            "0   0.000000e+00  0.004901  \n",
            "1  1.193366e-293  0.265636  \n",
            "2   8.409421e-60  1.000000  \n",
            "3  1.912908e-249  1.357693  \n",
            "4   0.000000e+00  0.999924  \n",
            "\n",
            "=== Variables Not in the Equation ===\n",
            "         Independent Variable  Score Statistic (LRT)  Significance\n",
            "0                 client_id_x                 50.832        0.0000\n",
            "1                     card_id                  8.562        0.0034\n",
            "2                 merchant_id                758.615        0.0000\n",
            "3                    mcc_code                 12.452        0.0004\n",
            "4         errors_missing_flag                185.550        0.0000\n",
            "5   use_chip_Chip Transaction                 87.763        0.0000\n",
            "6                 current_age                157.725        0.0000\n",
            "7              retirement_age                 10.933        0.0009\n",
            "8                    latitude                182.366        0.0000\n",
            "9                   longitude               1190.288        0.0000\n",
            "10              yearly_income                809.918        0.0000\n",
            "11                 total_debt                244.420        0.0000\n",
            "12               credit_score                 91.583        0.0000\n",
            "13           num_credit_cards                472.237        0.0000\n",
            "14                gender_Male                  1.472        0.2250\n",
            "15                        cvv                  7.500        0.0062\n",
            "16           num_cards_issued                  0.799        0.3713\n",
            "17               credit_limit                769.190        0.0000\n",
            "18           card_type_Credit                  4.808        0.0283\n",
            "19            card_type_Debit                106.497        0.0000\n",
            "20            card_brand_Amex                  2.762        0.0965\n",
            "21      card_brand_Mastercard                  1.373        0.2413\n",
            "22            card_brand_Visa                  7.080        0.0078\n",
            "23               has_chip_YES                 27.786        0.0000\n",
            "\n",
            "================================================\n",
            "\n",
            "=== Accuracy in Training and Testing dataset ===\n",
            "\n",
            "=== Classification Matrix — Training Sample ===\n",
            "Actual Group  Predicted Fraud (1)  Predicted Normal (0)   Total  % Correct\n",
            "   Fraud (1)                    0                 10591   10591        0.0\n",
            "  Normal (0)                    0               7121379 7121379      100.0\n",
            "       Total                    0               7131970 7131970       99.9\n",
            "\n",
            "=== Classification Matrix — Holdout (Test) Sample ===\n",
            "Actual Group  Predicted Fraud (1)  Predicted Normal (0)   Total  % Correct\n",
            "   Fraud (1)                    0                  2741    2741        0.0\n",
            "  Normal (0)                    0               1780252 1780252      100.0\n",
            "       Total                    0               1782993 1782993       99.8\n"
          ]
        }
      ],
      "source": [
        "overall_fit, coef_df, not_in_eq_df, model_4 = stepwise_logit_with_k_tables(\n",
        "    train_df, test_df, dep_var=\"is_fraud\", k=4\n",
        ")\n",
        "\n",
        "\n",
        "print(\"=== Overall Model Fit ===\")\n",
        "print(overall_fit)\n",
        "\n",
        "print(\"\\n=== Variables in the Equation ===\")\n",
        "print(coef_df)\n",
        "\n",
        "print(\"\\n=== Variables Not in the Equation ===\")\n",
        "print(not_in_eq_df)\n",
        "\n",
        "print(\"\\n================================================\")\n",
        "print(\"\\n=== Accuracy in Training and Testing dataset ===\")\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def classification_table(model, df, target_col=\"is_fraud\", cutoff=0.5):\n",
        "    y_true = df[target_col].astype(int)\n",
        "    X = sm.add_constant(df[model.params.index.drop(\"const\")])\n",
        "    y_pred_prob = model.predict(X)\n",
        "    y_pred = (y_pred_prob >= cutoff).astype(int)\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=[1, 0])\n",
        "    TP, FN, FP, TN = cm.ravel()\n",
        "\n",
        "    fraud_total = TP + FN\n",
        "    normal_total = FP + TN\n",
        "\n",
        "    fraud_correct = TP / fraud_total if fraud_total > 0 else 0\n",
        "    normal_correct = TN / normal_total if normal_total > 0 else 0\n",
        "    overall_correct = (TP + TN) / (fraud_total + normal_total)\n",
        "\n",
        "    table = pd.DataFrame({\n",
        "        \"Actual Group\": [\"Fraud (1)\", \"Normal (0)\", \"Total\"],\n",
        "        \"Predicted Fraud (1)\": [TP, FP, TP + FP],\n",
        "        \"Predicted Normal (0)\": [FN, TN, FN + TN],\n",
        "        \"Total\": [fraud_total, normal_total, fraud_total + normal_total],\n",
        "        \"% Correct\": [\n",
        "            round(fraud_correct * 100, 1),\n",
        "            round(normal_correct * 100, 1),\n",
        "            round(overall_correct * 100, 1),\n",
        "        ],\n",
        "    })\n",
        "    return table\n",
        "\n",
        "# Step 3. 產出訓練集和測試集的結果表\n",
        "train_table = classification_table(model_4, train_df, target_col=\"is_fraud\")\n",
        "test_table = classification_table(model_4, test_df, target_col=\"is_fraud\")\n",
        "\n",
        "print(\"\\n=== Classification Matrix — Training Sample ===\")\n",
        "print(train_table.to_string(index=False))\n",
        "print(\"\\n=== Classification Matrix — Holdout (Test) Sample ===\")\n",
        "print(test_table.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Stepwise completed with 22 variables: ['use_chip_Swipe Transaction', 'transaction_id', 'amount', 'zip', 'longitude', 'merchant_id', 'credit_limit', 'use_chip_Chip Transaction', 'num_credit_cards', 'errors_missing_flag', 'yearly_income', 'latitude', 'client_id_x', 'credit_score', 'has_chip_YES', 'mcc_code', 'card_type_Credit', 'card_type_Debit', 'retirement_age', 'card_brand_Visa', 'card_brand_Mastercard', 'card_brand_Amex']\n",
            "Train Accuracy: 0.9985, Test Accuracy: 0.9985\n",
            "=== Overall Model Fit ===\n",
            "                          Measure       Value Change_from_Base Change_pvalue\n",
            "0  -2 Log Likelihood (−2LL) value  126757.285        32353.327           0.0\n",
            "1                Cox and Snell R2       0.005                               \n",
            "2                   Nagelkerke R2       0.205                               \n",
            "3            Pseudo R2 (McFadden)       0.203                               \n",
            "4              Hosmer-Lemeshow χ2   32353.327                               \n",
            "\n",
            "=== Variables in the Equation ===\n",
            "          Independent Variable             B    Std. Error         Wald  df  \\\n",
            "0                        const -6.256912e+00  2.563705e-01   595.640161   1   \n",
            "1   use_chip_Swipe Transaction -6.317402e-01  4.051049e-02   243.187882   1   \n",
            "2               transaction_id -4.327101e-08  2.393265e-09   326.898286   1   \n",
            "3                       amount  3.268500e-01  9.245842e-03  1249.695048   1   \n",
            "4                          zip -9.311880e-05  1.369609e-06  4622.545936   1   \n",
            "5                    longitude -2.120861e-02  5.826634e-04  1324.917622   1   \n",
            "6                  merchant_id  1.008279e-05  3.732798e-07   729.612294   1   \n",
            "7                 credit_limit -1.469095e-05  1.313910e-06   125.016928   1   \n",
            "8    use_chip_Chip Transaction  8.980619e-01  3.409210e-02   693.913023   1   \n",
            "9             num_credit_cards  1.003507e-01  6.301139e-03   253.631224   1   \n",
            "10         errors_missing_flag -7.153805e-01  4.838233e-02   218.625447   1   \n",
            "11               yearly_income -7.072453e-06  5.859894e-07   145.666783   1   \n",
            "12                    latitude -2.051816e-02  1.966752e-03   108.837312   1   \n",
            "13                 client_id_x -1.197799e-04  1.696715e-05    49.836779   1   \n",
            "14                credit_score  7.208392e-04  1.579810e-04    20.819350   1   \n",
            "15                has_chip_YES -1.744584e-01  3.519409e-02    24.572222   1   \n",
            "16                    mcc_code -5.731581e-05  1.162935e-05    24.290588   1   \n",
            "17            card_type_Credit -3.798015e-01  4.105220e-02    85.593422   1   \n",
            "18             card_type_Debit -2.995360e-01  4.093374e-02    53.547023   1   \n",
            "19              retirement_age  1.062476e-02  2.911931e-03    13.313001   1   \n",
            "20             card_brand_Visa -3.433385e-01  5.606483e-02    37.502829   1   \n",
            "21       card_brand_Mastercard -3.057221e-01  5.672304e-02    29.049224   1   \n",
            "22             card_brand_Amex -3.185774e-01  6.353747e-02    25.140284   1   \n",
            "\n",
            "             Sig.    Exp(B)  \n",
            "0   1.486293e-131  0.001917  \n",
            "1    7.936401e-55  0.531666  \n",
            "2    4.553595e-73  1.000000  \n",
            "3   9.668510e-274  1.386593  \n",
            "4    0.000000e+00  0.999907  \n",
            "5   4.348229e-290  0.979015  \n",
            "6   1.087688e-160  1.000010  \n",
            "7    5.046238e-29  0.999985  \n",
            "8   6.300361e-153  2.454841  \n",
            "9    4.195873e-57  1.105559  \n",
            "10   1.803902e-49  0.489006  \n",
            "11   1.535281e-33  0.999993  \n",
            "12   1.761678e-25  0.979691  \n",
            "13   1.670825e-12  0.999880  \n",
            "14   5.047045e-06  1.000721  \n",
            "15   7.157613e-07  0.839912  \n",
            "16   8.284287e-07  0.999943  \n",
            "17   2.210094e-20  0.683997  \n",
            "18   2.524767e-13  0.741162  \n",
            "19   2.635722e-04  1.010681  \n",
            "20   9.128048e-10  0.709398  \n",
            "21   7.056236e-08  0.736591  \n",
            "22   5.330757e-07  0.727183  \n",
            "\n",
            "=== Variables Not in the Equation ===\n",
            "  Independent Variable  Score Statistic (LRT)  Significance\n",
            "0              card_id                  2.640        0.1042\n",
            "1          current_age                  0.041        0.8393\n",
            "2           total_debt                  2.182        0.1396\n",
            "3          gender_Male                  0.127        0.7217\n",
            "4                  cvv                  0.911        0.3397\n",
            "5     num_cards_issued                  0.300        0.5839\n",
            "\n",
            "================================================\n",
            "\n",
            "=== Accuracy in Training and Testing dataset ===\n",
            "\n",
            "=== Classification Matrix — Training Sample ===\n",
            "Actual Group  Predicted Fraud (1)  Predicted Normal (0)   Total  % Correct\n",
            "   Fraud (1)                    0                 10591   10591        0.0\n",
            "  Normal (0)                    0               7121379 7121379      100.0\n",
            "       Total                    0               7131970 7131970       99.9\n",
            "\n",
            "=== Classification Matrix — Holdout (Test) Sample ===\n",
            "Actual Group  Predicted Fraud (1)  Predicted Normal (0)   Total  % Correct\n",
            "   Fraud (1)                    0                  2741    2741        0.0\n",
            "  Normal (0)                    0               1780252 1780252      100.0\n",
            "       Total                    0               1782993 1782993       99.8\n"
          ]
        }
      ],
      "source": [
        "overall_fit, coef_df, not_in_eq_df, final_model = stepwise_logit_with_k_tables(\n",
        "    train_df, test_df, dep_var=\"is_fraud\", k=314657018\n",
        ")\n",
        "\n",
        "\n",
        "print(\"=== Overall Model Fit ===\")\n",
        "print(overall_fit)\n",
        "\n",
        "print(\"\\n=== Variables in the Equation ===\")\n",
        "print(coef_df)\n",
        "\n",
        "print(\"\\n=== Variables Not in the Equation ===\")\n",
        "print(not_in_eq_df)\n",
        "\n",
        "print(\"\\n================================================\")\n",
        "print(\"\\n=== Accuracy in Training and Testing dataset ===\")\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def classification_table(model, df, target_col=\"is_fraud\", cutoff=0.5):\n",
        "    y_true = df[target_col].astype(int)\n",
        "    X = sm.add_constant(df[model.params.index.drop(\"const\")])\n",
        "    y_pred_prob = model.predict(X)\n",
        "    y_pred = (y_pred_prob >= cutoff).astype(int)\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=[1, 0])\n",
        "    TP, FN, FP, TN = cm.ravel()\n",
        "\n",
        "    fraud_total = TP + FN\n",
        "    normal_total = FP + TN\n",
        "\n",
        "    fraud_correct = TP / fraud_total if fraud_total > 0 else 0\n",
        "    normal_correct = TN / normal_total if normal_total > 0 else 0\n",
        "    overall_correct = (TP + TN) / (fraud_total + normal_total)\n",
        "\n",
        "    table = pd.DataFrame({\n",
        "        \"Actual Group\": [\"Fraud (1)\", \"Normal (0)\", \"Total\"],\n",
        "        \"Predicted Fraud (1)\": [TP, FP, TP + FP],\n",
        "        \"Predicted Normal (0)\": [FN, TN, FN + TN],\n",
        "        \"Total\": [fraud_total, normal_total, fraud_total + normal_total],\n",
        "        \"% Correct\": [\n",
        "            round(fraud_correct * 100, 1),\n",
        "            round(normal_correct * 100, 1),\n",
        "            round(overall_correct * 100, 1),\n",
        "        ],\n",
        "    })\n",
        "    return table\n",
        "\n",
        "# Step 3. 產出訓練集和測試集的結果表\n",
        "train_table = classification_table(final_model, train_df, target_col=\"is_fraud\")\n",
        "test_table = classification_table(final_model, test_df, target_col=\"is_fraud\")\n",
        "\n",
        "print(\"\\n=== Classification Matrix — Training Sample ===\")\n",
        "print(train_table.to_string(index=False))\n",
        "print(\"\\n=== Classification Matrix — Holdout (Test) Sample ===\")\n",
        "print(test_table.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Cutoff      TN      FP   FN   TP  Accuracy  Sensitivity  Specificity  Youden   PPV   NPV\n",
            "   0.00       0 1780252    0 2741       0.2        100.0          0.0     0.0   0.2    NC\n",
            "   0.10 1780178      74 2734    7      99.8          0.3        100.0     0.3   8.6  99.8\n",
            "   0.20 1780248       4 2740    1      99.8          0.0        100.0     0.0  20.0  99.8\n",
            "   0.30 1780252       0 2741    0      99.8          0.0        100.0     0.0    NC  99.8\n",
            "   0.40 1780252       0 2741    0      99.8          0.0        100.0     0.0    NC  99.8\n",
            "   0.42 1780252       0 2741    0      99.8          0.0        100.0     0.0    NC  99.8\n",
            "   0.44 1780252       0 2741    0      99.8          0.0        100.0     0.0    NC  99.8\n",
            "   0.46 1780252       0 2741    0      99.8          0.0        100.0     0.0    NC  99.8\n",
            "   0.48 1780252       0 2741    0      99.8          0.0        100.0     0.0    NC  99.8\n",
            "   0.50 1780252       0 2741    0      99.8          0.0        100.0     0.0    NC  99.8\n",
            "   0.52 1780252       0 2741    0      99.8          0.0        100.0     0.0    NC  99.8\n",
            "   0.54 1780252       0 2741    0      99.8          0.0        100.0     0.0    NC  99.8\n",
            "   0.56 1780252       0 2741    0      99.8          0.0        100.0     0.0    NC  99.8\n",
            "   0.58 1780252       0 2741    0      99.8          0.0        100.0     0.0    NC  99.8\n",
            "   0.60 1780252       0 2741    0      99.8          0.0        100.0     0.0    NC  99.8\n",
            "   0.70 1780252       0 2741    0      99.8          0.0        100.0     0.0    NC  99.8\n",
            "   0.80 1780252       0 2741    0      99.8          0.0        100.0     0.0    NC  99.8\n",
            "   0.90 1780252       0 2741    0      99.8          0.0        100.0     0.0    NC  99.8\n",
            "   1.00 1780252       0 2741    0      99.8          0.0        100.0     0.0    NC  99.8\n"
          ]
        }
      ],
      "source": [
        "## 一些模型檢驗診斷\n",
        "def cutoff_analysis(model, df, target_col=\"is_fraud\", cutoffs=None):\n",
        "    \"\"\"\n",
        "    產出類似 Table 8.7 的結果\n",
        "    \"\"\"\n",
        "    if cutoffs is None:\n",
        "        cutoffs = np.arange(0, 1.01, 0.02)  # 預設 0, 0.02, ..., 1\n",
        "    \n",
        "    y_true = df[target_col].astype(int)\n",
        "    X = sm.add_constant(df[model.params.index.drop(\"const\")])\n",
        "    y_prob = model.predict(X)\n",
        "\n",
        "    rows = []\n",
        "    for cutoff in cutoffs:\n",
        "        y_pred = (y_prob >= cutoff).astype(int)\n",
        "        cm = confusion_matrix(y_true, y_pred, labels=[0, 1])  # 注意順序 [0,1]\n",
        "        TN, FP, FN, TP = cm.ravel()\n",
        "\n",
        "        total = TP + TN + FP + FN\n",
        "        accuracy = (TP + TN) / total if total > 0 else np.nan\n",
        "        sensitivity = TP / (TP + FN) if (TP + FN) > 0 else np.nan\n",
        "        specificity = TN / (TN + FP) if (TN + FP) > 0 else np.nan\n",
        "        youden = sensitivity + specificity - 1 if not np.isnan(sensitivity) and not np.isnan(specificity) else np.nan\n",
        "        ppv = TP / (TP + FP) if (TP + FP) > 0 else np.nan\n",
        "        npv = TN / (TN + FN) if (TN + FN) > 0 else np.nan\n",
        "\n",
        "        rows.append({\n",
        "            \"Cutoff\": cutoff,\n",
        "            \"TN\": TN, \"FP\": FP, \"FN\": FN, \"TP\": TP,\n",
        "            \"Accuracy\": round(accuracy*100, 1),\n",
        "            \"Sensitivity\": round(sensitivity*100, 1) if not np.isnan(sensitivity) else \"NC\",\n",
        "            \"Specificity\": round(specificity*100, 1) if not np.isnan(specificity) else \"NC\",\n",
        "            \"Youden\": round(youden*100, 1) if not np.isnan(youden) else \"NC\",\n",
        "            \"PPV\": round(ppv*100, 1) if not np.isnan(ppv) else \"NC\",\n",
        "            \"NPV\": round(npv*100, 1) if not np.isnan(npv) else \"NC\",\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "cutoff_table_all = cutoff_analysis(final_model, test_df, target_col=\"is_fraud\",\n",
        "                               cutoffs=[0,0.1,0.2,0.3,0.4,0.42,0.44,0.46,0.48,0.5,0.52,0.54,0.56,0.58,0.6,0.7,0.8,0.9,1])\n",
        "\n",
        "print(cutoff_table_all.to_string(index=False))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Cutoff      TN      FP   FN   TP  Accuracy  Sensitivity  Specificity  Youden   PPV    NPV\n",
            " 0.0000       0 1780252    0 2741       0.2        100.0          0.0     0.0   0.2     NC\n",
            " 0.0005 1214786  565466  215 2526      68.3         92.2         68.2    60.4   0.4  100.0\n",
            " 0.0010 1354820  425432  245 2496      76.1         91.1         76.1    67.2   0.6  100.0\n",
            " 0.0015 1427179  353073  295 2446      80.2         89.2         80.2    69.4   0.7  100.0\n",
            " 0.0020 1472380  307872  341 2400      82.7         87.6         82.7    70.3   0.8  100.0\n",
            " 0.0025 1506029  274223  400 2341      84.6         85.4         84.6    70.0   0.8  100.0\n",
            " 0.0030 1534355  245897  463 2278      86.2         83.1         86.2    69.3   0.9  100.0\n",
            " 0.0035 1558910  221342  530 2211      87.6         80.7         87.6    68.2   1.0  100.0\n",
            " 0.0040 1581214  199038  588 2153      88.8         78.5         88.8    67.4   1.1  100.0\n",
            " 0.0045 1601308  178944  645 2096      89.9         76.5         89.9    66.4   1.2  100.0\n",
            " 0.0050 1619492  160760  714 2027      90.9         74.0         91.0    64.9   1.2  100.0\n",
            " 0.0055 1635341  144911  765 1976      91.8         72.1         91.9    64.0   1.3  100.0\n",
            " 0.0060 1649322  130930  843 1898      92.6         69.2         92.6    61.9   1.4   99.9\n",
            " 0.0065 1661664  118588  906 1835      93.3         66.9         93.3    60.3   1.5   99.9\n",
            " 0.0070 1672718  107534  967 1774      93.9         64.7         94.0    58.7   1.6   99.9\n",
            " 0.0075 1682807   97445 1042 1699      94.5         62.0         94.5    56.5   1.7   99.9\n",
            " 0.0080 1691871   88381 1124 1617      95.0         59.0         95.0    54.0   1.8   99.9\n",
            " 0.0085 1699889   80363 1189 1552      95.4         56.6         95.5    52.1   1.9   99.9\n",
            " 0.0090 1707013   73239 1245 1496      95.8         54.6         95.9    50.5   2.0   99.9\n",
            " 0.0095 1713380   66872 1314 1427      96.2         52.1         96.2    48.3   2.1   99.9\n",
            " 0.0100 1719146   61106 1377 1364      96.5         49.8         96.6    46.3   2.2   99.9\n",
            " 0.0105 1724180   56072 1438 1303      96.8         47.5         96.9    44.4   2.3   99.9\n",
            " 0.0110 1728742   51510 1498 1243      97.0         45.3         97.1    42.5   2.4   99.9\n",
            " 0.0115 1732886   47366 1554 1187      97.3         43.3         97.3    40.6   2.4   99.9\n",
            " 0.0120 1736650   43602 1596 1145      97.5         41.8         97.6    39.3   2.6   99.9\n",
            " 0.0125 1740109   40143 1653 1088      97.7         39.7         97.7    37.4   2.6   99.9\n",
            " 0.0130 1743366   36886 1708 1033      97.8         37.7         97.9    35.6   2.7   99.9\n",
            " 0.0135 1746214   34038 1747  994      98.0         36.3         98.1    34.4   2.8   99.9\n",
            " 0.0140 1748870   31382 1789  952      98.1         34.7         98.2    33.0   2.9   99.9\n",
            " 0.0145 1751303   28949 1839  902      98.3         32.9         98.4    31.3   3.0   99.9\n",
            " 0.0150 1753446   26806 1877  864      98.4         31.5         98.5    30.0   3.1   99.9\n",
            " 0.0155 1755403   24849 1910  831      98.5         30.3         98.6    28.9   3.2   99.9\n",
            " 0.0160 1757181   23071 1953  788      98.6         28.7         98.7    27.5   3.3   99.9\n",
            " 0.0165 1758734   21518 1980  761      98.7         27.8         98.8    26.6   3.4   99.9\n",
            " 0.0170 1760142   20110 2005  736      98.8         26.9         98.9    25.7   3.5   99.9\n",
            " 0.0175 1761392   18860 2026  715      98.8         26.1         98.9    25.0   3.7   99.9\n",
            " 0.0180 1762614   17638 2053  688      98.9         25.1         99.0    24.1   3.8   99.9\n",
            " 0.0185 1763717   16535 2077  664      99.0         24.2         99.1    23.3   3.9   99.9\n",
            " 0.0190 1764750   15502 2105  636      99.0         23.2         99.1    22.3   3.9   99.9\n",
            " 0.0195 1765662   14590 2132  609      99.1         22.2         99.2    21.4   4.0   99.9\n",
            " 0.0200 1766539   13713 2158  583      99.1         21.3         99.2    20.5   4.1   99.9\n",
            " 0.0205 1767349   12903 2180  561      99.2         20.5         99.3    19.7   4.2   99.9\n",
            " 0.0210 1768200   12052 2204  537      99.2         19.6         99.3    18.9   4.3   99.9\n",
            " 0.0215 1768922   11330 2228  513      99.2         18.7         99.4    18.1   4.3   99.9\n",
            " 0.0220 1769602   10650 2249  492      99.3         17.9         99.4    17.4   4.4   99.9\n",
            " 0.0225 1770245   10007 2271  470      99.3         17.1         99.4    16.6   4.5   99.9\n",
            " 0.0230 1770838    9414 2288  453      99.3         16.5         99.5    16.0   4.6   99.9\n",
            " 0.0235 1771421    8831 2311  430      99.4         15.7         99.5    15.2   4.6   99.9\n",
            " 0.0240 1771919    8333 2329  412      99.4         15.0         99.5    14.6   4.7   99.9\n",
            " 0.0245 1772389    7863 2349  392      99.4         14.3         99.6    13.9   4.7   99.9\n",
            " 0.0250 1772842    7410 2362  379      99.5         13.8         99.6    13.4   4.9   99.9\n",
            " 0.0255 1773233    7019 2379  362      99.5         13.2         99.6    12.8   4.9   99.9\n",
            " 0.0260 1773626    6626 2394  347      99.5         12.7         99.6    12.3   5.0   99.9\n",
            " 0.0265 1774017    6235 2410  331      99.5         12.1         99.6    11.7   5.0   99.9\n",
            " 0.0270 1774324    5928 2427  314      99.5         11.5         99.7    11.1   5.0   99.9\n",
            " 0.0275 1774647    5605 2436  305      99.5         11.1         99.7    10.8   5.2   99.9\n",
            " 0.0280 1774948    5304 2447  294      99.6         10.7         99.7    10.4   5.3   99.9\n",
            " 0.0285 1775246    5006 2463  278      99.6         10.1         99.7     9.9   5.3   99.9\n",
            " 0.0290 1775504    4748 2476  265      99.6          9.7         99.7     9.4   5.3   99.9\n",
            " 0.0295 1775716    4536 2484  257      99.6          9.4         99.7     9.1   5.4   99.9\n",
            " 0.0300 1775915    4337 2501  240      99.6          8.8         99.8     8.5   5.2   99.9\n",
            " 0.0305 1776105    4147 2506  235      99.6          8.6         99.8     8.3   5.4   99.9\n",
            " 0.0310 1776291    3961 2516  225      99.6          8.2         99.8     8.0   5.4   99.9\n",
            " 0.0315 1776488    3764 2523  218      99.6          8.0         99.8     7.7   5.5   99.9\n",
            " 0.0320 1776672    3580 2535  206      99.7          7.5         99.8     7.3   5.4   99.9\n",
            " 0.0325 1776844    3408 2541  200      99.7          7.3         99.8     7.1   5.5   99.9\n",
            " 0.0330 1777009    3243 2547  194      99.7          7.1         99.8     6.9   5.6   99.9\n",
            " 0.0335 1777144    3108 2554  187      99.7          6.8         99.8     6.6   5.7   99.9\n",
            " 0.0340 1777302    2950 2557  184      99.7          6.7         99.8     6.5   5.9   99.9\n",
            " 0.0345 1777437    2815 2560  181      99.7          6.6         99.8     6.4   6.0   99.9\n",
            " 0.0350 1777553    2699 2565  176      99.7          6.4         99.8     6.3   6.1   99.9\n",
            " 0.0355 1777682    2570 2574  167      99.7          6.1         99.9     5.9   6.1   99.9\n",
            " 0.0360 1777795    2457 2578  163      99.7          5.9         99.9     5.8   6.2   99.9\n",
            " 0.0365 1777897    2355 2583  158      99.7          5.8         99.9     5.6   6.3   99.9\n",
            " 0.0370 1778014    2238 2588  153      99.7          5.6         99.9     5.5   6.4   99.9\n",
            " 0.0375 1778109    2143 2590  151      99.7          5.5         99.9     5.4   6.6   99.9\n",
            " 0.0380 1778198    2054 2596  145      99.7          5.3         99.9     5.2   6.6   99.9\n",
            " 0.0385 1778293    1959 2599  142      99.7          5.2         99.9     5.1   6.8   99.9\n",
            " 0.0390 1778367    1885 2600  141      99.7          5.1         99.9     5.0   7.0   99.9\n",
            " 0.0395 1778446    1806 2603  138      99.8          5.0         99.9     4.9   7.1   99.9\n",
            " 0.0400 1778520    1732 2608  133      99.8          4.9         99.9     4.8   7.1   99.9\n",
            " 0.0405 1778599    1653 2611  130      99.8          4.7         99.9     4.6   7.3   99.9\n",
            " 0.0410 1778659    1593 2614  127      99.8          4.6         99.9     4.5   7.4   99.9\n",
            " 0.0415 1778718    1534 2617  124      99.8          4.5         99.9     4.4   7.5   99.9\n",
            " 0.0420 1778784    1468 2622  119      99.8          4.3         99.9     4.3   7.5   99.9\n",
            " 0.0425 1778849    1403 2626  115      99.8          4.2         99.9     4.1   7.6   99.9\n",
            " 0.0430 1778902    1350 2631  110      99.8          4.0         99.9     3.9   7.5   99.9\n",
            " 0.0435 1778952    1300 2638  103      99.8          3.8         99.9     3.7   7.3   99.9\n",
            " 0.0440 1779004    1248 2643   98      99.8          3.6         99.9     3.5   7.3   99.9\n",
            " 0.0445 1779049    1203 2646   95      99.8          3.5         99.9     3.4   7.3   99.9\n",
            " 0.0450 1779080    1172 2651   90      99.8          3.3         99.9     3.2   7.1   99.9\n",
            " 0.0455 1779134    1118 2656   85      99.8          3.1         99.9     3.0   7.1   99.9\n",
            " 0.0460 1779172    1080 2659   82      99.8          3.0         99.9     2.9   7.1   99.9\n",
            " 0.0465 1779219    1033 2661   80      99.8          2.9         99.9     2.9   7.2   99.9\n",
            " 0.0470 1779252    1000 2661   80      99.8          2.9         99.9     2.9   7.4   99.9\n",
            " 0.0475 1779303     949 2661   80      99.8          2.9         99.9     2.9   7.8   99.9\n",
            " 0.0480 1779331     921 2663   78      99.8          2.8         99.9     2.8   7.8   99.9\n",
            " 0.0485 1779362     890 2665   76      99.8          2.8        100.0     2.7   7.9   99.9\n",
            " 0.0490 1779378     874 2669   72      99.8          2.6        100.0     2.6   7.6   99.9\n",
            " 0.0495 1779401     851 2671   70      99.8          2.6        100.0     2.5   7.6   99.9\n",
            " 0.0500 1779423     829 2672   69      99.8          2.5        100.0     2.5   7.7   99.9\n",
            " 0.0505 1779441     811 2674   67      99.8          2.4        100.0     2.4   7.6   99.8\n",
            " 0.0510 1779458     794 2676   65      99.8          2.4        100.0     2.3   7.6   99.8\n",
            " 0.0515 1779491     761 2680   61      99.8          2.2        100.0     2.2   7.4   99.8\n",
            " 0.0520 1779527     725 2683   58      99.8          2.1        100.0     2.1   7.4   99.8\n",
            " 0.0525 1779553     699 2685   56      99.8          2.0        100.0     2.0   7.4   99.8\n",
            " 0.0530 1779577     675 2685   56      99.8          2.0        100.0     2.0   7.7   99.8\n",
            " 0.0535 1779599     653 2688   53      99.8          1.9        100.0     1.9   7.5   99.8\n",
            " 0.0540 1779623     629 2689   52      99.8          1.9        100.0     1.9   7.6   99.8\n",
            " 0.0545 1779643     609 2690   51      99.8          1.9        100.0     1.8   7.7   99.8\n",
            " 0.0550 1779667     585 2691   50      99.8          1.8        100.0     1.8   7.9   99.8\n",
            " 0.0555 1779687     565 2691   50      99.8          1.8        100.0     1.8   8.1   99.8\n",
            " 0.0560 1779703     549 2693   48      99.8          1.8        100.0     1.7   8.0   99.8\n",
            " 0.0565 1779720     532 2694   47      99.8          1.7        100.0     1.7   8.1   99.8\n",
            " 0.0570 1779731     521 2695   46      99.8          1.7        100.0     1.6   8.1   99.8\n",
            " 0.0575 1779745     507 2695   46      99.8          1.7        100.0     1.6   8.3   99.8\n",
            " 0.0580 1779760     492 2695   46      99.8          1.7        100.0     1.7   8.6   99.8\n",
            " 0.0585 1779778     474 2698   43      99.8          1.6        100.0     1.5   8.3   99.8\n",
            " 0.0590 1779795     457 2701   40      99.8          1.5        100.0     1.4   8.0   99.8\n",
            " 0.0595 1779807     445 2704   37      99.8          1.3        100.0     1.3   7.7   99.8\n",
            " 0.0600 1779821     431 2705   36      99.8          1.3        100.0     1.3   7.7   99.8\n",
            " 0.0605 1779835     417 2705   36      99.8          1.3        100.0     1.3   7.9   99.8\n",
            " 0.0610 1779841     411 2707   34      99.8          1.2        100.0     1.2   7.6   99.8\n",
            " 0.0615 1779850     402 2708   33      99.8          1.2        100.0     1.2   7.6   99.8\n",
            " 0.0620 1779858     394 2709   32      99.8          1.2        100.0     1.1   7.5   99.8\n",
            " 0.0625 1779867     385 2710   31      99.8          1.1        100.0     1.1   7.5   99.8\n",
            " 0.0630 1779877     375 2710   31      99.8          1.1        100.0     1.1   7.6   99.8\n",
            " 0.0635 1779882     370 2710   31      99.8          1.1        100.0     1.1   7.7   99.8\n",
            " 0.0640 1779891     361 2710   31      99.8          1.1        100.0     1.1   7.9   99.8\n",
            " 0.0645 1779907     345 2710   31      99.8          1.1        100.0     1.1   8.2   99.8\n",
            " 0.0650 1779913     339 2710   31      99.8          1.1        100.0     1.1   8.4   99.8\n",
            " 0.0655 1779924     328 2711   30      99.8          1.1        100.0     1.1   8.4   99.8\n",
            " 0.0660 1779933     319 2712   29      99.8          1.1        100.0     1.0   8.3   99.8\n",
            " 0.0665 1779946     306 2716   25      99.8          0.9        100.0     0.9   7.6   99.8\n",
            " 0.0670 1779952     300 2716   25      99.8          0.9        100.0     0.9   7.7   99.8\n",
            " 0.0675 1779959     293 2716   25      99.8          0.9        100.0     0.9   7.9   99.8\n",
            " 0.0680 1779966     286 2716   25      99.8          0.9        100.0     0.9   8.0   99.8\n",
            " 0.0685 1779970     282 2716   25      99.8          0.9        100.0     0.9   8.1   99.8\n",
            " 0.0690 1779979     273 2717   24      99.8          0.9        100.0     0.9   8.1   99.8\n",
            " 0.0695 1779981     271 2718   23      99.8          0.8        100.0     0.8   7.8   99.8\n",
            " 0.0700 1779988     264 2718   23      99.8          0.8        100.0     0.8   8.0   99.8\n",
            " 0.0705 1779992     260 2718   23      99.8          0.8        100.0     0.8   8.1   99.8\n",
            " 0.0710 1779999     253 2719   22      99.8          0.8        100.0     0.8   8.0   99.8\n",
            " 0.0715 1780004     248 2720   21      99.8          0.8        100.0     0.8   7.8   99.8\n",
            " 0.0720 1780011     241 2720   21      99.8          0.8        100.0     0.8   8.0   99.8\n",
            " 0.0725 1780018     234 2720   21      99.8          0.8        100.0     0.8   8.2   99.8\n",
            " 0.0730 1780024     228 2720   21      99.8          0.8        100.0     0.8   8.4   99.8\n",
            " 0.0735 1780028     224 2720   21      99.8          0.8        100.0     0.8   8.6   99.8\n",
            " 0.0740 1780030     222 2720   21      99.8          0.8        100.0     0.8   8.6   99.8\n",
            " 0.0745 1780036     216 2720   21      99.8          0.8        100.0     0.8   8.9   99.8\n",
            " 0.0750 1780039     213 2721   20      99.8          0.7        100.0     0.7   8.6   99.8\n",
            " 0.0755 1780042     210 2723   18      99.8          0.7        100.0     0.6   7.9   99.8\n",
            " 0.0760 1780050     202 2723   18      99.8          0.7        100.0     0.6   8.2   99.8\n",
            " 0.0765 1780055     197 2724   17      99.8          0.6        100.0     0.6   7.9   99.8\n",
            " 0.0770 1780060     192 2724   17      99.8          0.6        100.0     0.6   8.1   99.8\n",
            " 0.0775 1780062     190 2724   17      99.8          0.6        100.0     0.6   8.2   99.8\n",
            " 0.0780 1780066     186 2724   17      99.8          0.6        100.0     0.6   8.4   99.8\n",
            " 0.0785 1780070     182 2724   17      99.8          0.6        100.0     0.6   8.5   99.8\n",
            " 0.0790 1780073     179 2724   17      99.8          0.6        100.0     0.6   8.7   99.8\n",
            " 0.0795 1780079     173 2724   17      99.8          0.6        100.0     0.6   8.9   99.8\n",
            " 0.0800 1780082     170 2724   17      99.8          0.6        100.0     0.6   9.1   99.8\n",
            " 0.0805 1780085     167 2724   17      99.8          0.6        100.0     0.6   9.2   99.8\n",
            " 0.0810 1780088     164 2725   16      99.8          0.6        100.0     0.6   8.9   99.8\n",
            " 0.0815 1780091     161 2725   16      99.8          0.6        100.0     0.6   9.0   99.8\n",
            " 0.0820 1780093     159 2725   16      99.8          0.6        100.0     0.6   9.1   99.8\n",
            " 0.0825 1780095     157 2725   16      99.8          0.6        100.0     0.6   9.2   99.8\n",
            " 0.0830 1780101     151 2726   15      99.8          0.5        100.0     0.5   9.0   99.8\n",
            " 0.0835 1780103     149 2726   15      99.8          0.5        100.0     0.5   9.1   99.8\n",
            " 0.0840 1780104     148 2726   15      99.8          0.5        100.0     0.5   9.2   99.8\n",
            " 0.0845 1780106     146 2726   15      99.8          0.5        100.0     0.5   9.3   99.8\n",
            " 0.0850 1780109     143 2726   15      99.8          0.5        100.0     0.5   9.5   99.8\n",
            " 0.0855 1780110     142 2726   15      99.8          0.5        100.0     0.5   9.6   99.8\n",
            " 0.0860 1780114     138 2728   13      99.8          0.5        100.0     0.5   8.6   99.8\n",
            " 0.0865 1780117     135 2728   13      99.8          0.5        100.0     0.5   8.8   99.8\n",
            " 0.0870 1780119     133 2728   13      99.8          0.5        100.0     0.5   8.9   99.8\n",
            " 0.0875 1780124     128 2728   13      99.8          0.5        100.0     0.5   9.2   99.8\n",
            " 0.0880 1780128     124 2728   13      99.8          0.5        100.0     0.5   9.5   99.8\n",
            " 0.0885 1780129     123 2728   13      99.8          0.5        100.0     0.5   9.6   99.8\n",
            " 0.0890 1780130     122 2728   13      99.8          0.5        100.0     0.5   9.6   99.8\n",
            " 0.0895 1780131     121 2728   13      99.8          0.5        100.0     0.5   9.7   99.8\n",
            " 0.0900 1780133     119 2728   13      99.8          0.5        100.0     0.5   9.8   99.8\n",
            " 0.0905 1780138     114 2728   13      99.8          0.5        100.0     0.5  10.2   99.8\n",
            " 0.0910 1780143     109 2729   12      99.8          0.4        100.0     0.4   9.9   99.8\n",
            " 0.0915 1780144     108 2729   12      99.8          0.4        100.0     0.4  10.0   99.8\n",
            " 0.0920 1780147     105 2729   12      99.8          0.4        100.0     0.4  10.3   99.8\n",
            " 0.0925 1780148     104 2730   11      99.8          0.4        100.0     0.4   9.6   99.8\n",
            " 0.0930 1780150     102 2730   11      99.8          0.4        100.0     0.4   9.7   99.8\n",
            " 0.0935 1780153      99 2730   11      99.8          0.4        100.0     0.4  10.0   99.8\n",
            " 0.0940 1780156      96 2731   10      99.8          0.4        100.0     0.4   9.4   99.8\n",
            " 0.0945 1780156      96 2731   10      99.8          0.4        100.0     0.4   9.4   99.8\n",
            " 0.0950 1780157      95 2731   10      99.8          0.4        100.0     0.4   9.5   99.8\n",
            " 0.0955 1780160      92 2733    8      99.8          0.3        100.0     0.3   8.0   99.8\n",
            " 0.0960 1780161      91 2733    8      99.8          0.3        100.0     0.3   8.1   99.8\n",
            " 0.0965 1780162      90 2733    8      99.8          0.3        100.0     0.3   8.2   99.8\n",
            " 0.0970 1780164      88 2733    8      99.8          0.3        100.0     0.3   8.3   99.8\n",
            " 0.0975 1780167      85 2733    8      99.8          0.3        100.0     0.3   8.6   99.8\n",
            " 0.0980 1780167      85 2733    8      99.8          0.3        100.0     0.3   8.6   99.8\n",
            " 0.0985 1780170      82 2733    8      99.8          0.3        100.0     0.3   8.9   99.8\n",
            " 0.0990 1780175      77 2733    8      99.8          0.3        100.0     0.3   9.4   99.8\n",
            " 0.0995 1780177      75 2733    8      99.8          0.3        100.0     0.3   9.6   99.8\n",
            " 0.1000 1780178      74 2734    7      99.8          0.3        100.0     0.3   8.6   99.8\n",
            " 0.1005 1780180      72 2734    7      99.8          0.3        100.0     0.3   8.9   99.8\n",
            " 0.1010 1780183      69 2734    7      99.8          0.3        100.0     0.3   9.2   99.8\n",
            " 0.1015 1780185      67 2734    7      99.8          0.3        100.0     0.3   9.5   99.8\n",
            " 0.1020 1780186      66 2734    7      99.8          0.3        100.0     0.3   9.6   99.8\n",
            " 0.1025 1780187      65 2734    7      99.8          0.3        100.0     0.3   9.7   99.8\n",
            " 0.1030 1780188      64 2734    7      99.8          0.3        100.0     0.3   9.9   99.8\n",
            " 0.1035 1780188      64 2735    6      99.8          0.2        100.0     0.2   8.6   99.8\n",
            " 0.1040 1780189      63 2735    6      99.8          0.2        100.0     0.2   8.7   99.8\n",
            " 0.1045 1780190      62 2735    6      99.8          0.2        100.0     0.2   8.8   99.8\n",
            " 0.1050 1780191      61 2735    6      99.8          0.2        100.0     0.2   9.0   99.8\n",
            " 0.1055 1780196      56 2735    6      99.8          0.2        100.0     0.2   9.7   99.8\n",
            " 0.1060 1780197      55 2735    6      99.8          0.2        100.0     0.2   9.8   99.8\n",
            " 0.1065 1780197      55 2735    6      99.8          0.2        100.0     0.2   9.8   99.8\n",
            " 0.1070 1780200      52 2735    6      99.8          0.2        100.0     0.2  10.3   99.8\n",
            " 0.1075 1780201      51 2735    6      99.8          0.2        100.0     0.2  10.5   99.8\n",
            " 0.1080 1780202      50 2735    6      99.8          0.2        100.0     0.2  10.7   99.8\n",
            " 0.1085 1780202      50 2735    6      99.8          0.2        100.0     0.2  10.7   99.8\n",
            " 0.1090 1780204      48 2735    6      99.8          0.2        100.0     0.2  11.1   99.8\n",
            " 0.1095 1780206      46 2735    6      99.8          0.2        100.0     0.2  11.5   99.8\n",
            " 0.2000 1780248       4 2740    1      99.8          0.0        100.0     0.0  20.0   99.8\n",
            " 0.3000 1780252       0 2741    0      99.8          0.0        100.0     0.0    NC   99.8\n",
            " 0.4000 1780252       0 2741    0      99.8          0.0        100.0     0.0    NC   99.8\n",
            " 0.5000 1780252       0 2741    0      99.8          0.0        100.0     0.0    NC   99.8\n",
            " 0.6000 1780252       0 2741    0      99.8          0.0        100.0     0.0    NC   99.8\n",
            " 0.7000 1780252       0 2741    0      99.8          0.0        100.0     0.0    NC   99.8\n",
            " 0.8000 1780252       0 2741    0      99.8          0.0        100.0     0.0    NC   99.8\n",
            " 0.9000 1780252       0 2741    0      99.8          0.0        100.0     0.0    NC   99.8\n",
            " 1.0000 1780252       0 2741    0      99.8          0.0        100.0     0.0    NC   99.8\n"
          ]
        }
      ],
      "source": [
        "cutoffs_v1 = [round(x, 4) for x in np.arange(0, 0.11, 0.0005)]  # 0 ~ 0.2 間隔 0.02\n",
        "cutoffs_v1 += [round(x, 4) for x in np.arange(0.2, 1.01, 0.1)]  # 0.3 ~ 1 間隔 0.1\n",
        "\n",
        "cutoff_table_zoom = cutoff_analysis(final_model, test_df, target_col=\"is_fraud\", cutoffs=cutoffs_v1)\n",
        "print(cutoff_table_zoom.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AUC values:\n",
            "Model_1 (amount): 0.7063\n",
            "Model_2 (Zip): 0.7700\n",
            "Model_3 (longitude): 0.7897\n",
            "Model_4 (mercahnt_id): 0.8962\n",
            "Final Model (26vars): 0.9060\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "def plot_multiple_roc(models, model_names, test_df, target_col=\"is_fraud\"):\n",
        "    \"\"\"\n",
        "    models: list of fitted statsmodels.Logit models\n",
        "    model_names: list of strings\n",
        "    test_df: test dataframe\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.style.use('seaborn-v0_8-darkgrid')\n",
        "    plt.rcParams['axes.facecolor'] = '#f0f0f0'\n",
        "\n",
        "    # 隨機機率線\n",
        "    plt.plot([0, 1], [0, 1], color='gray', linestyle='-', label='Random Chance')\n",
        "\n",
        "    auc_values = {}\n",
        "\n",
        "    for model, name in zip(models, model_names):\n",
        "        # 取出模型變數\n",
        "        vars_used = model.params.index.drop(\"const\")\n",
        "        X_test = sm.add_constant(test_df[vars_used])\n",
        "\n",
        "        # 計算預測機率\n",
        "        y_true = test_df[target_col]\n",
        "        y_score = model.predict(X_test)\n",
        "\n",
        "        # ROC 曲線\n",
        "        fpr, tpr, _ = roc_curve(y_true, y_score)\n",
        "        auc_val = roc_auc_score(y_true, y_score)\n",
        "        auc_values[name] = auc_val\n",
        "\n",
        "        plt.plot(fpr, tpr, label=f\"{name} (AUC={auc_val:.4f})\")\n",
        "\n",
        "    plt.title(\"Comparing Independent Variables and Final Stepwise Model with ROC Curve\", fontsize=13, fontweight='bold')\n",
        "    plt.xlabel(\"1 - Specificity\")\n",
        "    plt.ylabel(\"Sensitivity\")\n",
        "    plt.legend(loc=\"lower right\", frameon=True)\n",
        "    plt.show()\n",
        "\n",
        "    # 印出 AUC summary\n",
        "    print(\"AUC values:\")\n",
        "    for name, val in auc_values.items():\n",
        "        print(f\"{name}: {val:.4f}\")\n",
        "\n",
        "# 假設你已經有以下模型：\n",
        "# model_1 = sm.Logit(y_train, sm.add_constant(X_train[[\"X13\"]])).fit(disp=False)\n",
        "# model_2 = sm.Logit(y_train, sm.add_constant(X_train[[\"X17\"]])).fit(disp=False)\n",
        "# final_model = ...\n",
        "\n",
        "# 範例呼叫\n",
        "plot_multiple_roc(\n",
        "    models=[model_0, model_2, model_3,model_4, final_model],\n",
        "    model_names=[\"Model_1 (amount)\", \"Model_2 (Zip)\", \"Model_3 (longitude)\",\"Model_4 (mercahnt_id)\", \"Final Model (26vars)\"],\n",
        "    test_df=test_df,\n",
        "    target_col=\"is_fraud\"\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "virtual",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
