{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01_import dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 01-1_import data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "#https://drive.google.com/drive/folders/18qV82fNY3IIWu3BRoGqm_LNgJzE8Akbr?usp=drive_link\n",
        "#base_dir = \"/Users/Andypon/10_交大研究所/1141_01_機器學習與金融科技/data\"\n",
        "base_dir= '/Users/andyw.p.chen/Documents/Project/datasets'\n",
        "#base_dir=  \"c:\\Users\\user\\Downloads\\datasets\"\n",
        "\n",
        "def load_json_to_df(filename: str) -> pd.DataFrame:\n",
        "    file_path = os.path.join(base_dir, filename)\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # 如果是 { \"target\": {id: value, ...} }\n",
        "    if isinstance(data, dict) and len(data) == 1 and isinstance(next(iter(data.values())), dict):\n",
        "        key, inner = next(iter(data.items()))\n",
        "        return pd.DataFrame(list(inner.items()), columns=[\"id\", key])\n",
        "\n",
        "    # dict of scalar\n",
        "    if isinstance(data, dict):\n",
        "        return pd.DataFrame([{\"code\": k, \"desc\": v} for k, v in data.items()])\n",
        "\n",
        "    # list of dict\n",
        "    elif isinstance(data, list):\n",
        "        return pd.DataFrame(data)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported JSON structure in {filename}: {type(data)}\")\n",
        "\n",
        "\n",
        "def load_csv_to_df(filename: str) -> pd.DataFrame:\n",
        "    \"\"\"讀取 CSV 並轉為 DataFrame。\"\"\"\n",
        "    return pd.read_csv(os.path.join(base_dir, filename))\n",
        "\n",
        "# JSON 資料\n",
        "##mcc_codes_df = load_json_to_df(\"mcc_codes.json\")\n",
        "train_fraud_labels_df = load_json_to_df(\"train_fraud_labels.json\")\n",
        "\n",
        "# CSV 資料\n",
        "cards_df = load_csv_to_df(\"cards_data.csv\")\n",
        "transactions_df = load_csv_to_df(\"transactions_data.csv\")\n",
        "users_df = load_csv_to_df(\"users_data.csv\")\n",
        "\n",
        "# 簡單檢查\n",
        "#print(mcc_codes_df.head())\n",
        "#print(train_fraud_labels_df.head())\n",
        "#print(cards_df.head())\n",
        "#print(transactions_df.head())\n",
        "#print(users_df.apthead())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 01-2_rename variable in each data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_fraud_labels_df = train_fraud_labels_df.rename(columns={'id': 'transactions_id'})\n",
        "train_fraud_labels_df = train_fraud_labels_df.rename(columns={'target': 'is_fraud'})\n",
        "\n",
        "cards_df = cards_df.rename(columns={'id':'card_id'})\n",
        "\n",
        "users_df = users_df.rename(columns={'id':'client_id'})\n",
        "\n",
        "transactions_df = transactions_df.rename(columns={'mcc': 'mcc_code'})\n",
        "transactions_df = transactions_df.rename(columns={'id': 'transaction_id'})\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 01-3_變數型態統一及缺失值處理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_missing_flags(df: pd.DataFrame, cols: list) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    在 DataFrame 中對指定欄位建立 missing flag 欄位\n",
        "    flag=1 表示缺失值，flag=0 表示非缺失值\n",
        "    \n",
        "    參數\n",
        "    ----\n",
        "    df : pd.DataFrame\n",
        "        輸入的資料框\n",
        "    cols : list\n",
        "        要檢查的欄位名稱清單\n",
        "    \n",
        "    回傳\n",
        "    ----\n",
        "    pd.DataFrame : 新的資料框 (含新增的 flag 欄位)\n",
        "    \"\"\"\n",
        "    for col in cols:\n",
        "        df[f\"{col}_missing_flag\"] = df[col].isna().astype(int)\n",
        "    return df\n",
        "\n",
        "transactions_df = add_missing_flags(transactions_df, [\"merchant_state\", \"zip\", \"errors\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "##train_fraud_labels_df##\n",
        "train_fraud_labels_df[\"is_fraud\"]=train_fraud_labels_df[\"is_fraud\"].astype(\"category\") \n",
        "train_fraud_labels_df[\"transactions_id\"]=train_fraud_labels_df[\"transactions_id\"].astype(int) #合併資料需要\n",
        "\n",
        "##cards_df##\n",
        "cards_df[\"card_brand\"]=cards_df[\"card_brand\"].astype(\"category\") \n",
        "cards_df[\"card_type\"]=cards_df[\"card_type\"].astype(\"category\")\n",
        "#####不要load這行 cards_df[\"expires\"]=pd.to_datetime(cards_df[\"expires\"], format=\"%m/%Y\")\n",
        "cards_df[\"expires\"] = pd.to_datetime(cards_df[\"expires\"], format=\"%m/%Y\").dt.to_period(\"M\")\n",
        "cards_df[\"has_chip\"]=cards_df[\"has_chip\"].astype(\"category\")\n",
        "\n",
        "cards_df['credit_limit'] = cards_df['credit_limit'].replace(r'[\\$,]', '', regex=True).astype(int)\n",
        "#####不要load這行 cards_df[\"acct_open_date\"]=pd.to_datetime(cards_df[\"acct_open_date\"], format=\"%m/%Y\")\n",
        "cards_df[\"acct_open_date\"] = pd.to_datetime(cards_df[\"acct_open_date\"], format=\"%m/%Y\").dt.to_period(\"M\")\n",
        "#####不要load這行 cards_df[\"year_pin_last_changed\"]=pd.to_datetime(cards_df[\"year_pin_last_changed\"], format=\"%Y\")\n",
        "cards_df[\"year_pin_last_changed\"] = pd.to_datetime(cards_df[\"year_pin_last_changed\"], format=\"%Y\").dt.to_period(\"Y\")\n",
        "cards_df[\"card_on_dark_web\"]=cards_df[\"card_on_dark_web\"].astype(\"category\") \n",
        "\n",
        "##users_df##\n",
        "users_df[\"birth_year\"] = pd.to_datetime(users_df[\"birth_year\"], format=\"%Y\").dt.to_period(\"Y\")\n",
        "users_df[\"birth_month\"] = pd.to_datetime(users_df[\"birth_month\"], format=\"%m\").dt.to_period(\"M\")\n",
        "users_df[\"gender\"]=users_df[\"gender\"].astype(\"category\") \n",
        "users_df['per_capita_income'] = users_df['per_capita_income'].replace(r'[\\$,]', '', regex=True).astype(int)\n",
        "users_df['yearly_income'] = users_df['yearly_income'].replace(r'[\\$,]', '', regex=True).astype(int)\n",
        "users_df['total_debt'] = users_df['total_debt'].replace(r'[\\$,]', '', regex=True).astype(int)\n",
        "\n",
        "##transactions_df##\n",
        "transactions_df[\"date\"] = pd.to_datetime(transactions_df[\"date\"])\n",
        "#浮點數轉整數原因確定？\n",
        "transactions_df['amount'] = transactions_df['amount'].replace(r'[\\$,]', '', regex=True).astype(float).astype(int)\n",
        "##負數取log調成1\n",
        "#transactions_df['amount'] = transactions_df['amount'].replace(r'[\\$,]', '', regex=True).astype(float)\n",
        "\n",
        "transactions_df[\"use_chip\"]=transactions_df[\"use_chip\"].astype(\"category\") \n",
        "\n",
        "transactions_df.loc[\n",
        "    transactions_df['merchant_city'].str.lower() == 'online',\n",
        "    'merchant_state'\n",
        "] = 'online'\n",
        "\n",
        "transactions_df.loc[\n",
        "    transactions_df['merchant_city'].str.lower() == 'online',\n",
        "    'zip'\n",
        "] = 20000 #原本是-1\n",
        "## 我沒有全部改，這樣完之後仍有89006筆Missing，剩下都是在國外\n",
        "transactions_df['zip'] = transactions_df['zip'].fillna(10000) #原本是-999\n",
        "transactions_df[\"zip\"]=transactions_df[\"zip\"].astype(\"int64\")\n",
        "\n",
        "transactions_df['errors'] = transactions_df['errors'].astype('category')\n",
        "transactions_df['errors'] = transactions_df['errors'].cat.add_categories('No_error').fillna('No_error')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "#cars one hot encoding\n",
        "##統一類別變數轉dummy variable(要注意共線性問題，應刪掉其中之一)\n",
        "\n",
        "#card_type 原始種類：Debit_57%, Credit_33%, Debit(Prepaid)_9%\n",
        "#card_brand 原始種類：MasterCard_52%, Visa_38%, Amex_7%, Discovery_3%\n",
        "#has_chip 原始種類：Yes_89%, No_11%\n",
        "#card_on_dark_web 原始種類：No_0%\n",
        "cols_to_encode = ['card_type', 'card_brand', 'has_chip']\n",
        "cards_df[cols_to_encode] = cards_df[cols_to_encode].astype('category')\n",
        "dummies_cards = pd.get_dummies(\n",
        "    cards_df[cols_to_encode], \n",
        "    prefix=cols_to_encode, \n",
        "    dtype='uint8'\n",
        "    )\n",
        "cards_df = pd.concat([cards_df, dummies_cards], axis=1)\n",
        "\n",
        "#use_chip 原始種類：Swiped_52%, Chipe_36%, Online_12%\n",
        "dummies_use = pd.get_dummies(transactions_df['use_chip'], prefix='use_chip', dtype='uint8')\n",
        "transactions_df = pd.concat([transactions_df, dummies_use], axis=1)\n",
        "\n",
        "#gender 原始種類：Female_51%, Male_49%\n",
        "dummies_gender = pd.get_dummies(users_df['gender'], prefix='gender', dtype='uint8')\n",
        "users_df = pd.concat([users_df, dummies_gender], axis=1)\n",
        "\n",
        "\n",
        "cards_df.drop(columns=[\"has_chip_NO\",\"has_chip\"], inplace=True)\n",
        "transactions_df.drop(columns=[\"use_chip\"], inplace=True)\n",
        "users_df.drop(columns=[\"gender_Female\"], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 01-4_測試用code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  merchant_state merchant_city    zip\n",
            "0             ND        Beulah  58523\n",
            "1             IA    Bettendorf  52722\n",
            "2             CA         Vista  92084\n",
            "3             IN   Crown Point  46307\n",
            "4             MD       Harwood  20776\n"
          ]
        }
      ],
      "source": [
        "##不用執行～～(本來試圖建立對照表將Missing的zip補上)\n",
        "\n",
        "##檢查89006筆Missing的zip\n",
        "c_missing_zip = transactions_df[transactions_df[\"zip\"].isna()]\n",
        "c_mexico_zip = transactions_df[transactions_df[\"merchant_state\"]==\"Mexico\"]\n",
        "#c_mcc_mv_zip = c_missing_zip[\n",
        "#    (c_missing_zip[\"mcc_code\"] > 5400) & (c_missing_zip[\"mcc_code\"] < 5700)\n",
        "#]\n",
        "\n",
        "\n",
        "\n",
        "# 先建立 mapping table：一組 state+city 可能對應多個 zip\n",
        "mapping_df = (\n",
        "    transactions_df\n",
        "    .dropna(subset=[\"zip\"])                                   # 只要 zip 有值的 row\n",
        "    .drop_duplicates(subset=[\"merchant_state\", \"merchant_city\", \"zip\"]) \n",
        "    [[\"merchant_state\", \"merchant_city\", \"zip\"]]              # 只留下需要的欄位\n",
        ")\n",
        "\n",
        "print(mapping_df.head())\n",
        "\n",
        "\n",
        "# 假設 df 已經存在\n",
        "# 建立新的欄位 F，B 與 C 合併\n",
        "c_missing_zip[\"fullname\"] = c_missing_zip[\"merchant_city\"].astype(str) + c_missing_zip[\"merchant_state\"].astype(str)\n",
        "# 建立新的 DataFrame，只取 A, D, F\n",
        "df_small = c_missing_zip[[\"transaction_id\", \"fullname\",\"zip\"]]\n",
        "\n",
        "mapping_df[\"mfullname\"] = mapping_df[\"merchant_city\"].astype(str) + mapping_df[\"merchant_state\"].astype(str)\n",
        "\n",
        "# 先建立一個 lookup 字典\n",
        "lookup_dict = dict(zip(mapping_df[\"mfullname\"], mapping_df[\"zip\"]))\n",
        "\n",
        "# 用 map 當作 vlookup\n",
        "df_small[\"zip\"] = df_small[\"zip\"].fillna(df_small[\"fullname\"].map(lookup_dict))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02_資料整併成一張dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 02-1_資料整併"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "#transactions_df.loc[transactions_df[\"transaction_id\"] == 10649266] #transaction_id vs id\n",
        "\n",
        "#原始資料筆數：13305915\n",
        "### transactions_df+train_fraud_labels_df      left 會有4390952 missing values\n",
        "merged = pd.merge(transactions_df, train_fraud_labels_df, left_on=\"transaction_id\", right_on=\"transactions_id\", how=\"outer\")\n",
        "### transactions_df train_fraud_labels_df(8914963) + users_df 對過去不會有missing values\n",
        "merged = pd.merge(merged,users_df , left_on=\"client_id\", right_on=\"client_id\", how=\"left\")\n",
        "### transactions_df train_fraud_labels_df users_df + cards_df 對過去不會有missing values\n",
        "merged = pd.merge(merged,cards_df , left_on=\"card_id\", right_on=\"card_id\", how=\"left\")\n",
        "\n",
        "#刪掉重複的columns\n",
        "merged.drop(columns=[\"transactions_id\"], inplace=True)\n",
        "merged.drop(columns=[\"client_id_y\"], inplace=True)\n",
        "\n",
        "## 合併完之後最後處理is_fraud(原會有missing values問題)\n",
        "merged[\"is_fraud\"] = merged[\"is_fraud\"].astype(str)\n",
        "merged.loc[merged['is_fraud'].str.lower() == 'no','is_fraud'] = '0'\n",
        "merged.loc[merged['is_fraud'].str.lower() == 'yes','is_fraud'] = '1'\n",
        "merged[\"is_fraud\"] = pd.to_numeric(merged[\"is_fraud\"], errors=\"coerce\").astype(\"Int64\")\n",
        "\n",
        "merged = add_missing_flags(merged, [\"is_fraud\"])\n",
        "\n",
        "#merged.to_csv(\"merged.csv\", index=False)\n",
        "\n",
        "# 先刪除不需要的DataFrame以節省記憶體\n",
        "del transactions_df, users_df, cards_df, train_fraud_labels_df, cols_to_encode, dummies_cards, dummies_use, dummies_gender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "del mapping_df, c_missing_zip, c_mexico_zip, df_small, lookup_dict\n",
        "backup_merged = merged.copy()\n",
        "#merged = backup_merged.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03_EDA_Exploratory-Data-Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 03-1_資料型態"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "merged資料：8914963x37"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 03-2_資料統計指標"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged.describe(include='all')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 03-3_類別型資料frequency barchart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "cat_cols = merged.select_dtypes(include=[\"category\"]).columns\n",
        "\n",
        "n_rows, n_cols = 4, 2\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 50))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, col in enumerate(cat_cols):\n",
        "    ax = axes[i]\n",
        "    sns.countplot(data=merged, x=col, order=merged[col].value_counts().index, ax=ax)\n",
        "    ax.set_title(f\"Bar chart of {col}\")\n",
        "    ax.set_xlabel(col)\n",
        "    ax.set_ylabel(\"Count\")\n",
        "    if col == \"errors\":\n",
        "        ax.tick_params(axis='x', rotation=90)  # X軸標籤旋轉\n",
        "    else:\n",
        "        ax.tick_params(axis='x', rotation=0)  # X軸標籤旋轉\n",
        "    \n",
        "    # 在長條圖上加數字\n",
        "    for p in ax.patches:\n",
        "        height = p.get_height()\n",
        "        ax.text(x=p.get_x() + p.get_width()/2,\n",
        "                y=height + 0.05,\n",
        "                s=int(height),\n",
        "                ha='center')\n",
        "\n",
        "# 移除多餘空白子圖\n",
        "for j in range(i+1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 03-4_數值型資料histogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 設定 subplot 格式\n",
        "n_cols = 4   # 每列放4張圖\n",
        "n_rows = 6   # 每行放6列 (共 4x6=24)\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(20,15))  # 調整大小\n",
        "axes = axes.flatten()  # 攤平成一維方便迭代\n",
        "num_cols = merged.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "for i, col in enumerate(num_cols):\n",
        "    sns.histplot(data=merged, x=col, bins=30, kde=True, ax=axes[i])\n",
        "    axes[i].set_title(col)\n",
        "\n",
        "# 把多餘的 subplot 關掉（避免空白框）\n",
        "for j in range(i+1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 03-5_類別型資料box plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 抓出數值型欄位\n",
        "num_cols = merged.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# 建立 3x8 subplot\n",
        "fig, axes = plt.subplots(8, 3, figsize=(30, 50))  # 依照需求調整 figsize\n",
        "axes = axes.flatten()  # 攤平成一維 array，方便迴圈\n",
        "\n",
        "# 逐一畫圖\n",
        "for i, col in enumerate(num_cols):\n",
        "    sns.boxplot(y=merged[col], ax=axes[i])  # 每個 subplot 畫一個 boxplot\n",
        "    axes[i].set_title(col, fontsize=10)\n",
        "\n",
        "# 如果欄位數小於 3x8，隱藏多餘的子圖\n",
        "for j in range(len(num_cols), len(axes)):\n",
        "    axes[j].axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 03-6_數值型資料pair wise scatterplot(畫不出來？)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_cols = merged.select_dtypes(include=['int64', 'float64']).columns\n",
        "sns.pairplot(merged[num_cols])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 03-7_針對時間轉換資料"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mask = (merged[\"date\"] >= \"2018-01-01 00:00:00\") & (merged[\"date\"] <= \"2019-10-31 23:59:59\")\n",
        "merged = merged[mask]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged.drop(columns=[\"weekday\",\"hour\",\"is_fraud_missing_flag\"], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged[\"is_fraud\"]=merged[\"is_fraud\"].astype(\"int64\")\n",
        "target = 'is_fraud'  # 假設這是目標\n",
        "num_cols = merged.select_dtypes(include=['int64','float64']).columns.drop(target)\n",
        "\n",
        "for col in num_cols:\n",
        "    plt.figure(figsize=(5,4))\n",
        "    plt.scatter(merged[col], merged[target], alpha=0.3)  # alpha降低透明度，避免太擠\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel(target)\n",
        "    plt.title(f\"{target} vs {col}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 03-8_其他觀察 詐騙與否跟時間的關係"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 確保 date 是 datetime 格式\n",
        "merged[\"date\"] = pd.to_datetime(merged[\"date\"])\n",
        "\n",
        "# 按天統計詐騙事件數\n",
        "fraud_per_day = merged.groupby(merged[\"date\"].dt.date)[\"is_fraud\"].sum()\n",
        "\n",
        "# 畫折線圖\n",
        "plt.figure(figsize=(12,5))\n",
        "fraud_per_day.plot(kind=\"line\", marker=\"o\")\n",
        "plt.title(\"Daily Fraud Counts 日期 vs 詐騙次數\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Number of Frauds\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# 按小時\n",
        "merged[\"hour\"] = merged[\"date\"].dt.hour\n",
        "hourly_fraud = merged.groupby(\"hour\")[\"is_fraud\"].sum()\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "hourly_fraud.plot(kind=\"bar\")\n",
        "plt.title(\"Fraud Counts by Hour of Day\")\n",
        "plt.xlabel(\"Hour\")\n",
        "plt.ylabel(\"Number of Frauds\")\n",
        "plt.show()\n",
        "\n",
        "# 按星期幾\n",
        "merged[\"weekday\"] = merged[\"date\"].dt.day_name()\n",
        "weekday_fraud = merged.groupby(\"weekday\")[\"is_fraud\"].sum().reindex(\n",
        "    [\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"]\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "weekday_fraud.plot(kind=\"bar\")\n",
        "plt.title(\"Fraud Counts by Weekday\")\n",
        "plt.ylabel(\"Number of Frauds\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 想確認原始交易分布與詐騙無關\n",
        "# 取出小時\n",
        "merged[\"hour\"] = merged[\"date\"].dt.hour\n",
        "\n",
        "# 按小時計算交易數\n",
        "transactions_per_hour = merged[\"hour\"].value_counts().sort_index()\n",
        "\n",
        "# 畫長條圖\n",
        "plt.figure(figsize=(12,5))\n",
        "transactions_per_hour.plot(kind=\"bar\")\n",
        "plt.title(\"Transaction Distribution by Hour of Day\")\n",
        "plt.xlabel(\"Hour of Day\")\n",
        "plt.ylabel(\"Number of Transactions\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 03-9_correlation and heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "numeric_df = merged.select_dtypes(include=['int64', 'float64'])\n",
        "corr = numeric_df.corr()\n",
        "print(corr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,20))\n",
        "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", center=0)\n",
        "plt.title(\"Correlation Heatmap of Numeric Variables\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "# --- 原始資料 correlation ---\n",
        "corr_raw = numeric_df.corr()\n",
        "\n",
        "# --- 標準化後 correlation ---\n",
        "scaler = StandardScaler()\n",
        "num_scaled = scaler.fit_transform(numeric_df)   # 轉換成 Numpy array\n",
        "num_df_scaled = pd.DataFrame(num_scaled, columns=numeric_df.columns)\n",
        "corr_scaled = num_df_scaled.corr()\n",
        "\n",
        "# --- 繪圖 (上下對照) ---\n",
        "fig, axes = plt.subplots(2, 1, figsize=(12, 14))\n",
        "\n",
        "sns.heatmap(corr_raw, annot=True, fmt=\".2f\", cmap=\"coolwarm\", center=0, ax=axes[0])\n",
        "axes[0].set_title(\"Correlation Heatmap (Raw Data)\")\n",
        "\n",
        "sns.heatmap(corr_scaled, annot=True, fmt=\".2f\", cmap=\"coolwarm\", center=0, ax=axes[1])\n",
        "axes[1].set_title(\"Correlation Heatmap (Standardized Data)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 03-99_categoracal 轉 dummy分析_(不用執行)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "WVTi5S6XrUyN",
        "outputId": "7c85d826-19f0-4213-a5de-613565d7244e"
      },
      "outputs": [],
      "source": [
        "info_df = pd.DataFrame({\n",
        "    \"column\": merged.columns,\n",
        "    \"dtype\": merged.dtypes.astype(str)\n",
        "})\n",
        "info_df.to_csv(\"info.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 04_Benchmark model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 04-1_資料進行變數轉換以求模型配飾更佳表現"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "##有出事再趕快回復原狀\n",
        "#merged = backup_merged.copy()\n",
        "\n",
        "\n",
        "merged = merged[merged[\"zip\"].isin([20000])]\n",
        "#mask = (merged[\"date\"] >= \"2018-01-01 00:00:00\") & (merged[\"date\"] <= \"2019-10-31 23:59:59\")\n",
        "#merged = merged[mask]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "#merged[[\"card_id\",\"card_number\"]]\n",
        "import numpy as np\n",
        "from scipy import stats \n",
        "\n",
        "# === (1) log轉換 ===\n",
        "merged['amount'] = np.where(merged['amount'] < 0, 0, merged['amount'])  # 負數變 0\n",
        "merged['amount'] = np.log(merged['amount'] + 1)  \n",
        "\n",
        "# === (3) 平方根轉換 ===\n",
        "#merged['credit_limit']=np.sqrt(merged['credit_limit'])\n",
        "merged['total_debt']=np.sqrt(merged['total_debt'])\n",
        "\n",
        "# === (3) 立方根轉換 ===\n",
        "merged['credit_limit']=np.cbrt(merged['credit_limit'])\n",
        "merged['yearly_income']=np.cbrt(merged['yearly_income'])\n",
        "merged['per_capita_income']=np.cbrt(merged['per_capita_income'])\n",
        "\n",
        "## Box-Cox Transformation\n",
        "###merged['yearly_income'], fitted_lambda = stats.boxcox(merged['yearly_income'])\n",
        "\n",
        "# === (5) Yeo–Johnson 轉換（可處理負值） ===\n",
        "###merged['per_capita_income'], lambdaValue =stats.yeojohnson(merged['per_capita_income'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 單一變數測試用，查看哪種轉換較佳\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "\n",
        "# === 統一設定要分析的欄位名稱 ===\n",
        "col = \"total_debt\"\n",
        "\n",
        "# === 準備資料（不動原始 merged） ===\n",
        "data = merged[col].copy()\n",
        "\n",
        "# === 建立結果容器 ===\n",
        "results = []\n",
        "transformations = []  # 這個用來存放轉換後的資料，用來畫圖\n",
        "\n",
        "# === (0) 原始資料 ===\n",
        "orig_skew = stats.skew(data, bias=False, nan_policy='omit')\n",
        "orig_kurt = stats.kurtosis(data, bias=False, nan_policy='omit')\n",
        "results.append(['Original', orig_skew, orig_kurt])\n",
        "transformations.append(('Original', data))\n",
        "\n",
        "# === (1) Log 轉換（保留你的原始邏輯）===\n",
        "log_base = np.where(data < 0, 0, data)\n",
        "log_trans = np.log(log_base + 1)\n",
        "results.append([\n",
        "    'Log(x+1, negatives→0)',\n",
        "    stats.skew(log_trans, bias=False, nan_policy='omit'),\n",
        "    stats.kurtosis(log_trans, bias=False, nan_policy='omit')\n",
        "])\n",
        "transformations.append(('Log(x+1, negatives→0)', log_trans))\n",
        "\n",
        "# === (2) 平方根轉換 ===\n",
        "sqrt_base = np.where(data < 0, 0, data)\n",
        "sqrt_trans = np.sqrt(sqrt_base)\n",
        "results.append([\n",
        "    'Square root (negatives→0)',\n",
        "    stats.skew(sqrt_trans, bias=False, nan_policy='omit'),\n",
        "    stats.kurtosis(sqrt_trans, bias=False, nan_policy='omit')\n",
        "])\n",
        "transformations.append(('Square root (negatives→0)', sqrt_trans))\n",
        "\n",
        "# === (3) 立方根轉換 ===\n",
        "cube_trans = np.cbrt(data)  # 可直接處理負值\n",
        "results.append([\n",
        "    'Cube root',\n",
        "    stats.skew(cube_trans, bias=False, nan_policy='omit'),\n",
        "    stats.kurtosis(cube_trans, bias=False, nan_policy='omit')\n",
        "])\n",
        "transformations.append(('Cube root', cube_trans))\n",
        "\n",
        "# === (4) Box–Cox 轉換（需全為正值） ===\n",
        "boxcox_base = np.where(data <= 0, np.nan, data)\n",
        "if np.all(np.isfinite(boxcox_base)) and np.nanmin(boxcox_base) > 0:\n",
        "    boxcox_trans, lmbda = stats.boxcox(boxcox_base)\n",
        "    results.append([\n",
        "        f'Box–Cox (λ={lmbda:.3f})',\n",
        "        stats.skew(boxcox_trans, bias=False, nan_policy='omit'),\n",
        "        stats.kurtosis(boxcox_trans, bias=False, nan_policy='omit')\n",
        "    ])\n",
        "    transformations.append((f'Box–Cox (λ={lmbda:.3f})', boxcox_trans))\n",
        "else:\n",
        "    results.append(['Box–Cox (skip: nonpositive data)', np.nan, np.nan])\n",
        "\n",
        "# === (5) Yeo–Johnson 轉換（可處理負值） ===\n",
        "yj_trans, lmbda_yj = stats.yeojohnson(data)\n",
        "results.append([\n",
        "    f'Yeo–Johnson (λ={lmbda_yj:.3f})',\n",
        "    stats.skew(yj_trans, bias=False, nan_policy='omit'),\n",
        "    stats.kurtosis(yj_trans, bias=False, nan_policy='omit')\n",
        "])\n",
        "transformations.append((f'Yeo–Johnson (λ={lmbda_yj:.3f})', yj_trans))\n",
        "\n",
        "# === 結果表格 ===\n",
        "df_results = pd.DataFrame(results, columns=['Transformation', 'Skewness', 'Kurtosis'])\n",
        "print(df_results)\n",
        "\n",
        "# === (A) 畫出所有直方圖 ===\n",
        "n = len(transformations)\n",
        "fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(8, 3*n))\n",
        "plt.subplots_adjust(hspace=0.4)\n",
        "\n",
        "for i, (title, series) in enumerate(transformations):\n",
        "    ax = axes[i]\n",
        "    series = pd.Series(series).dropna()\n",
        "    ax.hist(series, bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "    ax.set_title(f\"{title}\", fontsize=12)\n",
        "    ax.set_xlabel(col)\n",
        "    ax.set_ylabel(\"Frequency\")\n",
        "    ax.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "## 參數調整參考 \n",
        "#fig, axes = plt.subplots(1, 2)\n",
        "#axes[0].hist(data, bins=100)\n",
        "#axes[1].hist(yj_trans, bins=100)\n",
        "\n",
        "## age都不建議轉換以保留參數可解釋性"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 04-2_分割訓練集及測試集"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "is_fraud\n",
            "0    7121379\n",
            "1      10591\n",
            "Name: count, dtype: Int64\n",
            "is_fraud\n",
            "0    1780252\n",
            "1       2741\n",
            "Name: count, dtype: Int64\n"
          ]
        }
      ],
      "source": [
        "#選取數值型變數\n",
        "num_cols = merged.select_dtypes(include=['int64', 'float64','uint8']).columns\n",
        "df=merged[num_cols]\n",
        "#dropna\n",
        "df_cleaned = df.dropna()\n",
        "del df\n",
        "#避免共線性\n",
        "df_cleaned.drop(columns=[\"is_fraud_missing_flag\",\"card_type_Debit (Prepaid)\", \"card_brand_Discover\", \"use_chip_Online Transaction\"], inplace=True)\n",
        "#分割訓練集及測試集\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_df, test_df = train_test_split(df_cleaned, test_size=0.2, random_state=888)\n",
        "\n",
        "#確認資料分布情形\n",
        "del df_cleaned, merged\n",
        "print(train_df['is_fraud'].value_counts(normalize=False))\n",
        "print(test_df['is_fraud'].value_counts(normalize=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 04-3(a)_Assumption:Linearity of Logit check\n",
        "\n",
        "📊 解讀圖形\n",
        "若點呈現大致直線 → 該變數與 logit 關係線性 ✅\n",
        "若呈現彎曲（U 型、凹凸） → 該變數不線性 ❌\n",
        "→ 建議對該變數取 log、平方根、或切分分箱再使用"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "檢測的連續變數共有 19 個：['transaction_id', 'client_id_x', 'card_id', 'amount', 'merchant_id', 'mcc_code', 'current_age', 'retirement_age', 'latitude', 'longitude', 'per_capita_income', 'yearly_income', 'total_debt', 'credit_score', 'num_credit_cards', 'card_number', 'cvv', 'num_cards_issued', 'credit_limit']\n",
            "\n",
            "\n",
            "=== Box–Tidwell 檢驗結果 ===\n",
            "             Variable        p_value  \\\n",
            "0      transaction_id  5.286873e-255   \n",
            "5            mcc_code   9.834315e-62   \n",
            "18       credit_limit   3.264931e-35   \n",
            "10  per_capita_income   1.176746e-26   \n",
            "1         client_id_x   1.752708e-12   \n",
            "8            latitude   1.257552e-09   \n",
            "12         total_debt   9.902024e-06   \n",
            "3              amount   6.020389e-04   \n",
            "16                cvv   1.010362e-03   \n",
            "4         merchant_id   1.537642e-03   \n",
            "13       credit_score   2.932186e-03   \n",
            "11      yearly_income   3.846137e-03   \n",
            "15        card_number   1.165131e-02   \n",
            "6         current_age   2.308379e-01   \n",
            "7      retirement_age   3.714964e-01   \n",
            "17   num_cards_issued   4.814017e-01   \n",
            "14   num_credit_cards   7.284627e-01   \n",
            "2             card_id   9.105358e-01   \n",
            "9           longitude            NaN   \n",
            "\n",
            "                                            Linearity  \n",
            "0                                               ❌ 不線性  \n",
            "5                                               ❌ 不線性  \n",
            "18                                              ❌ 不線性  \n",
            "10                                              ❌ 不線性  \n",
            "1                                               ❌ 不線性  \n",
            "8                                               ❌ 不線性  \n",
            "12                                              ❌ 不線性  \n",
            "3                                            ✅ 線性假設成立  \n",
            "16                                           ✅ 線性假設成立  \n",
            "4                                            ✅ 線性假設成立  \n",
            "13                                           ✅ 線性假設成立  \n",
            "11                                           ✅ 線性假設成立  \n",
            "15                                           ✅ 線性假設成立  \n",
            "6                                            ✅ 線性假設成立  \n",
            "7                                            ✅ 線性假設成立  \n",
            "17                                           ✅ 線性假設成立  \n",
            "14                                           ✅ 線性假設成立  \n",
            "2                                            ✅ 線性假設成立  \n",
            "9   ⚠️ 模型無法收斂 (zero-size array to reduction operat...  \n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#sample_df, _ = train_test_split(\n",
        "#    train_df,\n",
        "#    train_size=1,\n",
        "#    stratify=train_df['is_fraud'],\n",
        "#    random_state=88\n",
        "#)\n",
        "\n",
        "## Box–Tidwell 檢驗\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "\n",
        "def box_tidwell_test(df, target='is_fraud'):\n",
        "    \"\"\"\n",
        "    自動對連續變數執行 Box–Tidwell 檢驗。\n",
        "    適用於 Logistic Regression 的 linearity of logit 假設檢查。\n",
        "    \"\"\"\n",
        "\n",
        "    # 1️⃣ 篩出連續變數（排除二元或類別欄位）\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    numeric_cols = [c for c in numeric_cols if c != target]\n",
        "    # 排除只有 0/1 的欄位（常見於 flag）\n",
        "    continuous_vars = [c for c in numeric_cols if df[c].nunique() > 2]\n",
        "\n",
        "    print(f\"檢測的連續變數共有 {len(continuous_vars)} 個：{continuous_vars}\\n\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for var in continuous_vars:\n",
        "        temp = df[[target, var]].copy()\n",
        "        # 若有負數或 0，改為 NaN（Box–Tidwell 要求 X > 0）\n",
        "        temp[var] = np.where(temp[var] <= 0, np.nan, temp[var])\n",
        "        temp = temp.dropna()\n",
        "\n",
        "        # 建立交互項 Xi * log(Xi)\n",
        "        temp[f'{var}_log'] = np.log(temp[var])\n",
        "        temp[f'{var}_interaction'] = temp[var] * temp[f'{var}_log']\n",
        "\n",
        "        # Logistic Regression 模型\n",
        "        X = sm.add_constant(temp[[var, f'{var}_interaction']])\n",
        "        y = temp[target]\n",
        "\n",
        "        try:\n",
        "            model = sm.Logit(y, X).fit(disp=False)\n",
        "            p_value = model.pvalues.get(f'{var}_interaction', np.nan)\n",
        "            results.append({\n",
        "                'Variable': var,\n",
        "                'p_value': p_value,\n",
        "                'Linearity': '✅ 線性假設成立' if p_value >= 0.0001 else '❌ 不線性'\n",
        "            })\n",
        "        except Exception as e:\n",
        "            results.append({\n",
        "                'Variable': var,\n",
        "                'p_value': np.nan,\n",
        "                'Linearity': f'⚠️ 模型無法收斂 ({e})'\n",
        "            })\n",
        "\n",
        "    results_df = pd.DataFrame(results).sort_values('p_value', na_position='last')\n",
        "    return results_df\n",
        "\n",
        "# ✅ 執行檢驗\n",
        "#train_df[\"longitude\"]=train_df[\"longitude\"]*(-1)\n",
        "bt_results = box_tidwell_test(train_df, target='is_fraud')\n",
        "\n",
        "# 查看結果\n",
        "print(\"\\n=== Box–Tidwell 檢驗結果 ===\")\n",
        "print(bt_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 針對longitude模型無法收斂的變數重新檢查\n",
        "\n",
        "train_df[\"longitude\"]=train_df[\"longitude\"]*(-1)\n",
        "bt_results = box_tidwell_test(train_df, target='is_fraud')\n",
        "# 查看結果\n",
        "print(\"\\n=== Box–Tidwell 檢驗結果(修正longitude) ===\")\n",
        "print(bt_results)\n",
        "train_df[\"longitude\"]=train_df[\"longitude\"]*(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "#BY Linearity of Logit check drop 不符合假設的變數 \n",
        "nonlinear_vars =['mcc_code','transaction_id',\"per_capita_income\",\"client_id_x\",\"latitude\"]\n",
        "\n",
        "#bt_results[bt_results[\"Linearity\"] == '❌ 不線性']\n",
        "#nonlinear_vars = bt_results[bt_results[\"Linearity\"] == '❌ 不線性'][\"Variable\"].tolist()\n",
        "train_df.drop(columns=nonlinear_vars, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 04-3(b1)_(略04-3(a))Assumption:Avoid Multicollinearity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/andyw.p.chen/Documents/Project/2025_Fraud-Detection-project/virtual/lib/python3.13/site-packages/statsmodels/regression/linear_model.py:1782: RuntimeWarning: divide by zero encountered in scalar divide\n",
            "  return 1 - self.ssr/self.centered_tss\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "def calculate_vif(df):\n",
        "    # 1. 保留數值欄位\n",
        "    df_num = df.select_dtypes(include=[np.number]).copy()\n",
        "\n",
        "    # 2. 強制轉成 float64，避免 Int64 / uint8 / object 問題\n",
        "    df_num = df_num.astype(np.float64)\n",
        "\n",
        "    # 3. 檢查 inf / NaN\n",
        "    if not np.isfinite(df_num.values).all():\n",
        "        raise ValueError(\"Data contains NaN or infinite values, cannot compute VIF.\")\n",
        "\n",
        "    # 4. 加上截距\n",
        "    X = sm.add_constant(df_num)\n",
        "\n",
        "    # 5. 計算 VIF\n",
        "    vif = pd.DataFrame()\n",
        "    vif[\"features\"] = X.columns\n",
        "    vif[\"VIF Factor\"] = [variance_inflation_factor(X.values, i) \n",
        "                         for i in range(X.shape[1])]\n",
        "\n",
        "    return vif\n",
        "\n",
        "# 使用範例\n",
        "vif_result = calculate_vif(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(vif_result.sort_values(by=\"VIF Factor\", ascending=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#train_df.drop(columns=[\"card_number\"], inplace=True)\n",
        "vif_result = calculate_vif(train_df)\n",
        "print(vif_result.sort_values(by=\"VIF Factor\", ascending=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "##處理共線性\n",
        "train_df.drop(columns=[\"per_capita_income\"], inplace=True)\n",
        "train_df.drop(columns=[\"use_chip_Chip Transaction\",\"merchant_state_missing_flag\",\"zip_missing_flag\"], inplace=True)           \n",
        "train_df.drop(columns=[\"card_brand_Visa\" ,\"card_brand_Amex\",\"card_type_Credit\"], inplace=True)\n",
        "#再重跑一次VIF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(calculate_vif(train_df).sort_values(by=\"VIF Factor\", ascending=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df.drop(columns=[\"zip\"], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 04-3(b2)_Assumption:Avoid Multicollinearity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                      features  VIF Factor\n",
            "0                        const  721.155518\n",
            "11                  total_debt    1.672071\n",
            "17                credit_limit    1.613637\n",
            "7                  current_age    1.587193\n",
            "10               yearly_income    1.526840\n",
            "18             card_type_Debit    1.455911\n",
            "13            num_credit_cards    1.350577\n",
            "12                credit_score    1.201036\n",
            "5   use_chip_Swipe Transaction    1.165804\n",
            "20                has_chip_YES    1.139862\n",
            "19       card_brand_Mastercard    1.121520\n",
            "6                     is_fraud    1.073351\n",
            "9                    longitude    1.047833\n",
            "16            num_cards_issued    1.037242\n",
            "8               retirement_age    1.036495\n",
            "2                       amount    1.026405\n",
            "14                 gender_Male    1.019537\n",
            "1                      card_id    1.015288\n",
            "15                         cvv    1.010456\n",
            "3                  merchant_id    1.006617\n",
            "4          errors_missing_flag    1.005383\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "def calculate_vif(df):\n",
        "    # 1. 保留數值欄位\n",
        "    df_num = df.select_dtypes(include=[np.number]).copy()\n",
        "\n",
        "    # 2. 強制轉成 float64，避免 Int64 / uint8 / object 問題\n",
        "    df_num = df_num.astype(np.float64)\n",
        "\n",
        "    # 3. 檢查 inf / NaN\n",
        "    if not np.isfinite(df_num.values).all():\n",
        "        raise ValueError(\"Data contains NaN or infinite values, cannot compute VIF.\")\n",
        "\n",
        "    # 4. 加上截距\n",
        "    X = sm.add_constant(df_num)\n",
        "\n",
        "    # 5. 計算 VIF\n",
        "    vif = pd.DataFrame()\n",
        "    vif[\"features\"] = X.columns\n",
        "    vif[\"VIF Factor\"] = [variance_inflation_factor(X.values, i) \n",
        "                         for i in range(X.shape[1])]\n",
        "\n",
        "    return vif\n",
        "\n",
        "# 使用範例\n",
        "vif_result = calculate_vif(train_df)\n",
        "print(vif_result.sort_values(by=\"VIF Factor\", ascending=False))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "#train_df.drop(columns=[\"card_number\" ,\"card_brand_Amex\"], inplace=True)\n",
        "#train_df.drop(columns=[\"use_chip_Chip Transaction\",\"merchant_state_missing_flag\",\"zip_missing_flag\"], inplace=True)    \n",
        "train_df.drop(columns=[\"zip\",\"card_type_Credit\",\"card_brand_Visa\"], inplace=True)    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "# assume train_df is your dataframe and \"is_fraud\" is the dependent variable\n",
        "y = train_df[\"is_fraud\"]\n",
        "\n",
        "# exclude the dependent variable itself\n",
        "independent_vars = train_df.columns.drop(\"is_fraud\")\n",
        "\n",
        "results = []\n",
        "\n",
        "for var in independent_vars:\n",
        "    X = sm.add_constant(train_df[var])  # add intercept\n",
        "    model = sm.Logit(y, X)\n",
        "    try:\n",
        "        result = model.fit(disp=False)\n",
        "        coef = result.params[var]\n",
        "        pval = np.around(result.pvalues[var], 4)\n",
        "        results.append({\"variable\": var, \"coefficient\": coef, \"p_value\": pval})\n",
        "    except Exception as e:\n",
        "        results.append({\"variable\": var, \"coefficient\": None, \"p_value\": None})\n",
        "        print(f\"Skipped {var} due to error: {e}\")\n",
        "\n",
        "# convert to dataframe\n",
        "summary_df = pd.DataFrame(results)\n",
        "\n",
        "# optional: sort by p_value\n",
        "summary_df = summary_df.sort_values(\"p_value\", ascending=True).reset_index(drop=True)\n",
        "\n",
        "print(summary_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 04-4(a)_Forward only model fitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Stepwise Selection all variables\n",
        "\n",
        "def stepwise_logit(train_df, target_col=\"is_fraud\", entry_threshold=0.05):\n",
        "    # ✅ 確保索引對齊\n",
        "    train_df = train_df.reset_index(drop=True)\n",
        "    y = train_df[target_col].reset_index(drop=True)\n",
        "    \n",
        "    candidate_vars = list(train_df.columns.drop(target_col))\n",
        "    included_vars = []\n",
        "    step_results = []\n",
        "    \n",
        "    # Base model (only intercept)\n",
        "    X_base = sm.add_constant(pd.DataFrame({\"intercept\": [1]*len(train_df)}))\n",
        "    base_model = sm.Logit(y, X_base).fit(disp=False)\n",
        "    base_ll = -2 * base_model.llf\n",
        "    step_results.append({\"Step\": 0, \"Variable Entered\": None, \"-2 Log Likelihood\": base_ll})\n",
        "    \n",
        "    print(f\"Step 0: Base model estimated. -2LL = {base_ll:.3f}\")\n",
        "    \n",
        "    step = 1\n",
        "    while True:\n",
        "        best_pval = 1\n",
        "        best_var = None\n",
        "        best_model = None\n",
        "        \n",
        "        for var in candidate_vars:\n",
        "            try:\n",
        "                X_temp = sm.add_constant(train_df[included_vars + [var]])\n",
        "                model_temp = sm.Logit(y, X_temp).fit(disp=False)\n",
        "                pval = model_temp.pvalues[var]\n",
        "                \n",
        "                if pval < best_pval:\n",
        "                    best_pval = pval\n",
        "                    best_var = var\n",
        "                    best_model = model_temp\n",
        "                    \n",
        "            except Exception:\n",
        "                continue\n",
        "        \n",
        "        if best_var is None or best_pval > entry_threshold:\n",
        "            print(\"\\n✅ No more variables meet the entry threshold. Stepwise selection finished.\")\n",
        "            break\n",
        "        \n",
        "        included_vars.append(best_var)\n",
        "        candidate_vars.remove(best_var)\n",
        "        \n",
        "        ll = -2 * best_model.llf\n",
        "        step_results.append({\"Step\": step, \"Variable Entered\": best_var, \"-2 Log Likelihood\": ll})\n",
        "        \n",
        "        print(f\"Step {step}: Added {best_var}, p = {best_pval:.4f}, -2LL = {ll:.3f}\")\n",
        "        step += 1\n",
        "    \n",
        "    print(\"\\nFinal model summary:\")\n",
        "    print(best_model.summary())\n",
        "    \n",
        "    step_df = pd.DataFrame(step_results)\n",
        "    return step_df, best_model\n",
        "\n",
        "# 🚀 執行\n",
        "step_df, final_model = stepwise_logit(train_df, target_col=\"is_fraud\")\n",
        "step_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 04-4(b)_Forward/Backward v1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 版本1.0 (含k值控制，但0會有錯誤)\n",
        "import statsmodels.api as sm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "def stepwise_logit_with_k_v1(train_df, test_df, dep_var=\"is_fraud\", k=314657018,\n",
        "                                 threshold_in=0.05, threshold_out=0.10):\n",
        "    \"\"\"\n",
        "    Stepwise logistic regression (forward + backward) with flexible k control,\n",
        "    and 3 formatted output tables like table_for_first_step().\n",
        "    \"\"\"\n",
        "\n",
        "    y_train = train_df[dep_var]\n",
        "    X_train = train_df.drop(columns=[dep_var])\n",
        "    y_test = test_df[dep_var]\n",
        "    X_test = test_df.drop(columns=[dep_var])\n",
        "\n",
        "    included = []\n",
        "    step = 0\n",
        "    full_mode = (k == 314657018)\n",
        "    base_model = None\n",
        "\n",
        "    while True:\n",
        "        step += 1\n",
        "        changed = False\n",
        "\n",
        "        # ---------- Forward Step ----------\n",
        "        excluded = list(set(X_train.columns) - set(included))\n",
        "        new_pvals = pd.Series(index=excluded, dtype=float)\n",
        "        for new_var in excluded:\n",
        "            try:\n",
        "                model = sm.Logit(y_train, sm.add_constant(X_train[included + [new_var]])).fit(disp=False)\n",
        "                new_pvals[new_var] = model.pvalues[new_var]\n",
        "            except Exception:\n",
        "                new_pvals[new_var] = np.nan\n",
        "\n",
        "        if new_pvals.empty:\n",
        "            break\n",
        "\n",
        "        best_pval = new_pvals.min()\n",
        "        if best_pval < threshold_in:\n",
        "            best_var = new_pvals.idxmin()\n",
        "            included.append(best_var)\n",
        "            changed = True\n",
        "\n",
        "        # ---------- Backward Step ----------\n",
        "        if included:\n",
        "            model = sm.Logit(y_train, sm.add_constant(X_train[included])).fit(disp=False)\n",
        "            pvalues = model.pvalues.iloc[1:]\n",
        "            worst_pval = pvalues.max()\n",
        "            if worst_pval > threshold_out:\n",
        "                worst_var = pvalues.idxmax()\n",
        "                included.remove(worst_var)\n",
        "                changed = True\n",
        "\n",
        "        # ---------- 結束條件 ----------\n",
        "        if not changed:\n",
        "            break\n",
        "        if not full_mode and len(included) >= k:\n",
        "            break\n",
        "\n",
        "    # ========= Final Model =========\n",
        "    final_model = sm.Logit(y_train, sm.add_constant(X_train[included])).fit(disp=False)\n",
        "    ll_full = final_model.llf\n",
        "    ll_null = sm.Logit(y_train, sm.add_constant(np.ones(len(y_train)))).fit(disp=False).llf\n",
        "\n",
        "    # 1️⃣ Overall Model Fit\n",
        "    ll_diff = -2 * (ll_null - ll_full)\n",
        "    df_diff = len(final_model.params) - 1\n",
        "    p_value = stats.chi2.sf(ll_diff, df_diff)\n",
        "\n",
        "    overall_fit = pd.DataFrame({\n",
        "        \"Measure\": [\n",
        "            \"-2 Log Likelihood (−2LL) value\",\n",
        "            \"Cox and Snell R2\",\n",
        "            \"Nagelkerke R2\",\n",
        "            \"Pseudo R2 (McFadden)\",\n",
        "            \"Hosmer-Lemeshow χ2\"\n",
        "        ],\n",
        "        \"Value\": [\n",
        "            round(-2 * ll_full, 3),\n",
        "            round(1 - np.exp((2 / len(y_train)) * (ll_null - ll_full)), 3),\n",
        "            round((1 - np.exp((2 / len(y_train)) * (ll_null - ll_full))) / (1 - np.exp(2 * ll_null / len(y_train))), 3),\n",
        "            round(1 - (ll_full / ll_null), 3),\n",
        "            round(ll_diff, 3)\n",
        "        ],\n",
        "        \"Change_from_Base\": [\n",
        "            round(-2 * (ll_null - ll_full), 3),\n",
        "            \"\", \"\", \"\", \"\"\n",
        "        ],\n",
        "        \"Change_pvalue\": [\n",
        "            round(p_value, 4),\n",
        "            \"\", \"\", \"\", \"\"\n",
        "        ]\n",
        "    })\n",
        "\n",
        "    # 2️⃣ Variables in the Equation\n",
        "    coef_df = pd.DataFrame({\n",
        "        \"Independent Variable\": final_model.params.index,\n",
        "        \"B\": final_model.params.values,\n",
        "        \"Std. Error\": final_model.bse.values,\n",
        "        \"Wald\": (final_model.params / final_model.bse) ** 2,\n",
        "        \"df\": 1,\n",
        "        \"Sig.\": final_model.pvalues.values,\n",
        "        \"Exp(B)\": np.exp(final_model.params.values)\n",
        "    })\n",
        "    coef_df = coef_df.reset_index(drop=True)\n",
        "\n",
        "    # 3️⃣ Variables Not in the Equation\n",
        "    excluded_vars = [v for v in X_train.columns if v not in included]\n",
        "    not_in_eq = []\n",
        "    for var in excluded_vars:\n",
        "        try:\n",
        "            temp_model = sm.Logit(y_train, sm.add_constant(X_train[included + [var]])).fit(disp=False)\n",
        "            lr_stat = -2 * (final_model.llf - temp_model.llf)\n",
        "            p_val = stats.chi2.sf(lr_stat, 1)\n",
        "            not_in_eq.append({\"Independent Variable\": var,\n",
        "                              \"Score Statistic (LRT)\": round(lr_stat, 3),\n",
        "                              \"Significance\": round(p_val, 4)})\n",
        "        except Exception:\n",
        "            not_in_eq.append({\"Independent Variable\": var,\n",
        "                              \"Score Statistic (LRT)\": None,\n",
        "                              \"Significance\": None})\n",
        "\n",
        "    not_in_eq_df = pd.DataFrame(not_in_eq)\n",
        "\n",
        "    # 額外：Train / Test Accuracy\n",
        "    train_pred = (final_model.predict(sm.add_constant(X_train[included])) > 0.5).astype(int)\n",
        "    test_pred = (final_model.predict(sm.add_constant(X_test[included])) > 0.5).astype(int)\n",
        "    train_acc = (train_pred == y_train).mean()\n",
        "    test_acc = (test_pred == y_test).mean()\n",
        "\n",
        "    print(f\"\\n✅ Stepwise completed with {len(included)} variables: {included}\")\n",
        "    print(f\"Train Accuracy: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "    return overall_fit, coef_df, not_in_eq_df, final_model\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def classification_table(model, df, target_col=\"is_fraud\", cutoff=0.5):\n",
        "    y_true = df[target_col].astype(int)\n",
        "    X = sm.add_constant(df[model.params.index.drop(\"const\")])\n",
        "    y_pred_prob = model.predict(X)\n",
        "    y_pred = (y_pred_prob >= cutoff).astype(int)\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=[1, 0])\n",
        "    TP, FN, FP, TN = cm.ravel()\n",
        "\n",
        "    fraud_total = TP + FN\n",
        "    normal_total = FP + TN\n",
        "\n",
        "    fraud_correct = TP / fraud_total if fraud_total > 0 else 0\n",
        "    normal_correct = TN / normal_total if normal_total > 0 else 0\n",
        "    overall_correct = (TP + TN) / (fraud_total + normal_total)\n",
        "\n",
        "    table = pd.DataFrame({\n",
        "        \"Actual Group\": [\"Fraud (1)\", \"Normal (0)\", \"Total\"],\n",
        "        \"Predicted Fraud (1)\": [TP, FP, TP + FP],\n",
        "        \"Predicted Normal (0)\": [FN, TN, FN + TN],\n",
        "        \"Total\": [fraud_total, normal_total, fraud_total + normal_total],\n",
        "        \"% Correct\": [\n",
        "            round(fraud_correct * 100, 1),\n",
        "            round(normal_correct * 100, 1),\n",
        "            round(overall_correct * 100, 1),\n",
        "        ],\n",
        "    })\n",
        "    return table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 版本1.0用法 k=1\n",
        "overall_fit, coef_df, not_in_eq_df, model1_0 = stepwise_logit_with_k_v1(\n",
        "    train_df, test_df, dep_var=\"is_fraud\", k=0\n",
        ")\n",
        "\n",
        "print(\"=== Overall Model Fit ===\")\n",
        "print(overall_fit)\n",
        "\n",
        "print(\"\\n=== Variables in the Equation ===\")\n",
        "print(coef_df)\n",
        "\n",
        "print(\"\\n=== Variables Not in the Equation ===\")\n",
        "print(not_in_eq_df)\n",
        "\n",
        "print(\"\\n================================================\")\n",
        "print(\"\\n=== Accuracy in Training and Testing dataset ===\")\n",
        "\n",
        "\n",
        "\n",
        "# Step 3. 產出訓練集和測試集的結果表\n",
        "train_table = classification_table(model1_0, train_df, target_col=\"is_fraud\")\n",
        "test_table = classification_table(model1_0, test_df, target_col=\"is_fraud\")\n",
        "\n",
        "print(\"\\n=== Classification Matrix — Training Sample ===\")\n",
        "print(train_table.to_string(index=False))\n",
        "print(\"\\n=== Classification Matrix — Holdout (Test) Sample ===\")\n",
        "print(test_table.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 版本1.0用法 k=2\n",
        "overall_fit, coef_df, not_in_eq_df, model1_2 = stepwise_logit_with_k_v1(\n",
        "    train_df, test_df, dep_var=\"is_fraud\", k=2\n",
        ")\n",
        "\n",
        "print(\"=== Overall Model Fit ===\")\n",
        "print(overall_fit)\n",
        "\n",
        "print(\"\\n=== Variables in the Equation ===\")\n",
        "print(coef_df)\n",
        "\n",
        "print(\"\\n=== Variables Not in the Equation ===\")\n",
        "print(not_in_eq_df)\n",
        "\n",
        "print(\"\\n================================================\")\n",
        "print(\"\\n=== Accuracy in Training and Testing dataset ===\")\n",
        "\n",
        "\n",
        "\n",
        "# Step 3. 產出訓練集和測試集的結果表\n",
        "train_table = classification_table(model1_2, train_df, target_col=\"is_fraud\")\n",
        "test_table = classification_table(model1_2, test_df, target_col=\"is_fraud\")\n",
        "\n",
        "print(\"\\n=== Classification Matrix — Training Sample ===\")\n",
        "print(train_table.to_string(index=False))\n",
        "print(\"\\n=== Classification Matrix — Holdout (Test) Sample ===\")\n",
        "print(test_table.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 版本1.0用法 k=3\n",
        "overall_fit, coef_df, not_in_eq_df, model1_3 = stepwise_logit_with_k_v1(\n",
        "    train_df, test_df, dep_var=\"is_fraud\", k=3\n",
        ")\n",
        "\n",
        "print(\"=== Overall Model Fit ===\")\n",
        "print(overall_fit)\n",
        "\n",
        "print(\"\\n=== Variables in the Equation ===\")\n",
        "print(coef_df)\n",
        "\n",
        "print(\"\\n=== Variables Not in the Equation ===\")\n",
        "print(not_in_eq_df)\n",
        "\n",
        "print(\"\\n================================================\")\n",
        "print(\"\\n=== Accuracy in Training and Testing dataset ===\")\n",
        "\n",
        "\n",
        "\n",
        "# Step 3. 產出訓練集和測試集的結果表\n",
        "train_table = classification_table(model1_3, train_df, target_col=\"is_fraud\")\n",
        "test_table = classification_table(model1_3, test_df, target_col=\"is_fraud\")\n",
        "\n",
        "print(\"\\n=== Classification Matrix — Training Sample ===\")\n",
        "print(train_table.to_string(index=False))\n",
        "print(\"\\n=== Classification Matrix — Holdout (Test) Sample ===\")\n",
        "print(test_table.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 版本1.0用法 k=4\n",
        "overall_fit, coef_df, not_in_eq_df, model1_4 = stepwise_logit_with_k_v1(\n",
        "    train_df, test_df, dep_var=\"is_fraud\", k=4\n",
        ")\n",
        "\n",
        "print(\"=== Overall Model Fit ===\")\n",
        "print(overall_fit)\n",
        "\n",
        "print(\"\\n=== Variables in the Equation ===\")\n",
        "print(coef_df)\n",
        "\n",
        "print(\"\\n=== Variables Not in the Equation ===\")\n",
        "print(not_in_eq_df)\n",
        "\n",
        "print(\"\\n================================================\")\n",
        "print(\"\\n=== Accuracy in Training and Testing dataset ===\")\n",
        "\n",
        "\n",
        "\n",
        "# Step 3. 產出訓練集和測試集的結果表\n",
        "train_table = classification_table(model1_4, train_df, target_col=\"is_fraud\")\n",
        "test_table = classification_table(model1_4, test_df, target_col=\"is_fraud\")\n",
        "\n",
        "print(\"\\n=== Classification Matrix — Training Sample ===\")\n",
        "print(train_table.to_string(index=False))\n",
        "print(\"\\n=== Classification Matrix — Holdout (Test) Sample ===\")\n",
        "print(test_table.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 版本1.0用法 k=1\n",
        "overall_fit, coef_df, not_in_eq_df, model1_all = stepwise_logit_with_k_v1(\n",
        "    train_df, test_df, dep_var=\"is_fraud\", k=314657018\n",
        ")\n",
        "\n",
        "print(\"=== Overall Model Fit ===\")\n",
        "print(overall_fit)\n",
        "\n",
        "print(\"\\n=== Variables in the Equation ===\")\n",
        "print(coef_df)\n",
        "\n",
        "print(\"\\n=== Variables Not in the Equation ===\")\n",
        "print(not_in_eq_df)\n",
        "\n",
        "print(\"\\n================================================\")\n",
        "print(\"\\n=== Accuracy in Training and Testing dataset ===\")\n",
        "\n",
        "\n",
        "\n",
        "# Step 3. 產出訓練集和測試集的結果表\n",
        "train_table = classification_table(model1_all, train_df, target_col=\"is_fraud\")\n",
        "test_table = classification_table(model1_all, test_df, target_col=\"is_fraud\")\n",
        "\n",
        "print(\"\\n=== Classification Matrix — Training Sample ===\")\n",
        "print(train_table.to_string(index=False))\n",
        "print(\"\\n=== Classification Matrix — Holdout (Test) Sample ===\")\n",
        "print(test_table.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 04-4(c)_Forward/Backward v2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 版本2.0: 修正k=0，並加入即時追蹤\n",
        "import statsmodels.api as sm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "def stepwise_logit_with_k_tables(train_df, test_df, dep_var=\"is_fraud\", k=314657018,\n",
        "                                 threshold_in=0.05, threshold_out=0.10, verbose=True):\n",
        "    \"\"\"\n",
        "    Stepwise logistic regression (forward + backward) with flexible k control,\n",
        "    and 3 formatted output tables like table_for_first_step().\n",
        "    \"\"\"\n",
        "\n",
        "    y_train = train_df[dep_var]\n",
        "    X_train = train_df.drop(columns=[dep_var])\n",
        "    y_test = test_df[dep_var]\n",
        "    X_test = test_df.drop(columns=[dep_var])\n",
        "\n",
        "    included = []\n",
        "    step = 0\n",
        "    full_mode = (k == 314657018)\n",
        "\n",
        "    # ========= 🔸 特殊情況：k = 0，只跑 intercept =========\n",
        "    if k == 0:\n",
        "        final_model = sm.Logit(y_train, sm.add_constant(np.ones(len(y_train)))).fit(disp=False)\n",
        "        ll_full = final_model.llf\n",
        "        ll_null = ll_full\n",
        "\n",
        "        overall_fit = pd.DataFrame({\n",
        "            \"Measure\": [\"-2 Log Likelihood (−2LL) value\"],\n",
        "            \"Value\": [round(-2 * ll_full, 3)],\n",
        "            \"Change_from_Base\": [\"\"],\n",
        "            \"Change_pvalue\": [\"\"]\n",
        "        })\n",
        "\n",
        "        coef_df = pd.DataFrame({\n",
        "            \"Independent Variable\": [\"const\"],\n",
        "            \"B\": final_model.params.values,\n",
        "            \"Std. Error\": final_model.bse.values,\n",
        "            \"Wald\": [np.nan],\n",
        "            \"df\": [1],\n",
        "            \"Sig.\": [\"\"],\n",
        "            \"Exp(B)\": np.exp(final_model.params.values)\n",
        "        })\n",
        "\n",
        "        not_in_eq_df = pd.DataFrame({\n",
        "            \"Independent Variable\": X_train.columns,\n",
        "            \"Score Statistic (LRT)\": [None]*len(X_train.columns),\n",
        "            \"Significance\": [None]*len(X_train.columns)\n",
        "        })\n",
        "\n",
        "        print(f\"\\n✅ Stepwise completed with 0 variables (Intercept only).\")\n",
        "        return overall_fit, coef_df, not_in_eq_df, final_model\n",
        "    # ======================================================\n",
        "\n",
        "    # ---------- Regular Stepwise ----------\n",
        "    while True:\n",
        "        step += 1\n",
        "        changed = False\n",
        "\n",
        "        # ---------- Forward Step ----------\n",
        "        excluded = list(set(X_train.columns) - set(included))\n",
        "        new_pvals = pd.Series(index=excluded, dtype=float)\n",
        "        for new_var in excluded:\n",
        "            try:\n",
        "                model = sm.Logit(y_train, sm.add_constant(X_train[included + [new_var]])).fit(disp=False)\n",
        "                new_pvals[new_var] = model.pvalues[new_var]\n",
        "            except Exception:\n",
        "                new_pvals[new_var] = np.nan\n",
        "\n",
        "        if new_pvals.empty:\n",
        "            break\n",
        "\n",
        "        best_pval = new_pvals.min()\n",
        "        if best_pval < threshold_in:\n",
        "            best_var = new_pvals.idxmin()\n",
        "            included.append(best_var)\n",
        "            changed = True\n",
        "            if verbose:\n",
        "                print(f\"🟢 Step {step}: Forward — added variable: {best_var} (p={best_pval:.4g})\")\n",
        "\n",
        "        # ---------- Backward Step ----------\n",
        "        if included:\n",
        "            model = sm.Logit(y_train, sm.add_constant(X_train[included])).fit(disp=False)\n",
        "            pvalues = model.pvalues.iloc[1:]  # skip intercept\n",
        "            worst_pval = pvalues.max()\n",
        "            if worst_pval > threshold_out:\n",
        "                worst_var = pvalues.idxmax()\n",
        "                included.remove(worst_var)\n",
        "                changed = True\n",
        "                if verbose:\n",
        "                    print(f\"🔴 Step {step}: Backward — removed variable: {worst_var} (p={worst_pval:.4g})\")\n",
        "\n",
        "        # ---------- 結束條件 ----------\n",
        "        if not changed:\n",
        "            if verbose:\n",
        "                print(f\"⚪ Step {step}: No change — stopping iteration.\")\n",
        "            break\n",
        "        if not full_mode and len(included) >= k:\n",
        "            if verbose:\n",
        "                print(f\"🟡 Reached k={k}, stopping after {len(included)} variables.\")\n",
        "            break\n",
        "\n",
        "    # ========= Final Model =========\n",
        "    final_model = sm.Logit(y_train, sm.add_constant(X_train[included])).fit(disp=False)\n",
        "    ll_full = final_model.llf\n",
        "    ll_null = sm.Logit(y_train, sm.add_constant(np.ones(len(y_train)))).fit(disp=False).llf\n",
        "\n",
        "    # 1️⃣ Overall Model Fit\n",
        "    ll_diff = -2 * (ll_null - ll_full)\n",
        "    df_diff = len(final_model.params) - 1\n",
        "    p_value = stats.chi2.sf(ll_diff, df_diff)\n",
        "\n",
        "    overall_fit = pd.DataFrame({\n",
        "        \"Measure\": [\n",
        "            \"-2 Log Likelihood (−2LL) value\",\n",
        "            \"Cox and Snell R2\",\n",
        "            \"Nagelkerke R2\",\n",
        "            \"Pseudo R2 (McFadden)\",\n",
        "            \"Hosmer-Lemeshow χ2\"\n",
        "        ],\n",
        "        \"Value\": [\n",
        "            round(-2 * ll_full, 3),\n",
        "            round(1 - np.exp((2 / len(y_train)) * (ll_null - ll_full)), 3),\n",
        "            round((1 - np.exp((2 / len(y_train)) * (ll_null - ll_full))) / (1 - np.exp(2 * ll_null / len(y_train))), 3),\n",
        "            round(1 - (ll_full / ll_null), 3),\n",
        "            round(ll_diff, 3)\n",
        "        ],\n",
        "        \"Change_from_Base\": [\n",
        "            round(-2 * (ll_null - ll_full), 3),\n",
        "            \"\", \"\", \"\", \"\"\n",
        "        ],\n",
        "        \"Change_pvalue\": [\n",
        "            round(p_value, 4),\n",
        "            \"\", \"\", \"\", \"\"\n",
        "        ]\n",
        "    })\n",
        "\n",
        "    # 2️⃣ Variables in the Equation\n",
        "    coef_df = pd.DataFrame({\n",
        "        \"Independent Variable\": final_model.params.index,\n",
        "        \"B\": final_model.params.values,\n",
        "        \"Std. Error\": final_model.bse.values,\n",
        "        \"Wald\": (final_model.params / final_model.bse) ** 2,\n",
        "        \"df\": 1,\n",
        "        \"Sig.\": final_model.pvalues.values,\n",
        "        \"Exp(B)\": np.exp(final_model.params.values)\n",
        "    })\n",
        "    coef_df = coef_df.reset_index(drop=True)\n",
        "\n",
        "    # 3️⃣ Variables Not in the Equation\n",
        "    excluded_vars = [v for v in X_train.columns if v not in included]\n",
        "    not_in_eq = []\n",
        "    for var in excluded_vars:\n",
        "        try:\n",
        "            temp_model = sm.Logit(y_train, sm.add_constant(X_train[included + [var]])).fit(disp=False)\n",
        "            lr_stat = -2 * (final_model.llf - temp_model.llf)\n",
        "            p_val = stats.chi2.sf(lr_stat, 1)\n",
        "            not_in_eq.append({\"Independent Variable\": var,\n",
        "                              \"Score Statistic (LRT)\": round(lr_stat, 3),\n",
        "                              \"Significance\": round(p_val, 4)})\n",
        "        except Exception:\n",
        "            not_in_eq.append({\"Independent Variable\": var,\n",
        "                              \"Score Statistic (LRT)\": None,\n",
        "                              \"Significance\": None})\n",
        "\n",
        "    not_in_eq_df = pd.DataFrame(not_in_eq)\n",
        "\n",
        "    # 額外：Train / Test Accuracy\n",
        "    train_pred = (final_model.predict(sm.add_constant(X_train[included])) > 0.5).astype(int)\n",
        "    test_pred = (final_model.predict(sm.add_constant(X_test[included])) > 0.5).astype(int)\n",
        "    train_acc = (train_pred == y_train).mean()\n",
        "    test_acc = (test_pred == y_test).mean()\n",
        "\n",
        "    print(f\"\\n✅ Stepwise completed with {len(included)} variables: {included}\")\n",
        "    print(f\"Train Accuracy: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "    return overall_fit, coef_df, not_in_eq_df, final_model\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def classification_table(model, df, target_col=\"is_fraud\", cutoff=0.0015):\n",
        "    y_true = df[target_col].astype(int)\n",
        "    X = sm.add_constant(df[model.params.index.drop(\"const\")])\n",
        "    y_pred_prob = model.predict(X)\n",
        "    y_pred = (y_pred_prob >= cutoff).astype(int)\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=[1, 0])\n",
        "    TP, FN, FP, TN = cm.ravel()\n",
        "\n",
        "    fraud_total = TP + FN\n",
        "    normal_total = FP + TN\n",
        "\n",
        "    fraud_correct = TP / fraud_total if fraud_total > 0 else 0\n",
        "    normal_correct = TN / normal_total if normal_total > 0 else 0\n",
        "    overall_correct = (TP + TN) / (fraud_total + normal_total)\n",
        "\n",
        "    table = pd.DataFrame({\n",
        "        \"Actual Group\": [\"Fraud (1)\", \"Normal (0)\", \"Total\"],\n",
        "        \"Predicted Fraud (1)\": [TP, FP, TP + FP],\n",
        "        \"Predicted Normal (0)\": [FN, TN, FN + TN],\n",
        "        \"Total\": [fraud_total, normal_total, fraud_total + normal_total],\n",
        "        \"% Correct\": [\n",
        "            round(fraud_correct * 100, 1),\n",
        "            round(normal_correct * 100, 1),\n",
        "            round(overall_correct * 100, 1),\n",
        "        ],\n",
        "        \"cutoff\": [cutoff,\"\",\"\"],\n",
        "        \"F1 Score\": [round(2 * TP / (2 * TP + FP + FN), 4),\"\",\"\"],\n",
        "    })\n",
        "    return table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 版本2.0用法 k=1\n",
        "\n",
        "overall_fit, coef_df, not_in_eq_df, model_0 = stepwise_logit_with_k_tables(\n",
        "    train_df, test_df, dep_var=\"is_fraud\", k=1, verbose=True\n",
        ")\n",
        "\n",
        "\n",
        "print(\"=== Overall Model Fit ===\")\n",
        "print(overall_fit)\n",
        "\n",
        "print(\"\\n=== Variables in the Equation ===\")\n",
        "print(coef_df)\n",
        "\n",
        "print(\"\\n=== Variables Not in the Equation ===\")\n",
        "print(not_in_eq_df)\n",
        "\n",
        "print(\"\\n================================================\")\n",
        "print(\"\\n=== Accuracy in Training and Testing dataset ===\")\n",
        "\n",
        "\n",
        "\n",
        "# Step 3. 產出訓練集和測試集的結果表\n",
        "train_table = classification_table(model_0, train_df, target_col=\"is_fraud\")\n",
        "test_table = classification_table(model_0, test_df, target_col=\"is_fraud\")\n",
        "\n",
        "print(\"\\n=== Classification Matrix — Training Sample ===\")\n",
        "print(train_table.to_string(index=False))\n",
        "print(\"\\n=== Classification Matrix — Holdout (Test) Sample ===\")\n",
        "print(test_table.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 版本2.0用法 k=3\n",
        "#overall_fit, coef_df, not_in_eq_df, model_3 = stepwise_logit_with_k_tables(\n",
        "#    train_df, test_df, dep_var=\"is_fraud\", k=0\n",
        "#)\n",
        "\n",
        "overall_fit, coef_df, not_in_eq_df, model_3 = stepwise_logit_with_k_tables(\n",
        "    train_df, test_df, dep_var=\"is_fraud\", k=3, verbose=True\n",
        ")\n",
        "\n",
        "\n",
        "print(\"=== Overall Model Fit ===\")\n",
        "print(overall_fit)\n",
        "\n",
        "print(\"\\n=== Variables in the Equation ===\")\n",
        "print(coef_df)\n",
        "\n",
        "print(\"\\n=== Variables Not in the Equation ===\")\n",
        "print(not_in_eq_df)\n",
        "\n",
        "print(\"\\n================================================\")\n",
        "print(\"\\n=== Accuracy in Training and Testing dataset ===\")\n",
        "\n",
        "\n",
        "# Step 3. 產出訓練集和測試集的結果表\n",
        "train_table = classification_table(model_3, train_df, target_col=\"is_fraud\")\n",
        "test_table = classification_table(model_3, test_df, target_col=\"is_fraud\")\n",
        "\n",
        "print(\"\\n=== Classification Matrix — Training Sample ===\")\n",
        "print(train_table.to_string(index=False))\n",
        "print(\"\\n=== Classification Matrix — Holdout (Test) Sample ===\")\n",
        "print(test_table.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🟢 Step 1: Forward — added variable: transaction_id (p=0)\n",
            "🟢 Step 2: Forward — added variable: yearly_income (p=8.322e-149)\n",
            "🟢 Step 3: Forward — added variable: amount (p=3.447e-57)\n",
            "🟢 Step 4: Forward — added variable: merchant_id (p=1.866e-36)\n",
            "🟢 Step 5: Forward — added variable: credit_limit (p=1.195e-19)\n",
            "🟢 Step 6: Forward — added variable: errors_missing_flag (p=1.088e-16)\n",
            "🟢 Step 7: Forward — added variable: latitude (p=1.086e-08)\n",
            "🟢 Step 8: Forward — added variable: card_type_Debit (p=5.796e-05)\n",
            "🟢 Step 9: Forward — added variable: card_number (p=3.331e-05)\n",
            "🟢 Step 10: Forward — added variable: num_credit_cards (p=0.0004267)\n",
            "🟢 Step 11: Forward — added variable: current_age (p=9.029e-05)\n",
            "🟢 Step 12: Forward — added variable: mcc_code (p=0.0007182)\n",
            "🟢 Step 13: Forward — added variable: retirement_age (p=0.02639)\n",
            "🟢 Step 14: Forward — added variable: longitude (p=0.04042)\n",
            "🟢 Step 15: Forward — added variable: cvv (p=0.04731)\n",
            "🟢 Step 16: Forward — added variable: num_cards_issued (p=0.04444)\n",
            "⚪ Step 17: No change — stopping iteration.\n",
            "\n",
            "✅ Stepwise completed with 16 variables: ['transaction_id', 'yearly_income', 'amount', 'merchant_id', 'credit_limit', 'errors_missing_flag', 'latitude', 'card_type_Debit', 'card_number', 'num_credit_cards', 'current_age', 'mcc_code', 'retirement_age', 'longitude', 'cvv', 'num_cards_issued']\n",
            "Train Accuracy: 0.9461, Test Accuracy: 0.9440\n",
            "=== Overall Model Fit ===\n",
            "                          Measure      Value Change_from_Base Change_pvalue\n",
            "0  -2 Log Likelihood (−2LL) value  14950.702          5473.84           0.0\n",
            "1                Cox and Snell R2      0.109                               \n",
            "2                   Nagelkerke R2      0.311                               \n",
            "3            Pseudo R2 (McFadden)      0.268                               \n",
            "4              Hosmer-Lemeshow χ2   5473.840                               \n",
            "\n",
            "=== Variables in the Equation ===\n",
            "   Independent Variable             B    Std. Error         Wald  df  \\\n",
            "0                 const -5.480037e+00  5.898185e-01    86.323738   1   \n",
            "1        transaction_id  3.577634e-07  7.821606e-09  2092.183752   1   \n",
            "2         yearly_income -1.033878e-01  4.691912e-03   485.555669   1   \n",
            "3                amount  2.165841e-01  1.421754e-02   232.062160   1   \n",
            "4           merchant_id  1.052980e-05  8.220572e-07   164.072476   1   \n",
            "5          credit_limit -1.970263e-02  3.382711e-03    33.924910   1   \n",
            "6   errors_missing_flag -1.097794e+00  1.314384e-01    69.758501   1   \n",
            "7              latitude -2.535606e-02  4.585419e-03    30.577755   1   \n",
            "8       card_type_Debit -2.414792e-01  5.236761e-02    21.263460   1   \n",
            "9           card_number  6.580066e-17  1.733692e-17    14.405109   1   \n",
            "10     num_credit_cards  7.152635e-02  1.505790e-02    22.563326   1   \n",
            "11          current_age -5.496915e-03  1.496055e-03    13.500275   1   \n",
            "12             mcc_code -8.628772e-05  2.621767e-05    10.832034   1   \n",
            "13       retirement_age  1.397192e-02  6.410559e-03     4.750294   1   \n",
            "14            longitude -2.786184e-03  1.413447e-03     3.885620   1   \n",
            "15                  cvv -1.550731e-04  7.690418e-05     4.066052   1   \n",
            "16     num_cards_issued  8.652342e-02  4.304903e-02     4.039622   1   \n",
            "\n",
            "             Sig.    Exp(B)  \n",
            "0    1.527638e-20  0.004169  \n",
            "1    0.000000e+00  1.000000  \n",
            "2   1.320860e-107  0.901777  \n",
            "3    2.116549e-52  1.241828  \n",
            "4    1.458575e-37  1.000011  \n",
            "5    5.728059e-09  0.980490  \n",
            "6    6.702834e-17  0.333606  \n",
            "7    3.207503e-08  0.974963  \n",
            "8    4.002897e-06  0.785465  \n",
            "9    1.474019e-04  1.000000  \n",
            "10   2.033292e-06  1.074146  \n",
            "11   2.385284e-04  0.994518  \n",
            "12   9.975899e-04  0.999914  \n",
            "13   2.929329e-02  1.014070  \n",
            "14   4.870127e-02  0.997218  \n",
            "15   4.375342e-02  0.999845  \n",
            "16   4.444378e-02  1.090377  \n",
            "\n",
            "=== Variables Not in the Equation ===\n",
            "         Independent Variable  Score Statistic (LRT)  Significance\n",
            "0                 client_id_x                  1.399        0.2369\n",
            "1                     card_id                  1.283        0.2573\n",
            "2  use_chip_Swipe Transaction                  2.292        0.1300\n",
            "3                  total_debt                  1.761        0.1845\n",
            "4                credit_score                  0.522        0.4701\n",
            "5                 gender_Male                  2.728        0.0986\n",
            "6       card_brand_Mastercard                  0.259        0.6108\n",
            "7                has_chip_YES                  0.364        0.5463\n",
            "\n",
            "================================================\n",
            "\n",
            "=== Accuracy in Training and Testing dataset ===\n",
            "\n",
            "=== Classification Matrix — Training Sample ===\n",
            "Actual Group  Predicted Fraud (1)  Predicted Normal (0)  Total  % Correct  cutoff F1 Score\n",
            "   Fraud (1)                 2564                    80   2644       97.0  0.0015   0.1165\n",
            "  Normal (0)                38813                  6152  44965       13.7                 \n",
            "       Total                41377                  6232  47609       18.3                 \n",
            "\n",
            "=== Classification Matrix — Holdout (Test) Sample ===\n",
            "Actual Group  Predicted Fraud (1)  Predicted Normal (0)  Total  % Correct  cutoff F1 Score\n",
            "   Fraud (1)                  660                    15    675       97.8  0.0015     0.12\n",
            "  Normal (0)                 9661                  1567  11228       14.0                 \n",
            "       Total                10321                  1582  11903       18.7                 \n"
          ]
        }
      ],
      "source": [
        "## 版本2.0 k=314657018 (Full Model)\n",
        "\n",
        "overall_fit, coef_df, not_in_eq_df, final_model = stepwise_logit_with_k_tables(\n",
        "    train_df, test_df, dep_var=\"is_fraud\", k=314657018, verbose=True\n",
        ")\n",
        "\n",
        "print(\"=== Overall Model Fit ===\")\n",
        "print(overall_fit)\n",
        "\n",
        "print(\"\\n=== Variables in the Equation ===\")\n",
        "print(coef_df)\n",
        "\n",
        "print(\"\\n=== Variables Not in the Equation ===\")\n",
        "print(not_in_eq_df)\n",
        "\n",
        "print(\"\\n================================================\")\n",
        "print(\"\\n=== Accuracy in Training and Testing dataset ===\")\n",
        "\n",
        "# Step 3. 產出訓練集和測試集的結果表\n",
        "train_table = classification_table(final_model, train_df, target_col=\"is_fraud\")\n",
        "test_table = classification_table(final_model, test_df, target_col=\"is_fraud\")\n",
        "\n",
        "print(\"\\n=== Classification Matrix — Training Sample ===\")\n",
        "print(train_table.to_string(index=False))\n",
        "print(\"\\n=== Classification Matrix — Holdout (Test) Sample ===\")\n",
        "print(test_table.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 一些模型檢驗診斷\n",
        "def cutoff_analysis(model, df, target_col=\"is_fraud\", cutoffs=None):\n",
        "    \"\"\"\n",
        "    產出類似 Table 8.7 的結果\n",
        "    \"\"\"\n",
        "    if cutoffs is None:\n",
        "        cutoffs = np.arange(0, 1.01, 0.02)  # 預設 0, 0.02, ..., 1\n",
        "    \n",
        "    y_true = df[target_col].astype(int)\n",
        "    X = sm.add_constant(df[model.params.index.drop(\"const\")])\n",
        "    y_prob = model.predict(X)\n",
        "\n",
        "    rows = []\n",
        "    for cutoff in cutoffs:\n",
        "        y_pred = (y_prob >= cutoff).astype(int)\n",
        "        cm = confusion_matrix(y_true, y_pred, labels=[0, 1])  # 注意順序 [0,1]\n",
        "        TN, FP, FN, TP = cm.ravel()\n",
        "\n",
        "        total = TP + TN + FP + FN\n",
        "        accuracy = (TP + TN) / total if total > 0 else np.nan\n",
        "        sensitivity = TP / (TP + FN) if (TP + FN) > 0 else np.nan\n",
        "        specificity = TN / (TN + FP) if (TN + FP) > 0 else np.nan\n",
        "        youden = sensitivity + specificity - 1 if not np.isnan(sensitivity) and not np.isnan(specificity) else np.nan\n",
        "        ppv = TP / (TP + FP) if (TP + FP) > 0 else np.nan\n",
        "        npv = TN / (TN + FN) if (TN + FN) > 0 else np.nan\n",
        "\n",
        "        rows.append({\n",
        "            \"Cutoff\": cutoff,\n",
        "            \"TN\": TN, \"FP\": FP, \"FN\": FN, \"TP\": TP,\n",
        "            \"Accuracy\": round(accuracy*100, 1),\n",
        "            \"Sensitivity\": round(sensitivity*100, 1) if not np.isnan(sensitivity) else \"NC\",\n",
        "            \"Specificity\": round(specificity*100, 1) if not np.isnan(specificity) else \"NC\",\n",
        "            \"Youden\": round(youden*100, 1) if not np.isnan(youden) else \"NC\",\n",
        "            \"PPV\": round(ppv*100, 1) if not np.isnan(ppv) else \"NC\",\n",
        "            \"NPV\": round(npv*100, 1) if not np.isnan(npv) else \"NC\",\n",
        "            \"F1 Score\": round(2*TP/(2*TP + FP + FN), 4) if (2*TP + FP + FN) > 0 else \"NC\"\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Cutoff    TN    FP  FN  TP  Accuracy  Sensitivity  Specificity  Youden   PPV    NPV  F1 Score\n",
            " 0.0000     0 11228   0 675       5.7        100.0          0.0     0.0   5.7     NC    0.1073\n",
            " 0.0005   104 11124   0 675       6.5        100.0          0.9     0.9   5.7  100.0    0.1082\n",
            " 0.0010   262 10966   0 675       7.9        100.0          2.3     2.3   5.8  100.0    0.1096\n",
            " 0.0015   468 10760   0 675       9.6        100.0          4.2     4.2   5.9  100.0    0.1115\n",
            " 0.0020   762 10466   3 672      12.0         99.6          6.8     6.3   6.0   99.6    0.1138\n",
            " 0.0025  1056 10172   3 672      14.5         99.6          9.4     9.0   6.2   99.7    0.1167\n",
            " 0.0030  1325  9903   3 672      16.8         99.6         11.8    11.4   6.4   99.8    0.1195\n",
            " 0.0035  1614  9614   4 671      19.2         99.4         14.4    13.8   6.5   99.8    0.1224\n",
            " 0.0040  1909  9319   6 669      21.7         99.1         17.0    16.1   6.7   99.7    0.1255\n",
            " 0.0045  2172  9056   6 669      23.9         99.1         19.3    18.5   6.9   99.7    0.1287\n",
            " 0.0050  2431  8797   8 667      26.0         98.8         21.7    20.5   7.0   99.7    0.1316\n",
            " 0.0055  2693  8535   9 666      28.2         98.7         24.0    22.7   7.2   99.7    0.1349\n",
            " 0.0060  2946  8282  11 664      30.3         98.4         26.2    24.6   7.4   99.6    0.1380\n",
            " 0.0065  3194  8034  11 664      32.4         98.4         28.4    26.8   7.6   99.7    0.1417\n",
            " 0.0070  3398  7830  13 662      34.1         98.1         30.3    28.3   7.8   99.6    0.1444\n",
            " 0.0075  3596  7632  15 660      35.8         97.8         32.0    29.8   8.0   99.6    0.1472\n",
            " 0.0080  3752  7476  15 660      37.1         97.8         33.4    31.2   8.1   99.6    0.1498\n",
            " 0.0085  3894  7334  17 658      38.2         97.5         34.7    32.2   8.2   99.6    0.1518\n",
            " 0.0090  4029  7199  18 657      39.4         97.3         35.9    33.2   8.4   99.6    0.1540\n",
            " 0.0095  4157  7071  19 656      40.4         97.2         37.0    34.2   8.5   99.5    0.1562\n",
            " 0.0100  4304  6924  21 654      41.7         96.9         38.3    35.2   8.6   99.5    0.1585\n",
            " 0.0105  4422  6806  21 654      42.6         96.9         39.4    36.3   8.8   99.5    0.1608\n",
            " 0.0110  4515  6713  22 653      43.4         96.7         40.2    37.0   8.9   99.5    0.1624\n",
            " 0.0115  4602  6626  23 652      44.1         96.6         41.0    37.6   9.0   99.5    0.1640\n",
            " 0.0120  4692  6536  24 651      44.9         96.4         41.8    38.2   9.1   99.5    0.1656\n",
            " 0.0125  4786  6442  26 649      45.7         96.1         42.6    38.8   9.2   99.5    0.1671\n",
            " 0.0130  4856  6372  27 648      46.2         96.0         43.2    39.2   9.2   99.4    0.1684\n",
            " 0.0135  4932  6296  28 647      46.9         95.9         43.9    39.8   9.3   99.4    0.1699\n",
            " 0.0140  4998  6230  30 645      47.4         95.6         44.5    40.1   9.4   99.4    0.1709\n",
            " 0.0145  5053  6175  32 643      47.9         95.3         45.0    40.3   9.4   99.4    0.1716\n",
            " 0.0150  5111  6117  33 642      48.3         95.1         45.5    40.6   9.5   99.4    0.1727\n",
            " 0.0155  5172  6056  35 640      48.8         94.8         46.1    40.9   9.6   99.3    0.1737\n",
            " 0.0160  5218  6010  38 637      49.2         94.4         46.5    40.8   9.6   99.3    0.1740\n",
            " 0.0165  5270  5958  38 637      49.6         94.4         46.9    41.3   9.7   99.3    0.1752\n",
            " 0.0170  5328  5900  39 636      50.1         94.2         47.5    41.7   9.7   99.3    0.1764\n",
            " 0.0175  5384  5844  41 634      50.6         93.9         48.0    41.9   9.8   99.2    0.1773\n",
            " 0.0180  5421  5807  43 632      50.9         93.6         48.3    41.9   9.8   99.2    0.1777\n",
            " 0.0185  5463  5765  46 629      51.2         93.2         48.7    41.8   9.8   99.2    0.1780\n",
            " 0.0190  5499  5729  46 629      51.5         93.2         49.0    42.2   9.9   99.2    0.1789\n",
            " 0.0195  5530  5698  46 629      51.7         93.2         49.3    42.4   9.9   99.2    0.1797\n",
            " 0.0200  5578  5650  47 628      52.1         93.0         49.7    42.7  10.0   99.2    0.1806\n",
            " 0.0205  5618  5610  50 625      52.4         92.6         50.0    42.6  10.0   99.1    0.1809\n",
            " 0.0210  5655  5573  51 624      52.8         92.4         50.4    42.8  10.1   99.1    0.1816\n",
            " 0.0215  5708  5520  54 621      53.2         92.0         50.8    42.8  10.1   99.1    0.1822\n",
            " 0.0220  5737  5491  55 620      53.4         91.9         51.1    42.9  10.1   99.1    0.1827\n",
            " 0.0225  5772  5456  57 618      53.7         91.6         51.4    43.0  10.2   99.0    0.1831\n",
            " 0.0230  5811  5417  57 618      54.0         91.6         51.8    43.3  10.2   99.0    0.1842\n",
            " 0.0235  5851  5377  57 618      54.3         91.6         52.1    43.7  10.3   99.0    0.1853\n",
            " 0.0240  5884  5344  58 617      54.6         91.4         52.4    43.8  10.4   99.0    0.1860\n",
            " 0.0245  5902  5326  58 617      54.8         91.4         52.6    44.0  10.4   99.0    0.1865\n",
            " 0.0250  5935  5293  59 616      55.0         91.3         52.9    44.1  10.4   99.0    0.1871\n",
            " 0.0255  5966  5262  61 614      55.3         91.0         53.1    44.1  10.4   99.0    0.1875\n",
            " 0.0260  5993  5235  62 613      55.5         90.8         53.4    44.2  10.5   99.0    0.1880\n",
            " 0.0265  6016  5212  63 612      55.7         90.7         53.6    44.2  10.5   99.0    0.1883\n",
            " 0.0270  6051  5177  63 612      56.0         90.7         53.9    44.6  10.6   99.0    0.1894\n",
            " 0.0275  6082  5146  65 610      56.2         90.4         54.2    44.5  10.6   98.9    0.1897\n",
            " 0.0280  6119  5109  66 609      56.5         90.2         54.5    44.7  10.7   98.9    0.1905\n",
            " 0.0285  6154  5074  67 608      56.8         90.1         54.8    44.9  10.7   98.9    0.1913\n",
            " 0.0290  6184  5044  67 608      57.1         90.1         55.1    45.2  10.8   98.9    0.1922\n",
            " 0.0295  6210  5018  69 606      57.3         89.8         55.3    45.1  10.8   98.9    0.1924\n",
            " 0.0300  6233  4995  72 603      57.4         89.3         55.5    44.8  10.8   98.9    0.1923\n",
            " 0.0305  6261  4967  74 601      57.6         89.0         55.8    44.8  10.8   98.8    0.1925\n",
            " 0.0310  6297  4931  75 600      57.9         88.9         56.1    45.0  10.8   98.8    0.1934\n",
            " 0.0315  6330  4898  76 599      58.2         88.7         56.4    45.1  10.9   98.8    0.1941\n",
            " 0.0320  6361  4867  77 598      58.5         88.6         56.7    45.2  10.9   98.8    0.1948\n",
            " 0.0325  6400  4828  78 597      58.8         88.4         57.0    45.4  11.0   98.8    0.1957\n",
            " 0.0330  6422  4806  78 597      59.0         88.4         57.2    45.6  11.0   98.8    0.1964\n",
            " 0.0335  6445  4783  78 597      59.2         88.4         57.4    45.8  11.1   98.8    0.1972\n",
            " 0.0340  6474  4754  80 595      59.4         88.1         57.7    45.8  11.1   98.8    0.1975\n",
            " 0.0345  6495  4733  82 593      59.5         87.9         57.8    45.7  11.1   98.8    0.1976\n",
            " 0.0350  6514  4714  84 591      59.7         87.6         58.0    45.6  11.1   98.7    0.1977\n",
            " 0.0355  6542  4686  84 591      59.9         87.6         58.3    45.8  11.2   98.7    0.1986\n",
            " 0.0360  6564  4664  86 589      60.1         87.3         58.5    45.7  11.2   98.7    0.1987\n",
            " 0.0365  6589  4639  86 589      60.3         87.3         58.7    45.9  11.3   98.7    0.1996\n",
            " 0.0370  6613  4615  87 588      60.5         87.1         58.9    46.0  11.3   98.7    0.2001\n",
            " 0.0375  6648  4580  89 586      60.8         86.8         59.2    46.0  11.3   98.7    0.2007\n",
            " 0.0380  6665  4563  90 585      60.9         86.7         59.4    46.0  11.4   98.7    0.2009\n",
            " 0.0385  6698  4530  91 584      61.2         86.5         59.7    46.2  11.4   98.7    0.2018\n",
            " 0.0390  6721  4507  91 584      61.4         86.5         59.9    46.4  11.5   98.7    0.2026\n",
            " 0.0395  6750  4478  92 583      61.6         86.4         60.1    46.5  11.5   98.7    0.2033\n",
            " 0.0400  6787  4441  92 583      61.9         86.4         60.4    46.8  11.6   98.7    0.2046\n",
            " 0.0405  6810  4418  93 582      62.1         86.2         60.7    46.9  11.6   98.7    0.2051\n",
            " 0.0410  6843  4385  94 581      62.4         86.1         60.9    47.0  11.7   98.6    0.2060\n",
            " 0.0415  6875  4353  94 581      62.6         86.1         61.2    47.3  11.8   98.7    0.2072\n",
            " 0.0420  6894  4334  94 581      62.8         86.1         61.4    47.5  11.8   98.7    0.2079\n",
            " 0.0425  6915  4313  94 581      63.0         86.1         61.6    47.7  11.9   98.7    0.2087\n",
            " 0.0430  6938  4290  95 580      63.2         85.9         61.8    47.7  11.9   98.6    0.2092\n",
            " 0.0435  6960  4268  98 577      63.3         85.5         62.0    47.5  11.9   98.6    0.2091\n",
            " 0.0440  6988  4240  98 577      63.6         85.5         62.2    47.7  12.0   98.6    0.2101\n",
            " 0.0445  7006  4222  99 576      63.7         85.3         62.4    47.7  12.0   98.6    0.2105\n",
            " 0.0450  7026  4202 100 575      63.9         85.2         62.6    47.8  12.0   98.6    0.2109\n",
            " 0.0455  7052  4176 101 574      64.1         85.0         62.8    47.8  12.1   98.6    0.2116\n",
            " 0.0460  7086  4142 101 574      64.4         85.0         63.1    48.1  12.2   98.6    0.2129\n",
            " 0.0465  7116  4112 101 574      64.6         85.0         63.4    48.4  12.2   98.6    0.2141\n",
            " 0.0470  7139  4089 101 574      64.8         85.0         63.6    48.6  12.3   98.6    0.2151\n",
            " 0.0475  7167  4061 102 573      65.0         84.9         63.8    48.7  12.4   98.6    0.2159\n",
            " 0.0480  7189  4039 105 570      65.2         84.4         64.0    48.5  12.4   98.6    0.2157\n",
            " 0.0485  7221  4007 106 569      65.4         84.3         64.3    48.6  12.4   98.6    0.2167\n",
            " 0.0490  7249  3979 108 567      65.7         84.0         64.6    48.6  12.5   98.5    0.2172\n",
            " 0.0495  7270  3958 110 565      65.8         83.7         64.7    48.5  12.5   98.5    0.2174\n",
            " 0.0500  7296  3932 110 565      66.0         83.7         65.0    48.7  12.6   98.5    0.2185\n",
            " 0.0505  7320  3908 112 563      66.2         83.4         65.2    48.6  12.6   98.5    0.2188\n",
            " 0.0510  7348  3880 114 561      66.4         83.1         65.4    48.6  12.6   98.5    0.2193\n",
            " 0.0515  7372  3856 116 559      66.6         82.8         65.7    48.5  12.7   98.5    0.2196\n",
            " 0.0520  7398  3830 116 559      66.8         82.8         65.9    48.7  12.7   98.5    0.2208\n",
            " 0.0525  7430  3798 119 556      67.1         82.4         66.2    48.5  12.8   98.4    0.2211\n",
            " 0.0530  7447  3781 119 556      67.2         82.4         66.3    48.7  12.8   98.4    0.2219\n",
            " 0.0535  7475  3753 122 553      67.4         81.9         66.6    48.5  12.8   98.4    0.2220\n",
            " 0.0540  7496  3732 124 551      67.6         81.6         66.8    48.4  12.9   98.4    0.2223\n",
            " 0.0545  7524  3704 125 550      67.8         81.5         67.0    48.5  12.9   98.4    0.2232\n",
            " 0.0550  7546  3682 127 548      68.0         81.2         67.2    48.4  13.0   98.3    0.2234\n",
            " 0.0555  7563  3665 129 546      68.1         80.9         67.4    48.2  13.0   98.3    0.2235\n",
            " 0.0560  7580  3648 130 545      68.3         80.7         67.5    48.3  13.0   98.3    0.2239\n",
            " 0.0565  7608  3620 131 544      68.5         80.6         67.8    48.4  13.1   98.3    0.2248\n",
            " 0.0570  7635  3593 131 544      68.7         80.6         68.0    48.6  13.1   98.3    0.2261\n",
            " 0.0575  7654  3574 132 543      68.9         80.4         68.2    48.6  13.2   98.3    0.2266\n",
            " 0.0580  7677  3551 133 542      69.0         80.3         68.4    48.7  13.2   98.3    0.2273\n",
            " 0.0585  7705  3523 136 539      69.3         79.9         68.6    48.5  13.3   98.3    0.2276\n",
            " 0.0590  7728  3500 137 538      69.4         79.7         68.8    48.5  13.3   98.3    0.2283\n",
            " 0.0595  7749  3479 137 538      69.6         79.7         69.0    48.7  13.4   98.3    0.2293\n",
            " 0.0600  7773  3455 138 537      69.8         79.6         69.2    48.8  13.5   98.3    0.2301\n",
            " 0.0605  7791  3437 139 536      70.0         79.4         69.4    48.8  13.5   98.2    0.2306\n",
            " 0.0610  7806  3422 143 532      70.0         78.8         69.5    48.3  13.5   98.2    0.2299\n",
            " 0.0615  7830  3398 145 530      70.2         78.5         69.7    48.3  13.5   98.2    0.2303\n",
            " 0.0620  7852  3376 146 529      70.4         78.4         69.9    48.3  13.5   98.2    0.2310\n",
            " 0.0625  7879  3349 146 529      70.6         78.4         70.2    48.5  13.6   98.2    0.2324\n",
            " 0.0630  7904  3324 147 528      70.8         78.2         70.4    48.6  13.7   98.2    0.2333\n",
            " 0.0635  7919  3309 153 522      70.9         77.3         70.5    47.9  13.6   98.1    0.2317\n",
            " 0.0640  7940  3288 157 518      71.1         76.7         70.7    47.5  13.6   98.1    0.2312\n",
            " 0.0645  7954  3274 158 517      71.2         76.6         70.8    47.4  13.6   98.1    0.2315\n",
            " 0.0650  7978  3250 158 517      71.4         76.6         71.1    47.6  13.7   98.1    0.2328\n",
            " 0.0655  7996  3232 158 517      71.5         76.6         71.2    47.8  13.8   98.1    0.2337\n",
            " 0.0660  8015  3213 159 516      71.7         76.4         71.4    47.8  13.8   98.1    0.2343\n",
            " 0.0665  8040  3188 160 515      71.9         76.3         71.6    47.9  13.9   98.0    0.2353\n",
            " 0.0670  8059  3169 160 515      72.0         76.3         71.8    48.1  14.0   98.1    0.2363\n",
            " 0.0675  8078  3150 163 512      72.2         75.9         71.9    47.8  14.0   98.0    0.2361\n",
            " 0.0680  8105  3123 164 511      72.4         75.7         72.2    47.9  14.1   98.0    0.2372\n",
            " 0.0685  8131  3097 167 508      72.6         75.3         72.4    47.7  14.1   98.0    0.2374\n",
            " 0.0690  8156  3072 167 508      72.8         75.3         72.6    47.9  14.2   98.0    0.2388\n",
            " 0.0695  8175  3053 169 506      72.9         75.0         72.8    47.8  14.2   98.0    0.2390\n",
            " 0.0700  8196  3032 169 506      73.1         75.0         73.0    48.0  14.3   98.0    0.2402\n",
            " 0.0705  8220  3008 171 504      73.3         74.7         73.2    47.9  14.4   98.0    0.2407\n",
            " 0.0710  8234  2994 171 504      73.4         74.7         73.3    48.0  14.4   98.0    0.2416\n",
            " 0.0715  8254  2974 171 504      73.6         74.7         73.5    48.2  14.5   98.0    0.2427\n",
            " 0.0720  8272  2956 172 503      73.7         74.5         73.7    48.2  14.5   98.0    0.2433\n",
            " 0.0725  8296  2932 173 502      73.9         74.4         73.9    48.3  14.6   98.0    0.2443\n",
            " 0.0730  8310  2918 174 501      74.0         74.2         74.0    48.2  14.7   97.9    0.2447\n",
            " 0.0735  8330  2898 176 499      74.2         73.9         74.2    48.1  14.7   97.9    0.2451\n",
            " 0.0740  8349  2879 178 497      74.3         73.6         74.4    48.0  14.7   97.9    0.2454\n",
            " 0.0745  8360  2868 180 495      74.4         73.3         74.5    47.8  14.7   97.9    0.2452\n",
            " 0.0750  8371  2857 181 494      74.5         73.2         74.6    47.7  14.7   97.9    0.2454\n",
            " 0.0755  8393  2835 182 493      74.7         73.0         74.8    47.8  14.8   97.9    0.2463\n",
            " 0.0760  8408  2820 182 493      74.8         73.0         74.9    47.9  14.9   97.9    0.2472\n",
            " 0.0765  8425  2803 182 493      74.9         73.0         75.0    48.1  15.0   97.9    0.2483\n",
            " 0.0770  8445  2783 183 492      75.1         72.9         75.2    48.1  15.0   97.9    0.2491\n",
            " 0.0775  8459  2769 183 492      75.2         72.9         75.3    48.2  15.1   97.9    0.2500\n",
            " 0.0780  8478  2750 183 492      75.4         72.9         75.5    48.4  15.2   97.9    0.2512\n",
            " 0.0785  8497  2731 187 488      75.5         72.3         75.7    48.0  15.2   97.8    0.2506\n",
            " 0.0790  8518  2710 187 488      75.7         72.3         75.9    48.2  15.3   97.9    0.2520\n",
            " 0.0795  8537  2691 188 487      75.8         72.1         76.0    48.2  15.3   97.8    0.2528\n",
            " 0.0800  8559  2669 188 487      76.0         72.1         76.2    48.4  15.4   97.9    0.2542\n",
            " 0.0805  8584  2644 190 485      76.2         71.9         76.5    48.3  15.5   97.8    0.2550\n",
            " 0.0810  8604  2624 192 483      76.3         71.6         76.6    48.2  15.5   97.8    0.2554\n",
            " 0.0815  8619  2609 192 483      76.5         71.6         76.8    48.3  15.6   97.8    0.2564\n",
            " 0.0820  8638  2590 192 483      76.6         71.6         76.9    48.5  15.7   97.8    0.2577\n",
            " 0.0825  8653  2575 192 483      76.8         71.6         77.1    48.6  15.8   97.8    0.2588\n",
            " 0.0830  8673  2555 195 480      76.9         71.1         77.2    48.4  15.8   97.8    0.2588\n",
            " 0.0835  8694  2534 195 480      77.1         71.1         77.4    48.5  15.9   97.8    0.2602\n",
            " 0.0840  8715  2513 199 476      77.2         70.5         77.6    48.1  15.9   97.8    0.2598\n",
            " 0.0845  8727  2501 199 476      77.3         70.5         77.7    48.2  16.0   97.8    0.2607\n",
            " 0.0850  8743  2485 201 474      77.4         70.2         77.9    48.1  16.0   97.8    0.2609\n",
            " 0.0855  8763  2465 205 470      77.6         69.6         78.0    47.7  16.0   97.7    0.2604\n",
            " 0.0860  8777  2451 208 467      77.7         69.2         78.2    47.4  16.0   97.7    0.2599\n",
            " 0.0865  8796  2432 210 465      77.8         68.9         78.3    47.2  16.1   97.7    0.2604\n",
            " 0.0870  8811  2417 210 465      77.9         68.9         78.5    47.4  16.1   97.7    0.2615\n",
            " 0.0875  8828  2400 211 464      78.1         68.7         78.6    47.4  16.2   97.7    0.2622\n",
            " 0.0880  8849  2379 212 463      78.2         68.6         78.8    47.4  16.3   97.7    0.2633\n",
            " 0.0885  8861  2367 215 460      78.3         68.1         78.9    47.1  16.3   97.6    0.2627\n",
            " 0.0890  8889  2339 215 460      78.5         68.1         79.2    47.3  16.4   97.6    0.2648\n",
            " 0.0895  8906  2322 218 457      78.7         67.7         79.3    47.0  16.4   97.6    0.2646\n",
            " 0.0900  8918  2310 218 457      78.8         67.7         79.4    47.1  16.5   97.6    0.2655\n",
            " 0.0905  8931  2297 219 456      78.9         67.6         79.5    47.1  16.6   97.6    0.2660\n",
            " 0.0910  8944  2284 221 454      79.0         67.3         79.7    46.9  16.6   97.6    0.2660\n",
            " 0.0915  8962  2266 223 452      79.1         67.0         79.8    46.8  16.6   97.6    0.2664\n",
            " 0.0920  8977  2251 226 449      79.2         66.5         80.0    46.5  16.6   97.5    0.2661\n",
            " 0.0925  8989  2239 230 445      79.3         65.9         80.1    46.0  16.6   97.5    0.2650\n",
            " 0.0930  9007  2221 231 444      79.4         65.8         80.2    46.0  16.7   97.5    0.2659\n",
            " 0.0935  9025  2203 234 441      79.5         65.3         80.4    45.7  16.7   97.5    0.2657\n",
            " 0.0940  9040  2188 236 439      79.6         65.0         80.5    45.6  16.7   97.5    0.2659\n",
            " 0.0945  9056  2172 237 438      79.8         64.9         80.7    45.5  16.8   97.4    0.2667\n",
            " 0.0950  9082  2146 238 437      80.0         64.7         80.9    45.6  16.9   97.4    0.2683\n",
            " 0.0955  9095  2133 238 437      80.1         64.7         81.0    45.7  17.0   97.4    0.2693\n",
            " 0.0960  9109  2119 241 434      80.2         64.3         81.1    45.4  17.0   97.4    0.2689\n",
            " 0.0965  9124  2104 242 433      80.3         64.1         81.3    45.4  17.1   97.4    0.2696\n",
            " 0.0970  9132  2096 243 432      80.3         64.0         81.3    45.3  17.1   97.4    0.2697\n",
            " 0.0975  9144  2084 244 431      80.4         63.9         81.4    45.3  17.1   97.4    0.2702\n",
            " 0.0980  9156  2072 246 429      80.5         63.6         81.5    45.1  17.2   97.4    0.2702\n",
            " 0.0985  9170  2058 247 428      80.6         63.4         81.7    45.1  17.2   97.4    0.2708\n",
            " 0.0990  9184  2044 247 428      80.8         63.4         81.8    45.2  17.3   97.4    0.2720\n",
            " 0.0995  9198  2030 247 428      80.9         63.4         81.9    45.3  17.4   97.4    0.2732\n",
            " 0.1000  9209  2019 254 421      80.9         62.4         82.0    44.4  17.3   97.3    0.2703\n",
            " 0.1005  9222  2006 254 421      81.0         62.4         82.1    44.5  17.3   97.3    0.2714\n",
            " 0.1010  9233  1995 254 421      81.1         62.4         82.2    44.6  17.4   97.3    0.2724\n",
            " 0.1015  9256  1972 255 420      81.3         62.2         82.4    44.7  17.6   97.3    0.2739\n",
            " 0.1020  9271  1957 256 419      81.4         62.1         82.6    44.6  17.6   97.3    0.2747\n",
            " 0.1025  9287  1941 256 419      81.5         62.1         82.7    44.8  17.8   97.3    0.2761\n",
            " 0.1030  9304  1924 258 417      81.7         61.8         82.9    44.6  17.8   97.3    0.2765\n",
            " 0.1035  9319  1909 262 413      81.8         61.2         83.0    44.2  17.8   97.3    0.2756\n",
            " 0.1040  9336  1892 263 412      81.9         61.0         83.1    44.2  17.9   97.3    0.2766\n",
            " 0.1045  9355  1873 264 411      82.0         60.9         83.3    44.2  18.0   97.3    0.2778\n",
            " 0.1050  9377  1851 266 409      82.2         60.6         83.5    44.1  18.1   97.2    0.2787\n",
            " 0.1055  9394  1834 267 408      82.3         60.4         83.7    44.1  18.2   97.2    0.2797\n",
            " 0.1060  9403  1825 268 407      82.4         60.3         83.7    44.0  18.2   97.2    0.2800\n",
            " 0.1065  9412  1816 270 405      82.5         60.0         83.8    43.8  18.2   97.2    0.2797\n",
            " 0.1070  9423  1805 271 404      82.6         59.9         83.9    43.8  18.3   97.2    0.2802\n",
            " 0.1075  9433  1795 271 404      82.6         59.9         84.0    43.9  18.4   97.2    0.2811\n",
            " 0.1080  9442  1786 271 404      82.7         59.9         84.1    43.9  18.4   97.2    0.2820\n",
            " 0.1085  9455  1773 272 403      82.8         59.7         84.2    43.9  18.5   97.2    0.2827\n",
            " 0.1090  9466  1762 274 401      82.9         59.4         84.3    43.7  18.5   97.2    0.2826\n",
            " 0.1095  9487  1741 276 399      83.1         59.1         84.5    43.6  18.6   97.2    0.2835\n",
            " 0.1100  9504  1724 279 396      83.2         58.7         84.6    43.3  18.7   97.1    0.2834\n",
            " 0.1105  9517  1711 282 393      83.3         58.2         84.8    43.0  18.7   97.1    0.2828\n",
            " 0.1110  9526  1702 284 391      83.3         57.9         84.8    42.8  18.7   97.1    0.2825\n",
            " 0.1115  9540  1688 286 389      83.4         57.6         85.0    42.6  18.7   97.1    0.2827\n",
            " 0.1120  9554  1674 287 388      83.5         57.5         85.1    42.6  18.8   97.1    0.2835\n",
            " 0.1125  9565  1663 287 388      83.6         57.5         85.2    42.7  18.9   97.1    0.2847\n",
            " 0.1130  9580  1648 287 388      83.7         57.5         85.3    42.8  19.1   97.1    0.2862\n",
            " 0.1135  9598  1630 290 385      83.9         57.0         85.5    42.5  19.1   97.1    0.2862\n",
            " 0.1140  9610  1618 292 383      84.0         56.7         85.6    42.3  19.1   97.1    0.2862\n",
            " 0.1145  9625  1603 292 383      84.1         56.7         85.7    42.5  19.3   97.1    0.2879\n",
            " 0.1150  9633  1595 293 382      84.1         56.6         85.8    42.4  19.3   97.0    0.2881\n",
            " 0.1155  9641  1587 293 382      84.2         56.6         85.9    42.5  19.4   97.1    0.2890\n",
            " 0.1160  9653  1575 293 382      84.3         56.6         86.0    42.6  19.5   97.1    0.2903\n",
            " 0.1165  9664  1564 295 380      84.4         56.3         86.1    42.4  19.5   97.0    0.2902\n",
            " 0.1170  9678  1550 295 380      84.5         56.3         86.2    42.5  19.7   97.0    0.2917\n",
            " 0.1175  9693  1535 296 379      84.6         56.1         86.3    42.5  19.8   97.0    0.2928\n",
            " 0.1180  9705  1523 296 379      84.7         56.1         86.4    42.6  19.9   97.0    0.2941\n",
            " 0.1185  9709  1519 297 378      84.7         56.0         86.5    42.5  19.9   97.0    0.2939\n",
            " 0.1190  9725  1503 300 375      84.9         55.6         86.6    42.2  20.0   97.0    0.2938\n",
            " 0.1195  9736  1492 301 374      84.9         55.4         86.7    42.1  20.0   97.0    0.2944\n",
            " 0.1200  9749  1479 305 370      85.0         54.8         86.8    41.6  20.0   97.0    0.2932\n",
            " 0.1205  9767  1461 305 370      85.2         54.8         87.0    41.8  20.2   97.0    0.2953\n",
            " 0.1210  9773  1455 306 369      85.2         54.7         87.0    41.7  20.2   97.0    0.2953\n",
            " 0.1215  9781  1447 306 369      85.3         54.7         87.1    41.8  20.3   97.0    0.2963\n",
            " 0.1220  9793  1435 307 368      85.4         54.5         87.2    41.7  20.4   97.0    0.2970\n",
            " 0.1225  9803  1425 309 366      85.4         54.2         87.3    41.5  20.4   96.9    0.2968\n",
            " 0.1230  9812  1416 311 364      85.5         53.9         87.4    41.3  20.4   96.9    0.2965\n",
            " 0.1235  9825  1403 312 363      85.6         53.8         87.5    41.3  20.6   96.9    0.2974\n",
            " 0.1240  9831  1397 313 362      85.6         53.6         87.6    41.2  20.6   96.9    0.2975\n",
            " 0.1245  9837  1391 316 359      85.7         53.2         87.6    40.8  20.5   96.9    0.2961\n",
            " 0.1250  9846  1382 318 357      85.7         52.9         87.7    40.6  20.5   96.9    0.2958\n",
            " 0.1255  9859  1369 320 355      85.8         52.6         87.8    40.4  20.6   96.9    0.2960\n",
            " 0.1260  9871  1357 320 355      85.9         52.6         87.9    40.5  20.7   96.9    0.2974\n",
            " 0.1265  9880  1348 323 352      86.0         52.1         88.0    40.1  20.7   96.8    0.2964\n",
            " 0.1270  9891  1337 323 352      86.1         52.1         88.1    40.2  20.8   96.8    0.2978\n",
            " 0.1275  9895  1333 326 349      86.1         51.7         88.1    39.8  20.7   96.8    0.2961\n",
            " 0.1280  9908  1320 329 346      86.1         51.3         88.2    39.5  20.8   96.8    0.2956\n",
            " 0.1285  9919  1309 332 343      86.2         50.8         88.3    39.2  20.8   96.8    0.2948\n",
            " 0.1290  9931  1297 333 342      86.3         50.7         88.4    39.1  20.9   96.8    0.2956\n",
            " 0.1295  9951  1277 334 341      86.5         50.5         88.6    39.1  21.1   96.8    0.2974\n",
            " 0.1300  9957  1271 335 340      86.5         50.4         88.7    39.1  21.1   96.7    0.2975\n",
            " 0.1305  9966  1262 335 340      86.6         50.4         88.8    39.1  21.2   96.7    0.2986\n",
            " 0.1310  9977  1251 335 340      86.7         50.4         88.9    39.2  21.4   96.8    0.3001\n",
            " 0.1315  9988  1240 335 340      86.8         50.4         89.0    39.3  21.5   96.8    0.3016\n",
            " 0.1320  9993  1235 339 336      86.8         49.8         89.0    38.8  21.4   96.7    0.2992\n",
            " 0.1325 10006  1222 343 332      86.9         49.2         89.1    38.3  21.4   96.7    0.2979\n",
            " 0.1330 10010  1218 343 332      86.9         49.2         89.2    38.3  21.4   96.7    0.2984\n",
            " 0.1335 10016  1212 347 328      86.9         48.6         89.2    37.8  21.3   96.7    0.2962\n",
            " 0.1340 10025  1203 347 328      87.0         48.6         89.3    37.9  21.4   96.7    0.2974\n",
            " 0.1345 10034  1194 348 327      87.0         48.4         89.4    37.8  21.5   96.6    0.2978\n",
            " 0.1350 10043  1185 349 326      87.1         48.3         89.4    37.7  21.6   96.6    0.2983\n",
            " 0.1355 10054  1174 350 325      87.2         48.1         89.5    37.7  21.7   96.6    0.2990\n",
            " 0.1360 10061  1167 351 324      87.2         48.0         89.6    37.6  21.7   96.6    0.2992\n",
            " 0.1365 10073  1155 351 324      87.3         48.0         89.7    37.7  21.9   96.6    0.3008\n",
            " 0.1370 10082  1146 354 321      87.4         47.6         89.8    37.3  21.9   96.6    0.2997\n",
            " 0.1375 10096  1132 354 321      87.5         47.6         89.9    37.5  22.1   96.6    0.3017\n",
            " 0.1380 10099  1129 356 319      87.5         47.3         89.9    37.2  22.0   96.6    0.3005\n",
            " 0.1385 10108  1120 358 317      87.6         47.0         90.0    37.0  22.1   96.6    0.3002\n",
            " 0.1390 10113  1115 358 317      87.6         47.0         90.1    37.0  22.1   96.6    0.3009\n",
            " 0.1395 10119  1109 361 314      87.7         46.5         90.1    36.6  22.1   96.6    0.2993\n",
            " 0.1400 10129  1099 365 310      87.7         45.9         90.2    36.1  22.0   96.5    0.2975\n",
            " 0.1405 10138  1090 365 310      87.8         45.9         90.3    36.2  22.1   96.5    0.2988\n",
            " 0.1410 10144  1084 368 307      87.8         45.5         90.3    35.8  22.1   96.5    0.2972\n",
            " 0.1415 10158  1070 368 307      87.9         45.5         90.5    36.0  22.3   96.5    0.2992\n",
            " 0.1420 10167  1061 369 306      88.0         45.3         90.6    35.9  22.4   96.5    0.2997\n",
            " 0.1425 10173  1055 370 305      88.0         45.2         90.6    35.8  22.4   96.5    0.2998\n",
            " 0.1430 10182  1046 370 305      88.1         45.2         90.7    35.9  22.6   96.5    0.3011\n",
            " 0.1435 10194  1034 373 302      88.2         44.7         90.8    35.5  22.6   96.5    0.3003\n",
            " 0.1440 10206  1022 374 301      88.3         44.6         90.9    35.5  22.8   96.5    0.3013\n",
            " 0.1445 10213  1015 374 301      88.3         44.6         91.0    35.6  22.9   96.5    0.3024\n",
            " 0.1450 10217  1011 377 298      88.3         44.1         91.0    35.1  22.8   96.4    0.3004\n",
            " 0.1455 10219  1009 378 297      88.3         44.0         91.0    35.0  22.7   96.4    0.2998\n",
            " 0.1460 10231   997 379 296      88.4         43.9         91.1    35.0  22.9   96.4    0.3008\n",
            " 0.1465 10232   996 381 294      88.4         43.6         91.1    34.7  22.8   96.4    0.2992\n",
            " 0.1470 10244   984 381 294      88.5         43.6         91.2    34.8  23.0   96.4    0.3011\n",
            " 0.1475 10250   978 386 289      88.5         42.8         91.3    34.1  22.8   96.4    0.2976\n",
            " 0.1480 10256   972 388 287      88.6         42.5         91.3    33.9  22.8   96.4    0.2968\n",
            " 0.1485 10267   961 389 286      88.7         42.4         91.4    33.8  22.9   96.3    0.2976\n",
            " 0.1490 10271   957 389 286      88.7         42.4         91.5    33.8  23.0   96.4    0.2982\n",
            " 0.1495 10277   951 390 285      88.7         42.2         91.5    33.8  23.1   96.3    0.2983\n",
            " 0.1500 10281   947 390 285      88.8         42.2         91.6    33.8  23.1   96.3    0.2989\n",
            " 0.1505 10289   939 391 284      88.8         42.1         91.6    33.7  23.2   96.3    0.2993\n",
            " 0.1510 10294   934 391 284      88.9         42.1         91.7    33.8  23.3   96.3    0.3001\n",
            " 0.1515 10300   928 392 283      88.9         41.9         91.7    33.7  23.4   96.3    0.3001\n",
            " 0.1520 10306   922 392 283      89.0         41.9         91.8    33.7  23.5   96.3    0.3011\n",
            " 0.1525 10310   918 392 283      89.0         41.9         91.8    33.7  23.6   96.3    0.3017\n",
            " 0.1530 10324   904 392 283      89.1         41.9         91.9    33.9  23.8   96.3    0.3040\n",
            " 0.1535 10330   898 392 283      89.2         41.9         92.0    33.9  24.0   96.3    0.3050\n",
            " 0.1540 10335   893 396 279      89.2         41.3         92.0    33.4  23.8   96.3    0.3021\n",
            " 0.1545 10343   885 397 278      89.2         41.2         92.1    33.3  23.9   96.3    0.3025\n",
            " 0.1550 10351   877 398 277      89.3         41.0         92.2    33.2  24.0   96.3    0.3029\n",
            " 0.1555 10359   869 398 277      89.4         41.0         92.3    33.3  24.2   96.3    0.3042\n",
            " 0.1560 10364   864 398 277      89.4         41.0         92.3    33.3  24.3   96.3    0.3051\n",
            " 0.1565 10372   856 400 275      89.4         40.7         92.4    33.1  24.3   96.3    0.3045\n",
            " 0.1570 10378   850 402 273      89.5         40.4         92.4    32.9  24.3   96.3    0.3037\n",
            " 0.1575 10387   841 405 270      89.5         40.0         92.5    32.5  24.3   96.2    0.3024\n",
            " 0.1580 10391   837 406 269      89.6         39.9         92.5    32.4  24.3   96.2    0.3021\n",
            " 0.1585 10400   828 409 266      89.6         39.4         92.6    32.0  24.3   96.2    0.3007\n",
            " 0.1590 10403   825 409 266      89.6         39.4         92.7    32.1  24.4   96.2    0.3012\n",
            " 0.1595 10412   816 410 265      89.7         39.3         92.7    32.0  24.5   96.2    0.3018\n",
            " 0.1600 10418   810 412 263      89.7         39.0         92.8    31.7  24.5   96.2    0.3009\n",
            " 0.1605 10426   802 412 263      89.8         39.0         92.9    31.8  24.7   96.2    0.3023\n",
            " 0.1610 10429   799 413 262      89.8         38.8         92.9    31.7  24.7   96.2    0.3018\n",
            " 0.1615 10433   795 415 260      89.8         38.5         92.9    31.4  24.6   96.2    0.3006\n",
            " 0.1620 10438   790 415 260      89.9         38.5         93.0    31.5  24.8   96.2    0.3014\n",
            " 0.1625 10445   783 416 259      89.9         38.4         93.0    31.4  24.9   96.2    0.3017\n",
            " 0.1630 10451   777 417 258      90.0         38.2         93.1    31.3  24.9   96.2    0.3018\n",
            " 0.1635 10458   770 420 255      90.0         37.8         93.1    30.9  24.9   96.1    0.3000\n",
            " 0.1640 10464   764 421 254      90.0         37.6         93.2    30.8  25.0   96.1    0.3001\n",
            " 0.1645 10468   760 423 252      90.1         37.3         93.2    30.6  24.9   96.1    0.2988\n",
            " 0.1650 10475   753 426 249      90.1         36.9         93.3    30.2  24.9   96.1    0.2970\n",
            " 0.1655 10480   748 428 247      90.1         36.6         93.3    29.9  24.8   96.1    0.2958\n",
            " 0.1660 10486   742 428 247      90.2         36.6         93.4    30.0  25.0   96.1    0.2969\n",
            " 0.1665 10490   738 429 246      90.2         36.4         93.4    29.9  25.0   96.1    0.2966\n",
            " 0.1670 10493   735 431 244      90.2         36.1         93.5    29.6  24.9   96.1    0.2950\n",
            " 0.1675 10499   729 432 243      90.2         36.0         93.5    29.5  25.0   96.0    0.2951\n",
            " 0.1680 10501   727 432 243      90.3         36.0         93.5    29.5  25.1   96.0    0.2954\n",
            " 0.1685 10506   722 433 242      90.3         35.9         93.6    29.4  25.1   96.0    0.2953\n",
            " 0.1690 10511   717 434 241      90.3         35.7         93.6    29.3  25.2   96.0    0.2952\n",
            " 0.1695 10519   709 435 240      90.4         35.6         93.7    29.2  25.3   96.0    0.2956\n",
            " 0.1700 10522   706 436 239      90.4         35.4         93.7    29.1  25.3   96.0    0.2951\n",
            " 0.1705 10526   702 438 237      90.4         35.1         93.7    28.9  25.2   96.0    0.2937\n",
            " 0.1710 10535   693 439 236      90.5         35.0         93.8    28.8  25.4   96.0    0.2943\n",
            " 0.1715 10541   687 440 235      90.5         34.8         93.9    28.7  25.5   96.0    0.2943\n",
            " 0.1720 10547   681 441 234      90.6         34.7         93.9    28.6  25.6   96.0    0.2943\n",
            " 0.1725 10553   675 441 234      90.6         34.7         94.0    28.7  25.7   96.0    0.2955\n",
            " 0.1730 10556   672 441 234      90.6         34.7         94.0    28.7  25.8   96.0    0.2960\n",
            " 0.1735 10561   667 443 232      90.7         34.4         94.1    28.4  25.8   96.0    0.2948\n",
            " 0.1740 10563   665 445 230      90.7         34.1         94.1    28.2  25.7   96.0    0.2930\n",
            " 0.1745 10569   659 448 227      90.7         33.6         94.1    27.8  25.6   95.9    0.2908\n",
            " 0.1750 10576   652 450 225      90.7         33.3         94.2    27.5  25.7   95.9    0.2899\n",
            " 0.1755 10580   648 451 224      90.8         33.2         94.2    27.4  25.7   95.9    0.2896\n",
            " 0.1760 10581   647 452 223      90.8         33.0         94.2    27.3  25.6   95.9    0.2887\n",
            " 0.1765 10585   643 452 223      90.8         33.0         94.3    27.3  25.8   95.9    0.2894\n",
            " 0.1770 10589   639 453 222      90.8         32.9         94.3    27.2  25.8   95.9    0.2891\n",
            " 0.1775 10594   634 454 221      90.9         32.7         94.4    27.1  25.8   95.9    0.2889\n",
            " 0.1780 10597   631 455 220      90.9         32.6         94.4    27.0  25.9   95.9    0.2883\n",
            " 0.1785 10604   624 457 218      90.9         32.3         94.4    26.7  25.9   95.9    0.2874\n",
            " 0.1790 10612   616 458 217      91.0         32.1         94.5    26.7  26.1   95.9    0.2878\n",
            " 0.1795 10615   613 458 217      91.0         32.1         94.5    26.7  26.1   95.9    0.2884\n",
            " 0.1800 10621   607 460 215      91.0         31.9         94.6    26.4  26.2   95.8    0.2872\n",
            " 0.1805 10626   602 460 215      91.1         31.9         94.6    26.5  26.3   95.9    0.2882\n",
            " 0.1810 10631   597 461 214      91.1         31.7         94.7    26.4  26.4   95.8    0.2880\n",
            " 0.1815 10642   586 462 213      91.2         31.6         94.8    26.3  26.7   95.8    0.2890\n",
            " 0.1820 10645   583 465 210      91.2         31.1         94.8    25.9  26.5   95.8    0.2861\n",
            " 0.1825 10648   580 466 209      91.2         31.0         94.8    25.8  26.5   95.8    0.2855\n",
            " 0.1830 10652   576 471 204      91.2         30.2         94.9    25.1  26.2   95.8    0.2804\n",
            " 0.1835 10655   573 474 201      91.2         29.8         94.9    24.7  26.0   95.7    0.2774\n",
            " 0.1840 10663   565 474 201      91.3         29.8         95.0    24.7  26.2   95.7    0.2790\n",
            " 0.1845 10667   561 474 201      91.3         29.8         95.0    24.8  26.4   95.7    0.2797\n",
            " 0.1850 10671   557 474 201      91.3         29.8         95.0    24.8  26.5   95.7    0.2805\n",
            " 0.1855 10674   554 474 201      91.4         29.8         95.1    24.8  26.6   95.7    0.2811\n",
            " 0.1860 10676   552 474 201      91.4         29.8         95.1    24.9  26.7   95.7    0.2815\n",
            " 0.1865 10679   549 474 201      91.4         29.8         95.1    24.9  26.8   95.8    0.2821\n",
            " 0.1870 10683   545 474 201      91.4         29.8         95.1    24.9  26.9   95.8    0.2829\n",
            " 0.1875 10687   541 475 200      91.5         29.6         95.2    24.8  27.0   95.7    0.2825\n",
            " 0.1880 10691   537 476 199      91.5         29.5         95.2    24.7  27.0   95.7    0.2821\n",
            " 0.1885 10694   534 477 198      91.5         29.3         95.2    24.6  27.0   95.7    0.2814\n",
            " 0.1890 10696   532 478 197      91.5         29.2         95.3    24.4  27.0   95.7    0.2806\n",
            " 0.1895 10699   529 479 196      91.5         29.0         95.3    24.3  27.0   95.7    0.2800\n",
            " 0.1900 10701   527 479 196      91.5         29.0         95.3    24.3  27.1   95.7    0.2804\n",
            " 0.1905 10703   525 480 195      91.6         28.9         95.3    24.2  27.1   95.7    0.2796\n",
            " 0.1910 10708   520 481 194      91.6         28.7         95.4    24.1  27.2   95.7    0.2793\n",
            " 0.1915 10711   517 481 194      91.6         28.7         95.4    24.1  27.3   95.7    0.2799\n",
            " 0.1920 10712   516 482 193      91.6         28.6         95.4    24.0  27.2   95.7    0.2789\n",
            " 0.1925 10713   515 484 191      91.6         28.3         95.4    23.7  27.1   95.7    0.2766\n",
            " 0.1930 10719   509 484 191      91.7         28.3         95.5    23.8  27.3   95.7    0.2778\n",
            " 0.1935 10721   507 485 190      91.7         28.1         95.5    23.6  27.3   95.7    0.2770\n",
            " 0.1940 10730   498 486 189      91.7         28.0         95.6    23.6  27.5   95.7    0.2775\n",
            " 0.1945 10732   496 486 189      91.7         28.0         95.6    23.6  27.6   95.7    0.2779\n",
            " 0.1950 10733   495 486 189      91.8         28.0         95.6    23.6  27.6   95.7    0.2781\n",
            " 0.1955 10740   488 486 189      91.8         28.0         95.7    23.7  27.9   95.7    0.2796\n",
            " 0.1960 10748   480 486 189      91.9         28.0         95.7    23.7  28.3   95.7    0.2812\n",
            " 0.1965 10752   476 487 188      91.9         27.9         95.8    23.6  28.3   95.7    0.2808\n",
            " 0.1970 10758   470 490 185      91.9         27.4         95.8    23.2  28.2   95.6    0.2782\n",
            " 0.1975 10765   463 490 185      92.0         27.4         95.9    23.3  28.5   95.6    0.2797\n",
            " 0.1980 10766   462 490 185      92.0         27.4         95.9    23.3  28.6   95.6    0.2799\n",
            " 0.1985 10770   458 490 185      92.0         27.4         95.9    23.3  28.8   95.6    0.2807\n",
            " 0.1990 10774   454 491 184      92.1         27.3         96.0    23.2  28.8   95.6    0.2803\n",
            " 0.1995 10775   453 491 184      92.1         27.3         96.0    23.2  28.9   95.6    0.2805\n",
            " 0.2100 10841   387 511 164      92.5         24.3         96.6    20.8  29.8   95.5    0.2675\n",
            " 0.3100 11151    77 630  45      94.1          6.7         99.3     6.0  36.9   94.7    0.1129\n",
            " 0.4100 11214    14 660  15      94.3          2.2         99.9     2.1  51.7   94.4    0.0426\n",
            " 0.5100 11227     1 670   5      94.4          0.7        100.0     0.7  83.3   94.4    0.0147\n",
            " 0.6100 11228     0 670   5      94.4          0.7        100.0     0.7 100.0   94.4    0.0147\n",
            " 0.7100 11228     0 670   5      94.4          0.7        100.0     0.7 100.0   94.4    0.0147\n",
            " 0.8100 11228     0 670   5      94.4          0.7        100.0     0.7 100.0   94.4    0.0147\n",
            " 0.9100 11228     0 673   2      94.3          0.3        100.0     0.3 100.0   94.3    0.0059\n"
          ]
        }
      ],
      "source": [
        "cutoffs_v1 = [round(x, 4) for x in np.arange(0, 0.2, 0.0005)]  # 0 ~ 0.2 間隔 0.02\n",
        "cutoffs_v1 += [round(x, 4) for x in np.arange(0.21, 1.01, 0.1)]  # 0.3 ~ 1 間隔 0.1\n",
        "\n",
        "cutoff_table_zoom = cutoff_analysis(final_model, test_df, target_col=\"is_fraud\", cutoffs=cutoffs_v1)\n",
        "print(cutoff_table_zoom.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "\n",
        "def plot_multiple_roc(models, model_names, test_df, target_col=\"is_fraud\"):\n",
        "    \"\"\"\n",
        "    models: list of fitted statsmodels.Logit models\n",
        "    model_names: list of strings\n",
        "    test_df: test dataframe\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.style.use('seaborn-v0_8-darkgrid')\n",
        "    plt.rcParams['axes.facecolor'] = '#f0f0f0'\n",
        "\n",
        "    # 隨機機率線\n",
        "    plt.plot([0, 1], [0, 1], color='gray', linestyle='-', label='Random Chance')\n",
        "\n",
        "    auc_values = {}\n",
        "\n",
        "    for model, name in zip(models, model_names):\n",
        "        # 取出模型變數\n",
        "        vars_used = model.params.index.drop(\"const\")\n",
        "        X_test = sm.add_constant(test_df[vars_used])\n",
        "\n",
        "        # 計算預測機率\n",
        "        y_true = test_df[target_col]\n",
        "        y_score = model.predict(X_test)\n",
        "\n",
        "        # ROC 曲線\n",
        "        fpr, tpr, _ = roc_curve(y_true, y_score)\n",
        "        auc_val = roc_auc_score(y_true, y_score)\n",
        "        auc_values[name] = auc_val\n",
        "\n",
        "        plt.plot(fpr, tpr, label=f\"{name} (AUC={auc_val:.4f})\")\n",
        "\n",
        "    plt.title(\"Comparing Independent Variables and Final Stepwise Model with ROC Curve\", fontsize=13, fontweight='bold')\n",
        "    plt.xlabel(\"1 - Specificity\")\n",
        "    plt.ylabel(\"Sensitivity\")\n",
        "    plt.legend(loc=\"lower right\", frameon=True)\n",
        "    plt.show()\n",
        "\n",
        "    # 印出 AUC summary\n",
        "    #print(\"AUC values:\")\n",
        "    #for name, val in auc_values.items():\n",
        "    #    print(f\"{name}: {val:.4f}\")\n",
        "\n",
        "# 假設你已經有以下模型：\n",
        "# model_1 = sm.Logit(y_train, sm.add_constant(X_train[[\"X13\"]])).fit(disp=False)\n",
        "# model_2 = sm.Logit(y_train, sm.add_constant(X_train[[\"X17\"]])).fit(disp=False)\n",
        "# final_model = ...\n",
        "\n",
        "# 範例呼叫\n",
        "plot_multiple_roc(\n",
        "    #models=[model_0, model_2, model_3,model_4, final_model],\n",
        "    models=[final_model],\n",
        "    model_names=[\"Final Model\"],\n",
        "    test_df=test_df,\n",
        "    target_col=\"is_fraud\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 04-4(d) 對照圖 amount 取log only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "def plot_multiple_roc(models, model_names, test_df, target_col=\"is_fraud\"):\n",
        "    \"\"\"\n",
        "    models: list of fitted statsmodels.Logit models\n",
        "    model_names: list of strings\n",
        "    test_df: test dataframe\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.style.use('seaborn-v0_8-darkgrid')\n",
        "    plt.rcParams['axes.facecolor'] = '#f0f0f0'\n",
        "\n",
        "    # 隨機機率線\n",
        "    plt.plot([0, 1], [0, 1], color='gray', linestyle='-', label='Random Chance')\n",
        "\n",
        "    auc_values = {}\n",
        "\n",
        "    for model, name in zip(models, model_names):\n",
        "        # 取出模型變數\n",
        "        vars_used = model.params.index.drop(\"const\")\n",
        "        X_test = sm.add_constant(test_df[vars_used])\n",
        "\n",
        "        # 計算預測機率\n",
        "        y_true = test_df[target_col]\n",
        "        y_score = model.predict(X_test)\n",
        "\n",
        "        # ROC 曲線\n",
        "        fpr, tpr, _ = roc_curve(y_true, y_score)\n",
        "        auc_val = roc_auc_score(y_true, y_score)\n",
        "        auc_values[name] = auc_val\n",
        "\n",
        "        plt.plot(fpr, tpr, label=f\"{name} (AUC={auc_val:.4f})\")\n",
        "\n",
        "    plt.title(\"Comparing Independent Variables and Final Stepwise Model with ROC Curve\", fontsize=13, fontweight='bold')\n",
        "    plt.xlabel(\"1 - Specificity\")\n",
        "    plt.ylabel(\"Sensitivity\")\n",
        "    plt.legend(loc=\"lower right\", frameon=True)\n",
        "    plt.show()\n",
        "\n",
        "    # 印出 AUC summary\n",
        "    print(\"AUC values:\")\n",
        "    for name, val in auc_values.items():\n",
        "        print(f\"{name}: {val:.4f}\")\n",
        "\n",
        "# 假設你已經有以下模型：\n",
        "# model_1 = sm.Logit(y_train, sm.add_constant(X_train[[\"X13\"]])).fit(disp=False)\n",
        "# model_2 = sm.Logit(y_train, sm.add_constant(X_train[[\"X17\"]])).fit(disp=False)\n",
        "# final_model = ...\n",
        "\n",
        "# 範例呼叫\n",
        "plot_multiple_roc(\n",
        "    models=[model_0, model_2, model_3, model_4, final_model],\n",
        "    model_names=[\"Model_1 (amount)\", \"Model_2 (Zip)\", \"Model_3 (longitude)\",\"Model_4 (mercahnt_id)\", \"Final Model (26vars)\"],\n",
        "    test_df=test_df,\n",
        "    target_col=\"is_fraud\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 04-5(a)_ Model BY SkLEARN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "included=[\n",
        " 'amount',\n",
        " 'use_chip_Swipe Transaction',\n",
        " 'transaction_id',\n",
        " 'zip',\n",
        " 'longitude',\n",
        " 'credit_limit',\n",
        " 'merchant_id',\n",
        " 'num_credit_cards',\n",
        " 'errors_missing_flag',\n",
        " 'yearly_income',\n",
        " 'mcc_code',\n",
        " 'has_chip_YES',\n",
        " 'latitude',\n",
        " 'client_id_x',\n",
        " 'credit_score',\n",
        " 'retirement_age',\n",
        " 'total_debt',\n",
        " 'card_brand_Mastercard',\n",
        " 'card_id',\n",
        " 'current_age',\n",
        " 'gender_Male',\n",
        " 'cvv',\n",
        " 'num_cards_issued',\n",
        " 'card_type_Debit'\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.stats import chi2\n",
        "import statsmodels.api as sm\n",
        "\n",
        "\n",
        "def _compute_cov_and_se(model, X_scaled):\n",
        "    n = X_scaled.shape[0]\n",
        "    X_design = np.hstack([np.ones((n, 1)), X_scaled])\n",
        "    p = model.predict_proba(X_scaled)[:, 1]\n",
        "    W = p * (1 - p)\n",
        "\n",
        "    # 增加小常數以避免奇異矩陣\n",
        "    XtWX = X_design.T @ (W[:, None] * X_design) + np.eye(X_design.shape[1]) * 1e-8\n",
        "\n",
        "    try:\n",
        "        cov = np.linalg.inv(XtWX)\n",
        "    except np.linalg.LinAlgError:\n",
        "        cov = np.linalg.pinv(XtWX)\n",
        "\n",
        "    se = np.sqrt(np.maximum(np.diag(cov), 0.0))\n",
        "    return cov, se\n",
        "\n",
        "\n",
        "def compute_not_in_eq(y_train, full_X_train_df, included_vars, excluded_vars):\n",
        "    results = []\n",
        "    eps = 1e-9\n",
        "\n",
        "    # --- base model ---\n",
        "    if len(included_vars) == 0:\n",
        "        p_mean = np.clip(np.mean(y_train), eps, 1 - eps)\n",
        "        base_ll = np.sum(y_train * np.log(p_mean) + (1 - y_train) * np.log(1 - p_mean))\n",
        "    else:\n",
        "        try:\n",
        "            X_incl_sm = sm.add_constant(full_X_train_df[included_vars])\n",
        "            base_model = sm.Logit(y_train, X_incl_sm).fit(disp=False)\n",
        "            base_ll = base_model.llf\n",
        "        except Exception:\n",
        "            X_incl_sk = full_X_train_df[included_vars].astype(float)\n",
        "            lr_base = LogisticRegression(solver=\"liblinear\", max_iter=2000)\n",
        "            lr_base.fit(X_incl_sk, y_train)\n",
        "            p_pred_base = np.clip(lr_base.predict_proba(X_incl_sk)[:, 1], eps, 1 - eps)\n",
        "            base_ll = np.sum(y_train * np.log(p_pred_base) + (1 - y_train) * np.log(1 - p_pred_base))\n",
        "\n",
        "    # --- test each excluded var ---\n",
        "    for var in excluded_vars:\n",
        "        try:\n",
        "            full_vars = included_vars + [var] if len(included_vars) > 0 else [var]\n",
        "            X_full_df = full_X_train_df[full_vars].copy()\n",
        "\n",
        "            try:\n",
        "                X_full_sm = sm.add_constant(X_full_df)\n",
        "                full_model = sm.Logit(y_train, X_full_sm).fit(disp=False)\n",
        "                full_ll = full_model.llf\n",
        "            except Exception:\n",
        "                X_full_sk = X_full_df.astype(float)\n",
        "                lr_full = LogisticRegression(solver=\"liblinear\", max_iter=2000)\n",
        "                lr_full.fit(X_full_sk, y_train)\n",
        "                p_pred_full = np.clip(lr_full.predict_proba(X_full_sk)[:, 1], eps, 1 - eps)\n",
        "                full_ll = np.sum(y_train * np.log(p_pred_full) + (1 - y_train) * np.log(1 - p_pred_full))\n",
        "\n",
        "            lr_stat = 2.0 * (full_ll - base_ll)\n",
        "            pval = chi2.sf(lr_stat, df=1)\n",
        "\n",
        "            results.append({\n",
        "                \"Independent Variable\": var,\n",
        "                \"Score Statistic (LRT)\": round(float(lr_stat), 3),\n",
        "                \"Significance\": round(float(pval), 4)\n",
        "            })\n",
        "        except KeyError:\n",
        "            results.append({\n",
        "                \"Independent Variable\": var,\n",
        "                \"Score Statistic (LRT)\": None,\n",
        "                \"Significance\": None\n",
        "            })\n",
        "        except Exception:\n",
        "            results.append({\n",
        "                \"Independent Variable\": var,\n",
        "                \"Score Statistic (LRT)\": None,\n",
        "                \"Significance\": None\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "\n",
        "def sklearn_logit_with_k(train_df, test_df, dep_var, included, k=314657018):\n",
        "    \"\"\"\n",
        "    主函式（修正版）：確保整體模型 LRT 正向（SPSS 一致），\n",
        "    Change_from_Base 為改善幅度，Change_pvalue 為整體模型顯著性。\n",
        "    \"\"\"\n",
        "    # === 1. 準備資料 ===\n",
        "    y_train = train_df[dep_var].astype(int).values\n",
        "    y_test = test_df[dep_var].astype(int).values\n",
        "\n",
        "    if k == 314657018:\n",
        "        selected_vars = included\n",
        "    else:\n",
        "        selected_vars = included[:k]\n",
        "\n",
        "    X_train = train_df[selected_vars].astype(float).copy()\n",
        "    X_test = test_df[selected_vars].astype(float).copy()\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # === 2. 建立模型 ===\n",
        "    model = LogisticRegression(solver=\"liblinear\", max_iter=2000, class_weight=\"balanced\")\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # === 3. 模型擬合度 ===\n",
        "    eps = 1e-9\n",
        "    p_pred = np.clip(model.predict_proba(X_train_scaled)[:, 1], eps, 1 - eps)\n",
        "    full_ll = np.sum(y_train * np.log(p_pred) + (1 - y_train) * np.log(1 - p_pred))\n",
        "\n",
        "    # Null model (只有截距)\n",
        "    p_mean = np.clip(np.mean(y_train), eps, 1 - eps)\n",
        "    base_ll = np.sum(y_train * np.log(p_mean) + (1 - y_train) * np.log(1 - p_mean))\n",
        "\n",
        "    # 注意：SPSS 用 (-2LL_null) - (-2LL_model)\n",
        "    delta_minus2LL = -2 * base_ll - (-2 * full_ll)\n",
        "    LR_chi2 = max(delta_minus2LL, 0)\n",
        "    LR_df = len(selected_vars)\n",
        "    LR_pvalue = float(chi2.sf(LR_chi2, df=LR_df))\n",
        "\n",
        "    n = len(y_train)\n",
        "    r2_cox_snell = 1 - np.exp(-LR_chi2 / n)\n",
        "    denom = 1 - np.exp(-2 * base_ll / n)\n",
        "    r2_nagelkerke = r2_cox_snell / denom if denom != 0 else np.nan\n",
        "    r2_mcfadden = 1 - (full_ll / base_ll)\n",
        "\n",
        "    overall_fit = pd.DataFrame({\n",
        "        \"Measure\": [\n",
        "            \"-2 Log Likelihood (−2LL) value\",\n",
        "            \"Cox and Snell R2\",\n",
        "            \"Nagelkerke R2\",\n",
        "            \"Pseudo R2 (McFadden)\",\n",
        "            \"Likelihood Ratio χ2\",\n",
        "        ],\n",
        "        \"Value\": [\n",
        "            round(-2 * full_ll, 3),\n",
        "            round(r2_cox_snell, 3),\n",
        "            round(r2_nagelkerke, 3),\n",
        "            round(r2_mcfadden, 3),\n",
        "            round(LR_chi2, 3)\n",
        "        ],\n",
        "        \"Change_from_Base\": [round(LR_chi2, 3), \"\", \"\", \"\", \"\"],\n",
        "        \"Change_pvalue\": [f\"{LR_pvalue:.6f}\", \"\", \"\", \"\", \"\"]\n",
        "    })\n",
        "\n",
        "    # === 4. 係數表 ===\n",
        "    coef = model.coef_[0]\n",
        "    intercept = model.intercept_[0]\n",
        "    cov, se_all = _compute_cov_and_se(model, X_train_scaled)\n",
        "    se_beta = se_all[1:]\n",
        "    se_intercept = se_all[0]\n",
        "\n",
        "    wald_vals, pvals = [], []\n",
        "    for b, se in zip(coef, se_beta):\n",
        "        if se == 0 or np.isnan(se):\n",
        "            wald_vals.append(np.nan)\n",
        "            pvals.append(np.nan)\n",
        "        else:\n",
        "            w = (b / se) ** 2\n",
        "            wald_vals.append(w)\n",
        "            pvals.append(float(chi2.sf(w, df=1)))\n",
        "\n",
        "    intercept_wald = (intercept / se_intercept) ** 2 if se_intercept != 0 else np.nan\n",
        "    intercept_pval = float(chi2.sf(intercept_wald, df=1)) if not np.isnan(intercept_wald) else np.nan\n",
        "\n",
        "    coef_df = pd.DataFrame({\n",
        "        \"Independent Variable\": [\"(Intercept)\"] + selected_vars,\n",
        "        \"B\": np.round([intercept] + coef.tolist(), 6),\n",
        "        \"Std. Error\": [round(float(x), 6) for x in se_all],\n",
        "        \"Wald\": [round(float(intercept_wald), 3)] +\n",
        "                [round(float(x), 3) if not np.isnan(x) else None for x in wald_vals],\n",
        "        \"df\": [1] * (len(coef) + 1),\n",
        "        \"Sig.\": [round(float(intercept_pval), 6)] +\n",
        "                [round(float(x), 6) if not np.isnan(x) else None for x in pvals],\n",
        "        \"Exp(B)\": np.round(np.exp([intercept] + coef.tolist()), 6)\n",
        "    })\n",
        "\n",
        "    # === 5. Variables not in Equation ===\n",
        "    excluded_vars = [v for v in included if v not in selected_vars]\n",
        "    not_in_eq_df = compute_not_in_eq(y_train, train_df, selected_vars, excluded_vars)\n",
        "\n",
        "    return overall_fit, coef_df, not_in_eq_df, model, scaler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "overall_fit, coef_df, not_in_eq_df, skmodel, scaler = sklearn_logit_with_k(\n",
        "    train_df, test_df, dep_var=\"is_fraud\", included=included, k=18\n",
        ")\n",
        "\n",
        "print(\"=== Overall Model Fit ===\")\n",
        "print(overall_fit)\n",
        "print(\"\\n=== Variables in the Equation ===\")\n",
        "print(coef_df)\n",
        "print(\"\\n=== Variables Not in the Equation ===\")\n",
        "print(not_in_eq_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "def cutoff_analysis_sklearn(model, scaler, df, included_vars, target_col=\"is_fraud\", cutoffs=None):\n",
        "    \"\"\"\n",
        "    對 sklearn logistic regression 模型做 cutoff 分析\n",
        "    （模擬 SPSS Table 8.7 結果）\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    model : sklearn.linear_model.LogisticRegression\n",
        "        已訓練完成的 logistic regression 模型。\n",
        "    scaler : sklearn.preprocessing.StandardScaler\n",
        "        與訓練模型時一致的 scaler。\n",
        "    df : pandas.DataFrame\n",
        "        包含所有預測變數與 target 欄位的資料集。\n",
        "    included_vars : list\n",
        "        模型使用的變數名稱（與訓練時一致）。\n",
        "    target_col : str, default='is_fraud'\n",
        "        真實標籤欄位名稱。\n",
        "    cutoffs : list or array, optional\n",
        "        要分析的 cutoff 清單，預設為 np.arange(0, 1.01, 0.05)。\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        包含各 cutoff 的混淆矩陣與各種評估指標。\n",
        "    \"\"\"\n",
        "    if cutoffs is None:\n",
        "        cutoffs = np.arange(0, 1.01, 0.05)\n",
        "\n",
        "    # === 準備資料 ===\n",
        "    y_true = df[target_col].astype(int).values\n",
        "\n",
        "    # 保證與 scaler 的 feature_names 對齊\n",
        "    feature_order = [col for col in scaler.feature_names_in_ if col in df.columns]\n",
        "    missing = [col for col in scaler.feature_names_in_ if col not in df.columns]\n",
        "    if missing:\n",
        "        print(f\"⚠️ Warning: 以下欄位在資料集中缺失，將被忽略：{missing}\")\n",
        "\n",
        "    X = df[feature_order].astype(float).copy()\n",
        "    X_scaled = scaler.transform(X)\n",
        "\n",
        "    # === 預測機率 ===\n",
        "    y_prob = model.predict_proba(X_scaled)[:, 1]\n",
        "\n",
        "    rows = []\n",
        "    for cutoff in cutoffs:\n",
        "        y_pred = (y_prob >= cutoff).astype(int)\n",
        "\n",
        "        # confusion_matrix 的排列是 [[TN, FP], [FN, TP]]\n",
        "        cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
        "\n",
        "        if cm.shape == (2, 2):\n",
        "            TN, FP, FN, TP = cm.ravel()\n",
        "        else:\n",
        "            TN = FP = FN = TP = 0\n",
        "\n",
        "        total = TP + TN + FP + FN\n",
        "        accuracy = (TP + TN) / total if total > 0 else np.nan\n",
        "        sensitivity = TP / (TP + FN) if (TP + FN) > 0 else np.nan\n",
        "        specificity = TN / (TN + FP) if (TN + FP) > 0 else np.nan\n",
        "        youden = (sensitivity + specificity - 1) if (\n",
        "            not np.isnan(sensitivity) and not np.isnan(specificity)\n",
        "        ) else np.nan\n",
        "        ppv = TP / (TP + FP) if (TP + FP) > 0 else np.nan\n",
        "        npv = TN / (TN + FN) if (TN + FN) > 0 else np.nan\n",
        "        f1 = 2 * TP / (2 * TP + FP + FN) if (2 * TP + FP + FN) > 0 else np.nan\n",
        "\n",
        "        rows.append({\n",
        "            \"Cutoff\": round(cutoff, 2),\n",
        "            \"TN\": TN, \"FP\": FP, \"FN\": FN, \"TP\": TP,\n",
        "            \"Accuracy (%)\": round(accuracy * 100, 2) if not np.isnan(accuracy) else \"NC\",\n",
        "            \"Sensitivity (%)\": round(sensitivity * 100, 2) if not np.isnan(sensitivity) else \"NC\",\n",
        "            \"Specificity (%)\": round(specificity * 100, 2) if not np.isnan(specificity) else \"NC\",\n",
        "            \"Youden Index\": round(youden, 4) if not np.isnan(youden) else \"NC\",\n",
        "            \"PPV (%)\": round(ppv * 100, 2) if not np.isnan(ppv) else \"NC\",\n",
        "            \"NPV (%)\": round(npv * 100, 2) if not np.isnan(npv) else \"NC\",\n",
        "            \"F1 Score\": round(f1, 4) if not np.isnan(f1) else \"NC\"\n",
        "        })\n",
        "\n",
        "    df_result = pd.DataFrame(rows)\n",
        "    return df_result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 範例執行 ===\n",
        "cutoff_table_all = cutoff_analysis_sklearn(\n",
        "    skmodel,            # 來自 sklearn_logit_with_k 的 model\n",
        "    scaler,           # 對應的標準化器\n",
        "    test_df,          # 測試資料\n",
        "    included[:3],     # 與模型相同的自變數\n",
        "    target_col=\"is_fraud\"\n",
        "    \n",
        ")\n",
        "\n",
        "print(cutoff_table_all.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 04-5(b)_對應 sklearn 版本的多模型 ROC 繪圖"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "def plot_multiple_roc_sklearn(models, scalers, model_names, test_df, included_var_sets, target_col=\"is_fraud\"):\n",
        "    \"\"\"\n",
        "    對應 sklearn LogisticRegression 版本的多模型 ROC 圖比較\n",
        "    \n",
        "    models: list of fitted sklearn models\n",
        "    scalers: list of對應的 StandardScaler（每個模型一個）\n",
        "    model_names: list of模型名稱\n",
        "    test_df: 測試資料\n",
        "    included_var_sets: list of各模型使用的變數名稱（list of list）\n",
        "    target_col: 目標變數名稱\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.style.use('seaborn-v0_8-darkgrid')\n",
        "    plt.rcParams['axes.facecolor'] = '#f0f0f0'\n",
        "\n",
        "    # 隨機機率線\n",
        "    plt.plot([0, 1], [0, 1], color='gray', linestyle='-', label='Random Chance')\n",
        "\n",
        "    auc_values = {}\n",
        "\n",
        "    for model, scaler, name, vars_used in zip(models, scalers, model_names, included_var_sets):\n",
        "        # 準備測試資料\n",
        "        X_test = test_df[vars_used].astype(float)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "        y_true = test_df[target_col].astype(int)\n",
        "\n",
        "        # 模型預測機率\n",
        "        y_score = model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "        # ROC 曲線與 AUC\n",
        "        fpr, tpr, _ = roc_curve(y_true, y_score)\n",
        "        auc_val = roc_auc_score(y_true, y_score)\n",
        "        auc_values[name] = auc_val\n",
        "\n",
        "        plt.plot(fpr, tpr, lw=2, label=f\"{name} (AUC={auc_val:.4f})\")\n",
        "\n",
        "    plt.title(\"Comparing Independent Variables and Final Stepwise Model with ROC Curve\", fontsize=13, fontweight='bold')\n",
        "    plt.xlabel(\"1 - Specificity (False Positive Rate)\")\n",
        "    plt.ylabel(\"Sensitivity (True Positive Rate)\")\n",
        "    plt.legend(loc=\"lower right\", frameon=True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 印出 AUC Summary\n",
        "    print(\"AUC values:\")\n",
        "    for name, val in auc_values.items():\n",
        "        print(f\"{name}: {val:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 範例：建立四個不同變數集的模型\n",
        "skmodel_1_info = sklearn_logit_with_k(train_df, test_df, dep_var=\"is_fraud\", included=included, k=1)\n",
        "skmodel_2_info = sklearn_logit_with_k(train_df, test_df, dep_var=\"is_fraud\", included=included, k=2)\n",
        "skmodel_3_info = sklearn_logit_with_k(train_df, test_df, dep_var=\"is_fraud\", included=included, k=3)\n",
        "skmodel_final_info = sklearn_logit_with_k(train_df, test_df, dep_var=\"is_fraud\", included=included, k=18)\n",
        "\n",
        "\n",
        "# 解包結果 (overall_fit, coef_df, not_in_eq_df, model, scaler)\n",
        "_, _, _, skmodel_1, skscaler_1 = skmodel_1_info\n",
        "_, _, _, skmodel_2, skscaler_2 = skmodel_2_info\n",
        "_, _, _, skmodel_3, skscaler_3 = skmodel_3_info\n",
        "_, _, _, skmodel_final, skscaler_final = skmodel_final_info\n",
        "\n",
        "# 對應的變數集\n",
        "vars_1 = included[:1]\n",
        "vars_2 = included[:2]\n",
        "vars_3 = included[:3]\n",
        "vars_final = included[:18]  # 全部變數"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_multiple_roc_sklearn(\n",
        "    models=[skmodel_1, skmodel_2, skmodel_3, skmodel_final],\n",
        "    scalers=[skscaler_1, skscaler_2, skscaler_3, skscaler_final],\n",
        "    model_names=[\"Model_1 +amount\", \n",
        "                 \"Model_2 +use_chip_Swipe Tran.\", \n",
        "                 \"Model_3 +transaction_id\", \n",
        "                 \"Final Model_18vars\"],\n",
        "    test_df=test_df,\n",
        "    included_var_sets=[vars_1, vars_2, vars_3, vars_final],\n",
        "    target_col=\"is_fraud\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 計算 included 長度\n",
        "n_vars = 18\n",
        "print(f\"共有 {n_vars} 個變數可供建模\")\n",
        "\n",
        "# 建立模型與 scaler 的容器\n",
        "models = []\n",
        "scalers = []\n",
        "model_names = []\n",
        "included_var_sets = []\n",
        "\n",
        "# 自動迴圈建立模型\n",
        "for k in range(1, n_vars):\n",
        "    model_info = sklearn_logit_with_k(\n",
        "        train_df, test_df, dep_var=\"is_fraud\", included=included, k=k\n",
        "    )\n",
        "    _, _, _, model, scaler = model_info\n",
        "    \n",
        "    models.append(model)\n",
        "    scalers.append(scaler)\n",
        "    included_var_sets.append(included[:k])\n",
        "    model_names.append(f\"Model_{k}\")\n",
        "\n",
        "print(f\"總共建立 {n_vars} 個模型（含最終模型）\")\n",
        "\n",
        "# 🔹 繪製多模型 ROC 比較圖\n",
        "plot_multiple_roc_sklearn(\n",
        "    models=models,\n",
        "    scalers=scalers,\n",
        "    model_names=model_names,\n",
        "    test_df=test_df,\n",
        "    included_var_sets=included_var_sets,\n",
        "    target_col=\"is_fraud\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 05_地區分析"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "驚人發現 \"2018-01-01 00:00:00\" 到 \"2019-10-31 23:59:59\"的交易資料中，詐騙高度集中在網路交易、境外交易、ohio州"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df=train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(df[df['is_fraud']==0]['zip'], df[df['is_fraud']==0]['al'],\n",
        "            s=5, alpha=0.4, label='Normal', color='blue')\n",
        "plt.scatter(df[df['is_fraud']==1]['zip'], df[df['is_fraud']==1]['al'],\n",
        "            s=10, alpha=0.7, label='Fraud', color='red')\n",
        "plt.xlabel('zip')\n",
        "plt.ylabel('al')\n",
        "plt.title('Geographical Distribution of Fraud vs Normal Transactions')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import folium\n",
        "\n",
        "# 建立地圖中心（以資料平均位置為中心）\n",
        "center_lat = df['latitude'].mean()\n",
        "center_lon = df['longitude'].mean()\n",
        "\n",
        "m = folium.Map(location=[center_lat, center_lon], zoom_start=6)\n",
        "\n",
        "# 加入交易點\n",
        "for _, row in df.iterrows():\n",
        "    color = 'red' if row['is_fraud'] == 1 else 'blue'\n",
        "    folium.CircleMarker(\n",
        "        location=[row['latitude'], row['longitude']],\n",
        "        radius=2,\n",
        "        color=color,\n",
        "        fill=True,\n",
        "        fill_opacity=0.5\n",
        "    ).add_to(m)\n",
        "\n",
        "m.save('fraud_map.html')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "df[\"al\"]=df['amount']/df[\"credit_limit\"]\n",
        "\n",
        "fig = plt.figure(figsize=(10,7))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# 各類別上色\n",
        "# 先畫 Fraud\n",
        "ax.scatter(\n",
        "           df[df['is_fraud']==1]['zip'],\n",
        "          \n",
        "           df[df['is_fraud']==1]['yearly_income'],\n",
        "           df[df['is_fraud']==1]['al'],\n",
        "           c='red', label='Fraud', alpha=0.9, s=15)\n",
        "\n",
        "# 再畫 Normal\n",
        "ax.scatter(\n",
        "           df[df['is_fraud']==0]['zip'],\n",
        "           \n",
        "           df[df['is_fraud']==0]['yearly_income'],\n",
        "           df[df['is_fraud']==0]['al'],\n",
        "           c='blue', label='Normal', alpha=0.01, s=10)\n",
        "\n",
        "\n",
        "ax.set_xlabel('zip')\n",
        "ax.set_zlabel('al')\n",
        "ax.set_ylabel('yearly_income')\n",
        "ax.set_title('3D Visualization of Fraud by Location and Income')\n",
        "ax.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[\"al\"]=df['amount']/df[\"credit_limit\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[\"zip\"]==10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fraud = df[df[\"is_fraud\"] == 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.hist(fraud_all['zip'], bins=30, color='skyblue', edgecolor='black')\n",
        "plt.title('Distribution of ZIP Codes in fraud_all')\n",
        "plt.xlabel('ZIP Code')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "virtual",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
