{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67c97b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "#https://drive.google.com/drive/folders/18qV82fNY3IIWu3BRoGqm_LNgJzE8Akbr?usp=drive_link\n",
    "#base_dir = \"/Users/Andypon/10_äº¤å¤§ç ”ç©¶æ‰€/1141_01_æ©Ÿå™¨å­¸ç¿’èˆ‡é‡‘èç§‘æŠ€/data\"\n",
    "base_dir= '/Users/andyw.p.chen/Documents/Project/datasets'\n",
    "#base_dir=  \"c:\\Users\\user\\Downloads\\datasets\"\n",
    "\n",
    "def load_json_to_df(filename: str) -> pd.DataFrame:\n",
    "    file_path = os.path.join(base_dir, filename)\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # å¦‚æœæ˜¯ { \"target\": {id: value, ...} }\n",
    "    if isinstance(data, dict) and len(data) == 1 and isinstance(next(iter(data.values())), dict):\n",
    "        key, inner = next(iter(data.items()))\n",
    "        return pd.DataFrame(list(inner.items()), columns=[\"id\", key])\n",
    "\n",
    "    # dict of scalar\n",
    "    if isinstance(data, dict):\n",
    "        return pd.DataFrame([{\"code\": k, \"desc\": v} for k, v in data.items()])\n",
    "\n",
    "    # list of dict\n",
    "    elif isinstance(data, list):\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported JSON structure in {filename}: {type(data)}\")\n",
    "\n",
    "\n",
    "def load_csv_to_df(filename: str) -> pd.DataFrame:\n",
    "    \"\"\"è®€å– CSV ä¸¦è½‰ç‚º DataFrameã€‚\"\"\"\n",
    "    return pd.read_csv(os.path.join(base_dir, filename))\n",
    "\n",
    "# JSON è³‡æ–™\n",
    "##mcc_codes_df = load_json_to_df(\"mcc_codes.json\")\n",
    "train_fraud_labels_df = load_json_to_df(\"train_fraud_labels.json\")\n",
    "\n",
    "# CSV è³‡æ–™\n",
    "cards_df = load_csv_to_df(\"cards_data.csv\")\n",
    "transactions_df = load_csv_to_df(\"transactions_data.csv\")\n",
    "users_df = load_csv_to_df(\"users_data.csv\")\n",
    "\n",
    "# ç°¡å–®æª¢æŸ¥\n",
    "#print(mcc_codes_df.head())\n",
    "#print(train_fraud_labels_df.head())\n",
    "#print(cards_df.head())\n",
    "#print(transactions_df.head())\n",
    "#print(users_df.apthead())\n",
    "\n",
    "train_fraud_labels_df = train_fraud_labels_df.rename(columns={'id': 'transactions_id'})\n",
    "train_fraud_labels_df = train_fraud_labels_df.rename(columns={'target': 'is_fraud'})\n",
    "\n",
    "cards_df = cards_df.rename(columns={'id':'card_id'})\n",
    "\n",
    "users_df = users_df.rename(columns={'id':'client_id'})\n",
    "\n",
    "transactions_df = transactions_df.rename(columns={'mcc': 'mcc_code'})\n",
    "transactions_df = transactions_df.rename(columns={'id': 'transaction_id'})\n",
    "\n",
    "def add_missing_flags(df: pd.DataFrame, cols: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    åœ¨ DataFrame ä¸­å°æŒ‡å®šæ¬„ä½å»ºç«‹ missing flag æ¬„ä½\n",
    "    flag=1 è¡¨ç¤ºç¼ºå¤±å€¼ï¼Œflag=0 è¡¨ç¤ºéç¼ºå¤±å€¼\n",
    "    \n",
    "    åƒæ•¸\n",
    "    ----\n",
    "    df : pd.DataFrame\n",
    "        è¼¸å…¥çš„è³‡æ–™æ¡†\n",
    "    cols : list\n",
    "        è¦æª¢æŸ¥çš„æ¬„ä½åç¨±æ¸…å–®\n",
    "    \n",
    "    å›å‚³\n",
    "    ----\n",
    "    pd.DataFrame : æ–°çš„è³‡æ–™æ¡† (å«æ–°å¢çš„ flag æ¬„ä½)\n",
    "    \"\"\"\n",
    "    for col in cols:\n",
    "        df[f\"{col}_missing_flag\"] = df[col].isna().astype(int)\n",
    "    return df\n",
    "\n",
    "transactions_df = add_missing_flags(transactions_df, [\"merchant_state\", \"zip\", \"errors\"])\n",
    "\n",
    "##train_fraud_labels_df##\n",
    "train_fraud_labels_df[\"is_fraud\"]=train_fraud_labels_df[\"is_fraud\"].astype(\"category\") \n",
    "train_fraud_labels_df[\"transactions_id\"]=train_fraud_labels_df[\"transactions_id\"].astype(int) #åˆä½µè³‡æ–™éœ€è¦\n",
    "\n",
    "##cards_df##\n",
    "cards_df[\"card_brand\"]=cards_df[\"card_brand\"].astype(\"category\") \n",
    "cards_df[\"card_type\"]=cards_df[\"card_type\"].astype(\"category\")\n",
    "#####ä¸è¦loadé€™è¡Œ cards_df[\"expires\"]=pd.to_datetime(cards_df[\"expires\"], format=\"%m/%Y\")\n",
    "cards_df[\"expires\"] = pd.to_datetime(cards_df[\"expires\"], format=\"%m/%Y\").dt.to_period(\"M\")\n",
    "cards_df[\"has_chip\"]=cards_df[\"has_chip\"].astype(\"category\")\n",
    "\n",
    "cards_df['credit_limit'] = cards_df['credit_limit'].replace(r'[\\$,]', '', regex=True).astype(int)\n",
    "#####ä¸è¦loadé€™è¡Œ cards_df[\"acct_open_date\"]=pd.to_datetime(cards_df[\"acct_open_date\"], format=\"%m/%Y\")\n",
    "cards_df[\"acct_open_date\"] = pd.to_datetime(cards_df[\"acct_open_date\"], format=\"%m/%Y\").dt.to_period(\"M\")\n",
    "#####ä¸è¦loadé€™è¡Œ cards_df[\"year_pin_last_changed\"]=pd.to_datetime(cards_df[\"year_pin_last_changed\"], format=\"%Y\")\n",
    "cards_df[\"year_pin_last_changed\"] = pd.to_datetime(cards_df[\"year_pin_last_changed\"], format=\"%Y\").dt.to_period(\"Y\")\n",
    "cards_df[\"card_on_dark_web\"]=cards_df[\"card_on_dark_web\"].astype(\"category\") \n",
    "\n",
    "##users_df##\n",
    "users_df[\"birth_year\"] = pd.to_datetime(users_df[\"birth_year\"], format=\"%Y\").dt.to_period(\"Y\")\n",
    "users_df[\"birth_month\"] = pd.to_datetime(users_df[\"birth_month\"], format=\"%m\").dt.to_period(\"M\")\n",
    "users_df[\"gender\"]=users_df[\"gender\"].astype(\"category\") \n",
    "users_df['per_capita_income'] = users_df['per_capita_income'].replace(r'[\\$,]', '', regex=True).astype(int)\n",
    "users_df['yearly_income'] = users_df['yearly_income'].replace(r'[\\$,]', '', regex=True).astype(int)\n",
    "users_df['total_debt'] = users_df['total_debt'].replace(r'[\\$,]', '', regex=True).astype(int)\n",
    "\n",
    "##transactions_df##\n",
    "transactions_df[\"date\"] = pd.to_datetime(transactions_df[\"date\"])\n",
    "#æµ®é»æ•¸è½‰æ•´æ•¸åŸå› ç¢ºå®šï¼Ÿ\n",
    "transactions_df['amount'] = transactions_df['amount'].replace(r'[\\$,]', '', regex=True).astype(float).astype(int)\n",
    "##è² æ•¸å–logèª¿æˆ1\n",
    "#transactions_df['amount'] = transactions_df['amount'].replace(r'[\\$,]', '', regex=True).astype(float)\n",
    "\n",
    "transactions_df[\"use_chip\"]=transactions_df[\"use_chip\"].astype(\"category\") \n",
    "\n",
    "transactions_df.loc[\n",
    "    transactions_df['merchant_city'].str.lower() == 'online',\n",
    "    'merchant_state'\n",
    "] = 'online'\n",
    "\n",
    "transactions_df.loc[\n",
    "    transactions_df['merchant_city'].str.lower() == 'online',\n",
    "    'zip'\n",
    "] = -1\n",
    "## æˆ‘æ²’æœ‰å…¨éƒ¨æ”¹ï¼Œé€™æ¨£å®Œä¹‹å¾Œä»æœ‰89006ç­†Missingï¼Œå‰©ä¸‹éƒ½æ˜¯åœ¨åœ‹å¤–\n",
    "transactions_df['zip'] = transactions_df['zip'].fillna(-999)\n",
    "transactions_df[\"zip\"]=transactions_df[\"zip\"].astype(\"int64\")\n",
    "\n",
    "transactions_df['errors'] = transactions_df['errors'].astype('category')\n",
    "transactions_df['errors'] = transactions_df['errors'].cat.add_categories('No_error').fillna('No_error')\n",
    "\n",
    "\n",
    "#cars one hot encoding\n",
    "##çµ±ä¸€é¡åˆ¥è®Šæ•¸è½‰dummy variable(è¦æ³¨æ„å…±ç·šæ€§å•é¡Œï¼Œæ‡‰åˆªæ‰å…¶ä¸­ä¹‹ä¸€)\n",
    "\n",
    "#card_type åŸå§‹ç¨®é¡ï¼šDebit_57%, Credit_33%, Debit(Prepaid)_9%\n",
    "#card_brand åŸå§‹ç¨®é¡ï¼šMasterCard_52%, Visa_38%, Amex_7%, Discovery_3%\n",
    "#has_chip åŸå§‹ç¨®é¡ï¼šYes_89%, No_11%\n",
    "#card_on_dark_web åŸå§‹ç¨®é¡ï¼šNo_0%\n",
    "cols_to_encode = ['card_type', 'card_brand', 'has_chip']\n",
    "cards_df[cols_to_encode] = cards_df[cols_to_encode].astype('category')\n",
    "dummies_cards = pd.get_dummies(\n",
    "    cards_df[cols_to_encode], \n",
    "    prefix=cols_to_encode, \n",
    "    dtype='uint8'\n",
    "    )\n",
    "cards_df = pd.concat([cards_df, dummies_cards], axis=1)\n",
    "\n",
    "#use_chip åŸå§‹ç¨®é¡ï¼šSwiped_52%, Chipe_36%, Online_12%\n",
    "dummies_use = pd.get_dummies(transactions_df['use_chip'], prefix='use_chip', dtype='uint8')\n",
    "transactions_df = pd.concat([transactions_df, dummies_use], axis=1)\n",
    "\n",
    "#gender åŸå§‹ç¨®é¡ï¼šFemale_51%, Male_49%\n",
    "dummies_gender = pd.get_dummies(users_df['gender'], prefix='gender', dtype='uint8')\n",
    "users_df = pd.concat([users_df, dummies_gender], axis=1)\n",
    "\n",
    "\n",
    "cards_df.drop(columns=[\"has_chip_NO\",\"has_chip\"], inplace=True)\n",
    "transactions_df.drop(columns=[\"use_chip\"], inplace=True)\n",
    "users_df.drop(columns=[\"gender_Female\"], inplace=True)\n",
    "\n",
    "#transactions_df.loc[transactions_df[\"transaction_id\"] == 10649266] #transaction_id vs id\n",
    "\n",
    "#åŸå§‹è³‡æ–™ç­†æ•¸ï¼š13305915\n",
    "### transactions_df+train_fraud_labels_df      left æœƒæœ‰4390952 missing values\n",
    "merged = pd.merge(transactions_df, train_fraud_labels_df, left_on=\"transaction_id\", right_on=\"transactions_id\", how=\"outer\")\n",
    "### transactions_df train_fraud_labels_df(8914963) + users_df å°éå»ä¸æœƒæœ‰missing values\n",
    "merged = pd.merge(merged,users_df , left_on=\"client_id\", right_on=\"client_id\", how=\"left\")\n",
    "### transactions_df train_fraud_labels_df users_df + cards_df å°éå»ä¸æœƒæœ‰missing values\n",
    "merged = pd.merge(merged,cards_df , left_on=\"card_id\", right_on=\"card_id\", how=\"left\")\n",
    "\n",
    "#åˆªæ‰é‡è¤‡çš„columns\n",
    "merged.drop(columns=[\"transactions_id\"], inplace=True)\n",
    "merged.drop(columns=[\"client_id_y\"], inplace=True)\n",
    "\n",
    "del transactions_df, users_df, cards_df, train_fraud_labels_df\n",
    "\n",
    "merged[\"is_fraud\"] = merged[\"is_fraud\"].astype(str)\n",
    "merged.loc[merged['is_fraud'].str.lower() == 'no','is_fraud'] = '0'\n",
    "merged.loc[merged['is_fraud'].str.lower() == 'yes','is_fraud'] = '1'\n",
    "merged[\"is_fraud\"] = pd.to_numeric(merged[\"is_fraud\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "merged = add_missing_flags(merged, [\"is_fraud\"])\n",
    "\n",
    "#merged.to_csv(\"merged.csv\", index=False)\n",
    "\n",
    "del cols_to_encode, dummies_cards, dummies_use, dummies_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89dac21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_fraud\n",
      "0    7121379\n",
      "1      10591\n",
      "Name: count, dtype: Int64\n",
      "is_fraud\n",
      "0    1780252\n",
      "1       2741\n",
      "Name: count, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "num_cols = merged.select_dtypes(include=['int64', 'float64','uint8']).columns\n",
    "df=merged[num_cols]\n",
    "\n",
    "df_cleaned = df.dropna()\n",
    "del df\n",
    "df_cleaned.drop(columns=[\"is_fraud_missing_flag\",\"card_type_Debit (Prepaid)\", \"card_brand_Discover\", \"use_chip_Online Transaction\"], inplace=True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df_cleaned, test_size=0.2, random_state=888)\n",
    "\n",
    "del df_cleaned, merged\n",
    "trainp = train_df['is_fraud'].value_counts(normalize=False)\n",
    "print(trainp)\n",
    "testp = test_df['is_fraud'].value_counts(normalize=False)\n",
    "print(testp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a3e7f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def calculate_vif(df):\n",
    "    # 1. ä¿ç•™æ•¸å€¼æ¬„ä½\n",
    "    df_num = df.select_dtypes(include=[np.number]).copy()\n",
    "\n",
    "    # 2. å¼·åˆ¶è½‰æˆ float64ï¼Œé¿å… Int64 / uint8 / object å•é¡Œ\n",
    "    df_num = df_num.astype(np.float64)\n",
    "\n",
    "    # 3. æª¢æŸ¥ inf / NaN\n",
    "    if not np.isfinite(df_num.values).all():\n",
    "        raise ValueError(\"Data contains NaN or infinite values, cannot compute VIF.\")\n",
    "\n",
    "    # 4. åŠ ä¸Šæˆªè·\n",
    "    X = sm.add_constant(df_num)\n",
    "\n",
    "    # 5. è¨ˆç®— VIF\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"features\"] = X.columns\n",
    "    vif[\"VIF Factor\"] = [variance_inflation_factor(X.values, i) \n",
    "                         for i in range(X.shape[1])]\n",
    "\n",
    "    return vif\n",
    "\n",
    "# å…±ç·šæ€§è§€å¯Ÿ\n",
    "#vif_result = calculate_vif(train_df)\n",
    "#print(vif_result.sort_values(by=\"VIF Factor\", ascending=False).head(10))\n",
    "\n",
    "##ç¬¬ä¸€æ¬¡è™•ç†å…±ç·šæ€§\n",
    "train_df.drop(columns=[\"per_capita_income\"], inplace=True)\n",
    "##è§€å¯Ÿcard_numberçš„ç•°å¸¸è™•\n",
    "#print(train_df[\"card_number\"].nunique(), \"/\", len(train_df))\n",
    "##ç¬¬ä¸€æ¬¡è™•ç†card_number\n",
    "train_df.drop(columns=[\"card_number\"], inplace=True)\n",
    "#å†é‡è·‘ä¸€æ¬¡VIF\n",
    "#vif_result = calculate_vif(train_df)\n",
    "#print(vif_result.sort_values(by=\"VIF Factor\", ascending=False).head(10))\n",
    "##ç™¼ç¾missing_flagçš„å…±ç·šæ€§å•é¡Œï¼Œæ±ºå®šä¿ç•™one hot encodingé«˜vifå€¼çš„è®Šæ•¸\n",
    "train_df.drop(columns=[\"zip_missing_flag\",\"merchant_state_missing_flag\"], inplace=True)\n",
    "#vif_result = calculate_vif(train_df)\n",
    "#print(vif_result.sort_values(by=\"VIF Factor\", ascending=False).head(10))\n",
    "##ç§»é™¤missing_flagå…±ç·šæ€§å•é¡Œï¼Œå†æ¬¡ç¢ºèª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eeaf9858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      variable   coefficient  p_value\n",
      "0    use_chip_Chip Transaction -5.792166e-01   0.0000\n",
      "1              card_type_Debit -2.431305e-01   0.0000\n",
      "2             card_type_Credit  1.187650e-01   0.0000\n",
      "3                       amount  2.634523e-03   0.0000\n",
      "4                  merchant_id  5.815498e-06   0.0000\n",
      "5                          zip -1.023342e-04   0.0000\n",
      "6                     mcc_code -5.383161e-04   0.0000\n",
      "7          errors_missing_flag -1.034501e+00   0.0000\n",
      "8                   total_debt -1.507820e-06   0.0000\n",
      "9   use_chip_Swipe Transaction -2.282721e+00   0.0000\n",
      "10                 current_age  6.811940e-03   0.0000\n",
      "11                credit_limit -1.325726e-05   0.0000\n",
      "12                    latitude -8.201997e-03   0.0000\n",
      "13            num_credit_cards  1.108020e-01   0.0000\n",
      "14               yearly_income -4.518766e-06   0.0000\n",
      "15                has_chip_YES  1.266255e-01   0.0002\n",
      "16                credit_score  4.469540e-04   0.0027\n",
      "17                   longitude -1.721663e-03   0.0035\n",
      "18                     card_id -1.531231e-05   0.0081\n",
      "19             card_brand_Visa -4.786636e-02   0.0180\n",
      "20             card_brand_Amex  8.191560e-02   0.0324\n",
      "21                 gender_Male -3.298179e-02   0.0902\n",
      "22              transaction_id  2.666966e-09   0.1970\n",
      "23       card_brand_Mastercard -2.159213e-02   0.2679\n",
      "24              retirement_age  2.897547e-03   0.2859\n",
      "25                         cvv  3.560710e-05   0.2907\n",
      "26                 client_id_x -1.477671e-05   0.3766\n",
      "27            num_cards_issued -7.095726e-03   0.7070\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# assume train_df is your dataframe and \"is_fraud\" is the dependent variable\n",
    "y = train_df[\"is_fraud\"]\n",
    "\n",
    "# exclude the dependent variable itself\n",
    "independent_vars = train_df.columns.drop(\"is_fraud\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for var in independent_vars:\n",
    "    X = sm.add_constant(train_df[var])  # add intercept\n",
    "    model = sm.Logit(y, X)\n",
    "    try:\n",
    "        result = model.fit(disp=False)\n",
    "        coef = result.params[var]\n",
    "        pval = np.around(result.pvalues[var], 4)\n",
    "        results.append({\"variable\": var, \"coefficient\": coef, \"p_value\": pval})\n",
    "    except Exception as e:\n",
    "        results.append({\"variable\": var, \"coefficient\": None, \"p_value\": None})\n",
    "        print(f\"Skipped {var} due to error: {e}\")\n",
    "\n",
    "# convert to dataframe\n",
    "summary_df = pd.DataFrame(results)\n",
    "\n",
    "# optional: sort by p_value\n",
    "summary_df = summary_df.sort_values(\"p_value\", ascending=True).reset_index(drop=True)\n",
    "\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02c35fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LogisticRegression  # ğŸ”§ æ–°å¢\n",
    "from sklearn.preprocessing import StandardScaler     # ğŸ”§ æ–°å¢\n",
    "\n",
    "def stepwise_logit_with_k_tables(train_df, test_df, dep_var=\"is_fraud\", k=314657018,\n",
    "                                 threshold_in=0.05, threshold_out=0.10):\n",
    "    \"\"\"\n",
    "    Stepwise logistic regression (forward + backward) with flexible k control,\n",
    "    and 3 formatted output tables like table_for_first_step().\n",
    "    \"\"\"\n",
    "\n",
    "    y_train = train_df[dep_var]\n",
    "    X_train = train_df.drop(columns=[dep_var])\n",
    "    y_test = test_df[dep_var]\n",
    "    X_test = test_df.drop(columns=[dep_var])\n",
    "\n",
    "    included = []\n",
    "    step = 0\n",
    "    full_mode = (k == 314657018)\n",
    "    base_model = None\n",
    "\n",
    "    while True:\n",
    "        step += 1\n",
    "        changed = False\n",
    "\n",
    "        # ---------- Forward Step ----------\n",
    "        excluded = list(set(X_train.columns) - set(included))\n",
    "        new_pvals = pd.Series(index=excluded, dtype=float)\n",
    "        for new_var in excluded:\n",
    "            try:\n",
    "                model = sm.Logit(y_train, sm.add_constant(X_train[included + [new_var]])).fit(disp=False)\n",
    "                new_pvals[new_var] = model.pvalues[new_var]\n",
    "            except Exception:\n",
    "                new_pvals[new_var] = np.nan\n",
    "\n",
    "        if new_pvals.empty:\n",
    "            break\n",
    "\n",
    "        best_pval = new_pvals.min()\n",
    "        if best_pval < threshold_in:\n",
    "            best_var = new_pvals.idxmin()\n",
    "            included.append(best_var)\n",
    "            changed = True\n",
    "\n",
    "        # ---------- Backward Step ----------\n",
    "        if included:\n",
    "            model = sm.Logit(y_train, sm.add_constant(X_train[included])).fit(disp=False)\n",
    "            pvalues = model.pvalues.iloc[1:]\n",
    "            worst_pval = pvalues.max()\n",
    "            if worst_pval > threshold_out:\n",
    "                worst_var = pvalues.idxmax()\n",
    "                included.remove(worst_var)\n",
    "                changed = True\n",
    "\n",
    "        # ---------- çµæŸæ¢ä»¶ ----------\n",
    "        if not changed:\n",
    "            break\n",
    "        if not full_mode and len(included) >= k:\n",
    "            break\n",
    "\n",
    "    # ========= Final Model (Statsmodels) =========\n",
    "    final_model = sm.Logit(y_train, sm.add_constant(X_train[included])).fit(disp=False)\n",
    "    ll_full = final_model.llf\n",
    "    ll_null = sm.Logit(y_train, sm.add_constant(np.ones(len(y_train)))).fit(disp=False).llf\n",
    "\n",
    "    # 1ï¸âƒ£ Overall Model Fit\n",
    "    ll_diff = -2 * (ll_null - ll_full)\n",
    "    df_diff = len(final_model.params) - 1\n",
    "    p_value = stats.chi2.sf(ll_diff, df_diff)\n",
    "\n",
    "    overall_fit = pd.DataFrame({\n",
    "        \"Measure\": [\n",
    "            \"-2 Log Likelihood (âˆ’2LL) value\",\n",
    "            \"Cox and Snell R2\",\n",
    "            \"Nagelkerke R2\",\n",
    "            \"Pseudo R2 (McFadden)\",\n",
    "            \"Hosmer-Lemeshow Ï‡2\"\n",
    "        ],\n",
    "        \"Value\": [\n",
    "            round(-2 * ll_full, 3),\n",
    "            round(1 - np.exp((2 / len(y_train)) * (ll_null - ll_full)), 3),\n",
    "            round((1 - np.exp((2 / len(y_train)) * (ll_null - ll_full))) / (1 - np.exp(2 * ll_null / len(y_train))), 3),\n",
    "            round(1 - (ll_full / ll_null), 3),\n",
    "            round(ll_diff, 3)\n",
    "        ],\n",
    "        \"Change_from_Base\": [\n",
    "            round(-2 * (ll_null - ll_full), 3),\n",
    "            \"\", \"\", \"\", \"\"\n",
    "        ],\n",
    "        \"Change_pvalue\": [\n",
    "            round(p_value, 4),\n",
    "            \"\", \"\", \"\", \"\"\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    # 2ï¸âƒ£ Variables in the Equation\n",
    "    coef_df = pd.DataFrame({\n",
    "        \"Independent Variable\": final_model.params.index,\n",
    "        \"B\": final_model.params.values,\n",
    "        \"Std. Error\": final_model.bse.values,\n",
    "        \"Wald\": (final_model.params / final_model.bse) ** 2,\n",
    "        \"df\": 1,\n",
    "        \"Sig.\": final_model.pvalues.values,\n",
    "        \"Exp(B)\": np.exp(final_model.params.values)\n",
    "    })\n",
    "    coef_df = coef_df.reset_index(drop=True)\n",
    "\n",
    "    # 3ï¸âƒ£ Variables Not in the Equation\n",
    "    excluded_vars = [v for v in X_train.columns if v not in included]\n",
    "    not_in_eq = []\n",
    "    for var in excluded_vars:\n",
    "        try:\n",
    "            temp_model = sm.Logit(y_train, sm.add_constant(X_train[included + [var]])).fit(disp=False)\n",
    "            lr_stat = -2 * (final_model.llf - temp_model.llf)\n",
    "            p_val = stats.chi2.sf(lr_stat, 1)\n",
    "            not_in_eq.append({\"Independent Variable\": var,\n",
    "                              \"Score Statistic (LRT)\": round(lr_stat, 3),\n",
    "                              \"Significance\": round(p_val, 4)})\n",
    "        except Exception:\n",
    "            not_in_eq.append({\"Independent Variable\": var,\n",
    "                              \"Score Statistic (LRT)\": None,\n",
    "                              \"Significance\": None})\n",
    "\n",
    "    not_in_eq_df = pd.DataFrame(not_in_eq)\n",
    "\n",
    "    # ========= ğŸ”§ æ–°å¢ï¼šç”¨ sklearn é‡æ–°è¨“ç·´ Balanced Logistic Regression =========\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train[included])\n",
    "    X_test_scaled = scaler.transform(X_test[included])\n",
    "\n",
    "    balanced_model = LogisticRegression(\n",
    "        class_weight='balanced', solver='liblinear', max_iter=1000, random_state=42\n",
    "    )\n",
    "    balanced_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # ä¿ç•™ statsmodels çš„çµæ§‹è¼¸å‡º + sklearn æ¨¡å‹ä¸€èµ·è¿”å›\n",
    "    print(f\"\\nâœ… Stepwise completed with {len(included)} variables: {included}\")\n",
    "    print(f\"Train Accuracy (balanced sklearn): {balanced_model.score(X_train_scaled, y_train):.4f}\")\n",
    "    print(f\"Test Accuracy (balanced sklearn): {balanced_model.score(X_test_scaled, y_test):.4f}\")\n",
    "\n",
    "    return overall_fit, coef_df, not_in_eq_df, final_model, balanced_model, included, scaler  # ğŸ”§ ä¿®æ”¹ return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f2f13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_fit, coef_df, not_in_eq_df, final_model = stepwise_logit_with_k_tables(\n",
    "    train_df, test_df, dep_var=\"is_fraud\", k=314657018\n",
    ")\n",
    "\n",
    "\n",
    "print(\"=== Overall Model Fit ===\")\n",
    "print(overall_fit)\n",
    "\n",
    "print(\"\\n=== Variables in the Equation ===\")\n",
    "print(coef_df)\n",
    "\n",
    "print(\"\\n=== Variables Not in the Equation ===\")\n",
    "print(not_in_eq_df)\n",
    "\n",
    "print(\"\\n================================================\")\n",
    "print(\"\\n=== Accuracy in Training and Testing dataset ===\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def classification_table(model, df, target_col=\"is_fraud\", cutoff=0.5):\n",
    "    y_true = df[target_col].astype(int)\n",
    "    X = sm.add_constant(df[model.params.index.drop(\"const\")])\n",
    "    y_pred_prob = model.predict(X)\n",
    "    y_pred = (y_pred_prob >= cutoff).astype(int)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[1, 0])\n",
    "    TP, FN, FP, TN = cm.ravel()\n",
    "\n",
    "    fraud_total = TP + FN\n",
    "    normal_total = FP + TN\n",
    "\n",
    "    fraud_correct = TP / fraud_total if fraud_total > 0 else 0\n",
    "    normal_correct = TN / normal_total if normal_total > 0 else 0\n",
    "    overall_correct = (TP + TN) / (fraud_total + normal_total)\n",
    "\n",
    "    table = pd.DataFrame({\n",
    "        \"Actual Group\": [\"Fraud (1)\", \"Normal (0)\", \"Total\"],\n",
    "        \"Predicted Fraud (1)\": [TP, FP, TP + FP],\n",
    "        \"Predicted Normal (0)\": [FN, TN, FN + TN],\n",
    "        \"Total\": [fraud_total, normal_total, fraud_total + normal_total],\n",
    "        \"% Correct\": [\n",
    "            round(fraud_correct * 100, 1),\n",
    "            round(normal_correct * 100, 1),\n",
    "            round(overall_correct * 100, 1),\n",
    "        ],\n",
    "    })\n",
    "    return table\n",
    "\n",
    "# Step 3. ç”¢å‡ºè¨“ç·´é›†å’Œæ¸¬è©¦é›†çš„çµæœè¡¨\n",
    "train_table = classification_table(final_model, train_df, target_col=\"is_fraud\")\n",
    "test_table = classification_table(final_model, test_df, target_col=\"is_fraud\")\n",
    "\n",
    "print(\"\\n=== Classification Matrix â€” Training Sample ===\")\n",
    "print(train_table.to_string(index=False))\n",
    "print(\"\\n=== Classification Matrix â€” Holdout (Test) Sample ===\")\n",
    "print(test_table.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8358259",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ä¸€äº›æ¨¡å‹æª¢é©—è¨ºæ–·\n",
    "def cutoff_analysis(model, df, target_col=\"is_fraud\", cutoffs=None):\n",
    "    \"\"\"\n",
    "    ç”¢å‡ºé¡ä¼¼ Table 8.7 çš„çµæœ\n",
    "    \"\"\"\n",
    "    if cutoffs is None:\n",
    "        cutoffs = np.arange(0, 1.01, 0.02)  # é è¨­ 0, 0.02, ..., 1\n",
    "    \n",
    "    y_true = df[target_col].astype(int)\n",
    "    X = sm.add_constant(df[model.params.index.drop(\"const\")])\n",
    "    y_prob = model.predict(X)\n",
    "\n",
    "    rows = []\n",
    "    for cutoff in cutoffs:\n",
    "        y_pred = (y_prob >= cutoff).astype(int)\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=[0, 1])  # æ³¨æ„é †åº [0,1]\n",
    "        TN, FP, FN, TP = cm.ravel()\n",
    "\n",
    "        total = TP + TN + FP + FN\n",
    "        accuracy = (TP + TN) / total if total > 0 else np.nan\n",
    "        sensitivity = TP / (TP + FN) if (TP + FN) > 0 else np.nan\n",
    "        specificity = TN / (TN + FP) if (TN + FP) > 0 else np.nan\n",
    "        youden = sensitivity + specificity - 1 if not np.isnan(sensitivity) and not np.isnan(specificity) else np.nan\n",
    "        ppv = TP / (TP + FP) if (TP + FP) > 0 else np.nan\n",
    "        npv = TN / (TN + FN) if (TN + FN) > 0 else np.nan\n",
    "\n",
    "        rows.append({\n",
    "            \"Cutoff\": cutoff,\n",
    "            \"TN\": TN, \"FP\": FP, \"FN\": FN, \"TP\": TP,\n",
    "            \"Accuracy\": round(accuracy*100, 1),\n",
    "            \"Sensitivity\": round(sensitivity*100, 1) if not np.isnan(sensitivity) else \"NC\",\n",
    "            \"Specificity\": round(specificity*100, 1) if not np.isnan(specificity) else \"NC\",\n",
    "            \"Youden\": round(youden*100, 1) if not np.isnan(youden) else \"NC\",\n",
    "            \"PPV\": round(ppv*100, 1) if not np.isnan(ppv) else \"NC\",\n",
    "            \"NPV\": round(npv*100, 1) if not np.isnan(npv) else \"NC\",\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "cutoff_table_all = cutoff_analysis(final_model, test_df, target_col=\"is_fraud\",\n",
    "                               cutoffs=[0,0.1,0.2,0.3,0.4,0.42,0.44,0.46,0.48,0.5,0.52,0.54,0.56,0.58,0.6,0.7,0.8,0.9,1])\n",
    "\n",
    "print(cutoff_table_all.to_string(index=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208a92b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "def plot_multiple_roc(models, model_names, test_df, target_col=\"is_fraud\"):\n",
    "    \"\"\"\n",
    "    models: list of fitted statsmodels.Logit models\n",
    "    model_names: list of strings\n",
    "    test_df: test dataframe\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "    plt.rcParams['axes.facecolor'] = '#f0f0f0'\n",
    "\n",
    "    # éš¨æ©Ÿæ©Ÿç‡ç·š\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='-', label='Random Chance')\n",
    "\n",
    "    auc_values = {}\n",
    "\n",
    "    for model, name in zip(models, model_names):\n",
    "        # å–å‡ºæ¨¡å‹è®Šæ•¸\n",
    "        vars_used = model.params.index.drop(\"const\")\n",
    "        X_test = sm.add_constant(test_df[vars_used])\n",
    "\n",
    "        # è¨ˆç®—é æ¸¬æ©Ÿç‡\n",
    "        y_true = test_df[target_col]\n",
    "        y_score = model.predict(X_test)\n",
    "\n",
    "        # ROC æ›²ç·š\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "        auc_val = roc_auc_score(y_true, y_score)\n",
    "        auc_values[name] = auc_val\n",
    "\n",
    "        plt.plot(fpr, tpr, label=f\"{name} (AUC={auc_val:.4f})\")\n",
    "\n",
    "    plt.title(\"Comparing Independent Variables and Final Stepwise Model with ROC Curve\", fontsize=13, fontweight='bold')\n",
    "    plt.xlabel(\"1 - Specificity\")\n",
    "    plt.ylabel(\"Sensitivity\")\n",
    "    plt.legend(loc=\"lower right\", frameon=True)\n",
    "    plt.show()\n",
    "\n",
    "    # å°å‡º AUC summary\n",
    "    print(\"AUC values:\")\n",
    "    for name, val in auc_values.items():\n",
    "        print(f\"{name}: {val:.4f}\")\n",
    "\n",
    "# å‡è¨­ä½ å·²ç¶“æœ‰ä»¥ä¸‹æ¨¡å‹ï¼š\n",
    "# model_1 = sm.Logit(y_train, sm.add_constant(X_train[[\"X13\"]])).fit(disp=False)\n",
    "# model_2 = sm.Logit(y_train, sm.add_constant(X_train[[\"X17\"]])).fit(disp=False)\n",
    "# final_model = ...\n",
    "\n",
    "# ç¯„ä¾‹å‘¼å«\n",
    "plot_multiple_roc(\n",
    "    models=[model_0, model_2, model_3,model_4, final_model],\n",
    "    model_names=[\"Model_1 (amount)\", \"Model_2 (Zip)\", \"Model_3 (longitude)\",\"Model_4 (mercahnt_id)\", \"Final Model (26vars)\"],\n",
    "    test_df=test_df,\n",
    "    target_col=\"is_fraud\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
