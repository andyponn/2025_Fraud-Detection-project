{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01_import dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01-1_import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "#https://drive.google.com/drive/folders/18qV82fNY3IIWu3BRoGqm_LNgJzE8Akbr?usp=drive_link\n",
    "#base_dir = \"/Users/Andypon/10_äº¤å¤§ç ”ç©¶æ‰€/1141_01_æ©Ÿå™¨å­¸ç¿’èˆ‡é‡‘èç§‘æŠ€/data\"\n",
    "base_dir= '/Users/andyw.p.chen/Documents/Project/datasets'\n",
    "#base_dir=  \"c:\\Users\\user\\Downloads\\datasets\"\n",
    "\n",
    "def load_json_to_df(filename: str) -> pd.DataFrame:\n",
    "    file_path = os.path.join(base_dir, filename)\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # å¦‚æœæ˜¯ { \"target\": {id: value, ...} }\n",
    "    if isinstance(data, dict) and len(data) == 1 and isinstance(next(iter(data.values())), dict):\n",
    "        key, inner = next(iter(data.items()))\n",
    "        return pd.DataFrame(list(inner.items()), columns=[\"id\", key])\n",
    "\n",
    "    # dict of scalar\n",
    "    if isinstance(data, dict):\n",
    "        return pd.DataFrame([{\"code\": k, \"desc\": v} for k, v in data.items()])\n",
    "\n",
    "    # list of dict\n",
    "    elif isinstance(data, list):\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported JSON structure in {filename}: {type(data)}\")\n",
    "\n",
    "\n",
    "def load_csv_to_df(filename: str) -> pd.DataFrame:\n",
    "    \"\"\"è®€å– CSV ä¸¦è½‰ç‚º DataFrameã€‚\"\"\"\n",
    "    return pd.read_csv(os.path.join(base_dir, filename))\n",
    "\n",
    "# JSON è³‡æ–™\n",
    "##mcc_codes_df = load_json_to_df(\"mcc_codes.json\")\n",
    "train_fraud_labels_df = load_json_to_df(\"train_fraud_labels.json\")\n",
    "\n",
    "# CSV è³‡æ–™\n",
    "cards_df = load_csv_to_df(\"cards_data.csv\")\n",
    "transactions_df = load_csv_to_df(\"transactions_data.csv\")\n",
    "users_df = load_csv_to_df(\"users_data.csv\")\n",
    "\n",
    "# ç°¡å–®æª¢æŸ¥\n",
    "#print(mcc_codes_df.head())\n",
    "#print(train_fraud_labels_df.head())\n",
    "#print(cards_df.head())\n",
    "#print(transactions_df.head())\n",
    "#print(users_df.apthead())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01-2_rename variable in each data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fraud_labels_df = train_fraud_labels_df.rename(columns={'id': 'transactions_id'})\n",
    "train_fraud_labels_df = train_fraud_labels_df.rename(columns={'target': 'is_fraud'})\n",
    "\n",
    "cards_df = cards_df.rename(columns={'id':'card_id'})\n",
    "\n",
    "users_df = users_df.rename(columns={'id':'client_id'})\n",
    "\n",
    "transactions_df = transactions_df.rename(columns={'mcc': 'mcc_code'})\n",
    "transactions_df = transactions_df.rename(columns={'id': 'transaction_id'})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01-3_è®Šæ•¸å‹æ…‹çµ±ä¸€åŠç¼ºå¤±å€¼è™•ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing_flags(df: pd.DataFrame, cols: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    åœ¨ DataFrame ä¸­å°æŒ‡å®šæ¬„ä½å»ºç«‹ missing flag æ¬„ä½\n",
    "    flag=1 è¡¨ç¤ºç¼ºå¤±å€¼ï¼Œflag=0 è¡¨ç¤ºéç¼ºå¤±å€¼\n",
    "    \n",
    "    åƒæ•¸\n",
    "    ----\n",
    "    df : pd.DataFrame\n",
    "        è¼¸å…¥çš„è³‡æ–™æ¡†\n",
    "    cols : list\n",
    "        è¦æª¢æŸ¥çš„æ¬„ä½åç¨±æ¸…å–®\n",
    "    \n",
    "    å›å‚³\n",
    "    ----\n",
    "    pd.DataFrame : æ–°çš„è³‡æ–™æ¡† (å«æ–°å¢çš„ flag æ¬„ä½)\n",
    "    \"\"\"\n",
    "    for col in cols:\n",
    "        df[f\"{col}_missing_flag\"] = df[col].isna().astype(int)\n",
    "    return df\n",
    "\n",
    "transactions_df = add_missing_flags(transactions_df, [\"merchant_state\", \"zip\", \"errors\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##train_fraud_labels_df##\n",
    "train_fraud_labels_df[\"is_fraud\"]=train_fraud_labels_df[\"is_fraud\"].astype(\"category\") \n",
    "train_fraud_labels_df[\"transactions_id\"]=train_fraud_labels_df[\"transactions_id\"].astype(int) #åˆä½µè³‡æ–™éœ€è¦\n",
    "\n",
    "##cards_df##\n",
    "cards_df[\"card_brand\"]=cards_df[\"card_brand\"].astype(\"category\") \n",
    "cards_df[\"card_type\"]=cards_df[\"card_type\"].astype(\"category\")\n",
    "#####ä¸è¦loadé€™è¡Œ cards_df[\"expires\"]=pd.to_datetime(cards_df[\"expires\"], format=\"%m/%Y\")\n",
    "cards_df[\"expires\"] = pd.to_datetime(cards_df[\"expires\"], format=\"%m/%Y\").dt.to_period(\"M\")\n",
    "cards_df[\"has_chip\"]=cards_df[\"has_chip\"].astype(\"category\")\n",
    "\n",
    "cards_df['credit_limit'] = cards_df['credit_limit'].replace(r'[\\$,]', '', regex=True).astype(int)\n",
    "#####ä¸è¦loadé€™è¡Œ cards_df[\"acct_open_date\"]=pd.to_datetime(cards_df[\"acct_open_date\"], format=\"%m/%Y\")\n",
    "cards_df[\"acct_open_date\"] = pd.to_datetime(cards_df[\"acct_open_date\"], format=\"%m/%Y\").dt.to_period(\"M\")\n",
    "#####ä¸è¦loadé€™è¡Œ cards_df[\"year_pin_last_changed\"]=pd.to_datetime(cards_df[\"year_pin_last_changed\"], format=\"%Y\")\n",
    "cards_df[\"year_pin_last_changed\"] = pd.to_datetime(cards_df[\"year_pin_last_changed\"], format=\"%Y\").dt.to_period(\"Y\")\n",
    "cards_df[\"card_on_dark_web\"]=cards_df[\"card_on_dark_web\"].astype(\"category\") \n",
    "\n",
    "##users_df##\n",
    "users_df[\"birth_year\"] = pd.to_datetime(users_df[\"birth_year\"], format=\"%Y\").dt.to_period(\"Y\")\n",
    "users_df[\"birth_month\"] = pd.to_datetime(users_df[\"birth_month\"], format=\"%m\").dt.to_period(\"M\")\n",
    "users_df[\"gender\"]=users_df[\"gender\"].astype(\"category\") \n",
    "users_df['per_capita_income'] = users_df['per_capita_income'].replace(r'[\\$,]', '', regex=True).astype(int)\n",
    "users_df['yearly_income'] = users_df['yearly_income'].replace(r'[\\$,]', '', regex=True).astype(int)\n",
    "users_df['total_debt'] = users_df['total_debt'].replace(r'[\\$,]', '', regex=True).astype(int)\n",
    "\n",
    "##transactions_df##\n",
    "transactions_df[\"date\"] = pd.to_datetime(transactions_df[\"date\"])\n",
    "#æµ®é»æ•¸è½‰æ•´æ•¸åŸå› ç¢ºå®šï¼Ÿ\n",
    "transactions_df['amount'] = transactions_df['amount'].replace(r'[\\$,]', '', regex=True).astype(float).astype(int)\n",
    "##è² æ•¸å–logèª¿æˆ1\n",
    "#transactions_df['amount'] = transactions_df['amount'].replace(r'[\\$,]', '', regex=True).astype(float)\n",
    "\n",
    "transactions_df[\"use_chip\"]=transactions_df[\"use_chip\"].astype(\"category\") \n",
    "\n",
    "transactions_df.loc[\n",
    "    transactions_df['merchant_city'].str.lower() == 'online',\n",
    "    'merchant_state'\n",
    "] = 'online'\n",
    "\n",
    "transactions_df.loc[\n",
    "    transactions_df['merchant_city'].str.lower() == 'online',\n",
    "    'zip'\n",
    "] = 20000 #åŸæœ¬æ˜¯-1\n",
    "## æˆ‘æ²’æœ‰å…¨éƒ¨æ”¹ï¼Œé€™æ¨£å®Œä¹‹å¾Œä»æœ‰89006ç­†Missingï¼Œå‰©ä¸‹éƒ½æ˜¯åœ¨åœ‹å¤–\n",
    "transactions_df['zip'] = transactions_df['zip'].fillna(10000) #åŸæœ¬æ˜¯-999\n",
    "transactions_df[\"zip\"]=transactions_df[\"zip\"].astype(\"int64\")\n",
    "\n",
    "transactions_df['errors'] = transactions_df['errors'].astype('category')\n",
    "transactions_df['errors'] = transactions_df['errors'].cat.add_categories('No_error').fillna('No_error')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cars one hot encoding\n",
    "##çµ±ä¸€é¡åˆ¥è®Šæ•¸è½‰dummy variable(è¦æ³¨æ„å…±ç·šæ€§å•é¡Œï¼Œæ‡‰åˆªæ‰å…¶ä¸­ä¹‹ä¸€)\n",
    "\n",
    "#card_type åŸå§‹ç¨®é¡ï¼šDebit_57%, Credit_33%, Debit(Prepaid)_9%\n",
    "#card_brand åŸå§‹ç¨®é¡ï¼šMasterCard_52%, Visa_38%, Amex_7%, Discovery_3%\n",
    "#has_chip åŸå§‹ç¨®é¡ï¼šYes_89%, No_11%\n",
    "#card_on_dark_web åŸå§‹ç¨®é¡ï¼šNo_0%\n",
    "cols_to_encode = ['card_type', 'card_brand', 'has_chip']\n",
    "cards_df[cols_to_encode] = cards_df[cols_to_encode].astype('category')\n",
    "dummies_cards = pd.get_dummies(\n",
    "    cards_df[cols_to_encode], \n",
    "    prefix=cols_to_encode, \n",
    "    dtype='uint8'\n",
    "    )\n",
    "cards_df = pd.concat([cards_df, dummies_cards], axis=1)\n",
    "\n",
    "#use_chip åŸå§‹ç¨®é¡ï¼šSwiped_52%, Chipe_36%, Online_12%\n",
    "dummies_use = pd.get_dummies(transactions_df['use_chip'], prefix='use_chip', dtype='uint8')\n",
    "transactions_df = pd.concat([transactions_df, dummies_use], axis=1)\n",
    "\n",
    "#gender åŸå§‹ç¨®é¡ï¼šFemale_51%, Male_49%\n",
    "dummies_gender = pd.get_dummies(users_df['gender'], prefix='gender', dtype='uint8')\n",
    "users_df = pd.concat([users_df, dummies_gender], axis=1)\n",
    "\n",
    "\n",
    "cards_df.drop(columns=[\"has_chip_NO\",\"has_chip\"], inplace=True)\n",
    "transactions_df.drop(columns=[\"use_chip\"], inplace=True)\n",
    "users_df.drop(columns=[\"gender_Female\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02_è³‡æ–™æ•´ä½µæˆä¸€å¼µdataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02-1_è³‡æ–™æ•´ä½µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transactions_df.loc[transactions_df[\"transaction_id\"] == 10649266] #transaction_id vs id\n",
    "\n",
    "#åŸå§‹è³‡æ–™ç­†æ•¸ï¼š13305915\n",
    "### transactions_df+train_fraud_labels_df      left æœƒæœ‰4390952 missing values\n",
    "merged = pd.merge(transactions_df, train_fraud_labels_df, left_on=\"transaction_id\", right_on=\"transactions_id\", how=\"outer\")\n",
    "### transactions_df train_fraud_labels_df(8914963) + users_df å°éå»ä¸æœƒæœ‰missing values\n",
    "merged = pd.merge(merged,users_df , left_on=\"client_id\", right_on=\"client_id\", how=\"left\")\n",
    "### transactions_df train_fraud_labels_df users_df + cards_df å°éå»ä¸æœƒæœ‰missing values\n",
    "merged = pd.merge(merged,cards_df , left_on=\"card_id\", right_on=\"card_id\", how=\"left\")\n",
    "\n",
    "#åˆªæ‰é‡è¤‡çš„columns\n",
    "merged.drop(columns=[\"transactions_id\"], inplace=True)\n",
    "merged.drop(columns=[\"client_id_y\"], inplace=True)\n",
    "\n",
    "## åˆä½µå®Œä¹‹å¾Œæœ€å¾Œè™•ç†is_fraud(åŸæœƒæœ‰missing valueså•é¡Œ)\n",
    "merged[\"is_fraud\"] = merged[\"is_fraud\"].astype(str)\n",
    "merged.loc[merged['is_fraud'].str.lower() == 'no','is_fraud'] = '0'\n",
    "merged.loc[merged['is_fraud'].str.lower() == 'yes','is_fraud'] = '1'\n",
    "merged[\"is_fraud\"] = pd.to_numeric(merged[\"is_fraud\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "merged = add_missing_flags(merged, [\"is_fraud\"])\n",
    "\n",
    "#merged.to_csv(\"merged.csv\", index=False)\n",
    "\n",
    "# å…ˆåˆªé™¤ä¸éœ€è¦çš„DataFrameä»¥ç¯€çœè¨˜æ†¶é«”\n",
    "del transactions_df, users_df, cards_df, train_fraud_labels_df, cols_to_encode, dummies_cards, dummies_use, dummies_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_merged = merged.copy()\n",
    "#merged = backup_merged.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04_Benchmark model fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04-1_è³‡æ–™é€²è¡Œè®Šæ•¸è½‰æ›ä»¥æ±‚æ¨¡å‹é…é£¾æ›´ä½³è¡¨ç¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats \n",
    "\n",
    "# === (1) logè½‰æ› ===\n",
    "merged['amount'] = np.where(merged['amount'] < 0, 0, merged['amount'])  # è² æ•¸è®Š 0\n",
    "merged['amount'] = np.log(merged['amount'] + 1)  \n",
    "\n",
    "# === (3) å¹³æ–¹æ ¹è½‰æ› ===\n",
    "merged['credit_limit']=np.sqrt(merged['credit_limit'])\n",
    "merged['total_debt']=np.sqrt(merged['total_debt'])\n",
    "\n",
    "# === (3) ç«‹æ–¹æ ¹è½‰æ› ===\n",
    "merged['yearly_income']=np.cbrt(merged['yearly_income'])\n",
    "merged['per_capita_income']=np.cbrt(merged['per_capita_income'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04-2_åˆ†å‰²è¨“ç·´é›†åŠæ¸¬è©¦é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_fraud\n",
      "0    7121755\n",
      "1      10215\n",
      "Name: count, dtype: Int64\n",
      "is_fraud\n",
      "0    1779876\n",
      "1       3117\n",
      "Name: count, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "# --- é¸å–æ•¸å€¼å‹è®Šæ•¸ ---\n",
    "num_cols = merged.select_dtypes(include=['int64', 'float64','uint8','datetime64[ns]']).columns\n",
    "df2 = merged[num_cols]\n",
    "\n",
    "# --- dropna ---\n",
    "df_cleaned = df2.dropna()\n",
    "del df2\n",
    "\n",
    "# --- é¿å…å…±ç·šæ€§ ---\n",
    "df_cleaned.drop(columns=[\"is_fraud_missing_flag\",\"card_type_Debit (Prepaid)\", \n",
    "                         \"card_brand_Discover\", \"use_chip_Online Transaction\"], inplace=True)\n",
    "\n",
    "# --- ç¢ºä¿ date æ¬„ä½åœ¨ df_cleaned ä¸­ ---\n",
    "if 'date' not in df_cleaned.columns:\n",
    "    df_cleaned['date'] = merged.loc[df_cleaned.index, 'date']\n",
    "\n",
    "# --- ä¾æ™‚é–“æ’åº ---\n",
    "df_sorted = df_cleaned.sort_values('date')\n",
    "\n",
    "# --- æ™‚é–“åºåˆ—åˆ‡åˆ†ï¼ˆå‰ 80% è¨“ç·´, å¾Œ 20% æ¸¬è©¦ï¼‰ ---\n",
    "split_index = int(len(df_sorted) * 0.8)\n",
    "train_df = df_sorted.iloc[:split_index].drop(columns=['date'])  # å¯é¸æ“‡ä¸Ÿæ‰ date\n",
    "test_df  = df_sorted.iloc[split_index:].drop(columns=['date'])\n",
    "\n",
    "# --- æª¢æŸ¥è©æ¬ºè³‡æ–™åˆ†å¸ƒ ---\n",
    "print(train_df['is_fraud'].value_counts(normalize=False))\n",
    "print(test_df['is_fraud'].value_counts(normalize=False))\n",
    "\n",
    "# --- æ¸…ç†ä¸ç”¨çš„è®Šæ•¸ ---\n",
    "del df_cleaned, df_sorted, merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04-3(b1)_(ç•¥04-3(a))Assumption:Avoid Multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##è™•ç†å…±ç·šæ€§\n",
    "train_df.drop(columns=[\"per_capita_income\"], inplace=True)\n",
    "train_df.drop(columns=[\"use_chip_Chip Transaction\",\"merchant_state_missing_flag\",\"zip_missing_flag\"], inplace=True)           \n",
    "train_df.drop(columns=[\"card_brand_Visa\" ,\"card_brand_Amex\",\"card_type_Credit\"], inplace=True)\n",
    "#å†é‡è·‘ä¸€æ¬¡VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.drop(columns=[\"per_capita_income\"], inplace=True)\n",
    "test_df.drop(columns=[\"use_chip_Chip Transaction\",\"merchant_state_missing_flag\",\"zip_missing_flag\"], inplace=True)           \n",
    "test_df.drop(columns=[\"card_brand_Visa\" ,\"card_brand_Amex\",\"card_type_Credit\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04-3 Stepwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŸ¢ Step 1: Forward â€” added variable: zip (p=0)\n",
      "ğŸŸ¢ Step 2: Forward â€” added variable: amount (p=0)\n",
      "ğŸŸ¢ Step 3: Forward â€” added variable: longitude (p=0)\n",
      "ğŸŸ¢ Step 4: Forward â€” added variable: use_chip_Swipe Transaction (p=0)\n",
      "ğŸŸ¢ Step 5: Forward â€” added variable: transaction_id (p=0)\n",
      "ğŸŸ¢ Step 6: Forward â€” added variable: credit_limit (p=1.713e-191)\n",
      "ğŸŸ¢ Step 7: Forward â€” added variable: num_credit_cards (p=1.216e-90)\n",
      "ğŸŸ¢ Step 8: Forward â€” added variable: merchant_id (p=4.263e-76)\n",
      "ğŸŸ¢ Step 9: Forward â€” added variable: errors_missing_flag (p=1.674e-47)\n",
      "ğŸŸ¢ Step 10: Forward â€” added variable: yearly_income (p=6.749e-37)\n",
      "ğŸŸ¢ Step 11: Forward â€” added variable: has_chip_YES (p=2.578e-17)\n",
      "ğŸŸ¢ Step 12: Forward â€” added variable: mcc_code (p=5.329e-14)\n",
      "ğŸŸ¢ Step 13: Forward â€” added variable: client_id_x (p=2.824e-10)\n",
      "ğŸŸ¢ Step 14: Forward â€” added variable: card_type_Debit (p=1.613e-09)\n",
      "ğŸŸ¢ Step 15: Forward â€” added variable: card_brand_Mastercard (p=0.0001002)\n",
      "ğŸŸ¢ Step 16: Forward â€” added variable: credit_score (p=0.001266)\n",
      "ğŸŸ¢ Step 17: Forward â€” added variable: retirement_age (p=0.02412)\n",
      "ğŸŸ¢ Step 18: Forward â€” added variable: current_age (p=0.03339)\n",
      "âšª Step 19: No change â€” stopping iteration.\n",
      "\n",
      "âœ… Stepwise completed with 18 variables: ['zip', 'amount', 'longitude', 'use_chip_Swipe Transaction', 'transaction_id', 'credit_limit', 'num_credit_cards', 'merchant_id', 'errors_missing_flag', 'yearly_income', 'has_chip_YES', 'mcc_code', 'client_id_x', 'card_type_Debit', 'card_brand_Mastercard', 'credit_score', 'retirement_age', 'current_age']\n",
      "Train Accuracy:   0.9986\n",
      "Test Accuracy:    0.9983\n",
      "Train AUC:        0.8839854175689527\n",
      "Test AUC:         0.8374383139232547\n",
      "Train PR-AUC:     0.02238724505695911\n",
      "Test PR-AUC:      0.02563124407951196\n",
      "=== Overall Model Fit ===\n",
      "                          Measure       Value Change_from_Base Change_pvalue\n",
      "0  -2 Log Likelihood (âˆ’2LL) value  126794.175        27406.747           0.0\n",
      "1                Cox and Snell R2       0.004                               \n",
      "2                   Nagelkerke R2       0.179                               \n",
      "3            Pseudo R2 (McFadden)       0.178                               \n",
      "4              Hosmer-Lemeshow Ï‡2   27406.747                               \n",
      "\n",
      "=== Variables in the Equation ===\n",
      "          Independent Variable             B    Std. Error         Wald  df  \\\n",
      "0                        const -4.839464e+00  2.516300e-01   369.887627   1   \n",
      "1                          zip -4.051145e-05  7.111258e-07  3245.358041   1   \n",
      "2                       amount  4.898686e-01  9.581744e-03  2613.786103   1   \n",
      "3                    longitude -2.097175e-02  5.940925e-04  1246.123865   1   \n",
      "4   use_chip_Swipe Transaction -2.551385e+00  3.480398e-02  5373.954641   1   \n",
      "5               transaction_id -1.252969e-07  2.757469e-09  2064.713383   1   \n",
      "6                 credit_limit -5.481880e-03  2.488000e-04   485.465377   1   \n",
      "7             num_credit_cards  1.190505e-01  6.915561e-03   296.351944   1   \n",
      "8                  merchant_id  7.112716e-06  3.774567e-07   355.088424   1   \n",
      "9          errors_missing_flag -6.987038e-01  4.897898e-02   203.501123   1   \n",
      "10               yearly_income -2.449132e-02  2.059846e-03   141.369178   1   \n",
      "11                has_chip_YES -2.853412e-01  3.546672e-02    64.727218   1   \n",
      "12                    mcc_code -8.967991e-05  1.157385e-05    60.039208   1   \n",
      "13                 client_id_x -1.050384e-04  1.717738e-05    37.392334   1   \n",
      "14             card_type_Debit  1.197896e-01  2.374191e-02    25.456996   1   \n",
      "15       card_brand_Mastercard  7.880609e-02  2.100881e-02    14.070733   1   \n",
      "16                credit_score  4.330845e-04  1.605379e-04     7.277630   1   \n",
      "17              retirement_age  6.462013e-03  2.938806e-03     4.834971   1   \n",
      "18                 current_age -1.493644e-03  7.020950e-04     4.525874   1   \n",
      "\n",
      "             Sig.    Exp(B)  \n",
      "0    1.979982e-82  0.007911  \n",
      "1    0.000000e+00  0.999959  \n",
      "2    0.000000e+00  1.632102  \n",
      "3   5.773678e-273  0.979247  \n",
      "4    0.000000e+00  0.077974  \n",
      "5    0.000000e+00  1.000000  \n",
      "6   1.381986e-107  0.994533  \n",
      "7    2.053884e-66  1.126427  \n",
      "8    3.304347e-79  1.000007  \n",
      "9    3.596174e-46  0.497229  \n",
      "10   1.336030e-32  0.975806  \n",
      "11   8.601844e-16  0.751758  \n",
      "12   9.298648e-15  0.999910  \n",
      "13   9.660106e-10  0.999895  \n",
      "14   4.523542e-07  1.127260  \n",
      "15   1.760622e-04  1.081994  \n",
      "16   6.981860e-03  1.000433  \n",
      "17   2.788811e-02  1.006483  \n",
      "18   3.338603e-02  0.998507  \n",
      "\n",
      "=== Variables Not in the Equation ===\n",
      "  Independent Variable  Score Statistic (LRT)  Significance\n",
      "0              card_id                  0.051        0.8218\n",
      "1             latitude                  3.707        0.0542\n",
      "2           total_debt                  0.106        0.7451\n",
      "3          gender_Male                  0.128        0.7210\n",
      "4          card_number                  0.502        0.4787\n",
      "5                  cvv                  3.308        0.0689\n",
      "6     num_cards_issued                  0.813        0.3672\n",
      "\n",
      "================================================\n",
      "\n",
      "=== Accuracy in Training and Testing dataset ===\n",
      "\n",
      "=== Classification Matrix â€” Training Sample ===\n",
      "Actual Group  Predicted Fraud (1)  Predicted Normal (0)   Total  % Correct  cutoff F1 Score\n",
      "   Fraud (1)                 8431                  1784   10215       82.5  0.0015   0.0119\n",
      "  Normal (0)              1394896               5726859 7121755       80.4                 \n",
      "       Total              1403327               5728643 7131970       80.4                 \n",
      "\n",
      "=== Classification Matrix â€” Holdout (Test) Sample ===\n",
      "Actual Group  Predicted Fraud (1)  Predicted Normal (0)   Total  % Correct  cutoff F1 Score\n",
      "   Fraud (1)                 2158                   959    3117       69.2  0.0015   0.0124\n",
      "  Normal (0)               342403               1437473 1779876       80.8                 \n",
      "       Total               344561               1438432 1782993       80.7                 \n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, confusion_matrix\n",
    "\n",
    "\n",
    "def stepwise_logit_with_k_tables(train_df, test_df, dep_var=\"is_fraud\", k=314657018,\n",
    "                                 threshold_in=0.05, threshold_out=0.10, verbose=True):\n",
    "    \"\"\"\n",
    "    Stepwise logistic regression (forward + backward) with flexible k control,\n",
    "    and 3 formatted output tables like table_for_first_step().\n",
    "    \"\"\"\n",
    "\n",
    "    y_train = train_df[dep_var]\n",
    "    X_train = train_df.drop(columns=[dep_var])\n",
    "    y_test = test_df[dep_var]\n",
    "    X_test = test_df.drop(columns=[dep_var])\n",
    "\n",
    "    included = []\n",
    "    step = 0\n",
    "    full_mode = (k == 314657018)\n",
    "\n",
    "    # ========= ğŸ”¸ ç‰¹æ®Šæƒ…æ³ï¼šk = 0ï¼Œåªè·‘ intercept =========\n",
    "    if k == 0:\n",
    "        final_model = sm.Logit(y_train, sm.add_constant(np.ones(len(y_train)))).fit(disp=False)\n",
    "        ll_full = final_model.llf\n",
    "        ll_null = ll_full\n",
    "\n",
    "        overall_fit = pd.DataFrame({\n",
    "            \"Measure\": [\"-2 Log Likelihood (âˆ’2LL) value\"],\n",
    "            \"Value\": [round(-2 * ll_full, 3)],\n",
    "            \"Change_from_Base\": [\"\"],\n",
    "            \"Change_pvalue\": [\"\"]\n",
    "        })\n",
    "\n",
    "        coef_df = pd.DataFrame({\n",
    "            \"Independent Variable\": [\"const\"],\n",
    "            \"B\": final_model.params.values,\n",
    "            \"Std. Error\": final_model.bse.values,\n",
    "            \"Wald\": [np.nan],\n",
    "            \"df\": [1],\n",
    "            \"Sig.\": [\"\"],\n",
    "            \"Exp(B)\": np.exp(final_model.params.values)\n",
    "        })\n",
    "\n",
    "        not_in_eq_df = pd.DataFrame({\n",
    "            \"Independent Variable\": X_train.columns,\n",
    "            \"Score Statistic (LRT)\": [None]*len(X_train.columns),\n",
    "            \"Significance\": [None]*len(X_train.columns)\n",
    "        })\n",
    "\n",
    "        print(f\"\\nâœ… Stepwise completed with 0 variables (Intercept only).\")\n",
    "        return overall_fit, coef_df, not_in_eq_df, final_model\n",
    "    # ======================================================\n",
    "\n",
    "    # ---------- Regular Stepwise ----------\n",
    "    while True:\n",
    "        step += 1\n",
    "        changed = False\n",
    "\n",
    "        # ---------- Forward Step ----------\n",
    "        excluded = list(set(X_train.columns) - set(included))\n",
    "        new_pvals = pd.Series(index=excluded, dtype=float)\n",
    "        for new_var in excluded:\n",
    "            try:\n",
    "                model = sm.Logit(y_train, sm.add_constant(X_train[included + [new_var]])).fit(disp=False)\n",
    "                new_pvals[new_var] = model.pvalues[new_var]\n",
    "            except Exception:\n",
    "                new_pvals[new_var] = np.nan\n",
    "\n",
    "        if new_pvals.empty:\n",
    "            break\n",
    "\n",
    "        best_pval = new_pvals.min()\n",
    "        if best_pval < threshold_in:\n",
    "            best_var = new_pvals.idxmin()\n",
    "            included.append(best_var)\n",
    "            changed = True\n",
    "            if verbose:\n",
    "                print(f\"ğŸŸ¢ Step {step}: Forward â€” added variable: {best_var} (p={best_pval:.4g})\")\n",
    "\n",
    "        # ---------- Backward Step ----------\n",
    "        if included:\n",
    "            model = sm.Logit(y_train, sm.add_constant(X_train[included])).fit(disp=False)\n",
    "            pvalues = model.pvalues.iloc[1:]  # skip intercept\n",
    "            worst_pval = pvalues.max()\n",
    "            if worst_pval > threshold_out:\n",
    "                worst_var = pvalues.idxmax()\n",
    "                included.remove(worst_var)\n",
    "                changed = True\n",
    "                if verbose:\n",
    "                    print(f\"ğŸ”´ Step {step}: Backward â€” removed variable: {worst_var} (p={worst_pval:.4g})\")\n",
    "\n",
    "        # ---------- çµæŸæ¢ä»¶ ----------\n",
    "        if not changed:\n",
    "            if verbose:\n",
    "                print(f\"âšª Step {step}: No change â€” stopping iteration.\")\n",
    "            break\n",
    "        if not full_mode and len(included) >= k:\n",
    "            if verbose:\n",
    "                print(f\"ğŸŸ¡ Reached k={k}, stopping after {len(included)} variables.\")\n",
    "            break\n",
    "\n",
    "    # ========= Final Model =========\n",
    "    final_model = sm.Logit(y_train, sm.add_constant(X_train[included])).fit(disp=False)\n",
    "    ll_full = final_model.llf\n",
    "    ll_null = sm.Logit(y_train, sm.add_constant(np.ones(len(y_train)))).fit(disp=False).llf\n",
    "\n",
    "    # 1ï¸âƒ£ Overall Model Fit\n",
    "    ll_diff = -2 * (ll_null - ll_full)\n",
    "    df_diff = len(final_model.params) - 1\n",
    "    p_value = stats.chi2.sf(ll_diff, df_diff)\n",
    "\n",
    "    overall_fit = pd.DataFrame({\n",
    "        \"Measure\": [\n",
    "            \"-2 Log Likelihood (âˆ’2LL) value\",\n",
    "            \"Cox and Snell R2\",\n",
    "            \"Nagelkerke R2\",\n",
    "            \"Pseudo R2 (McFadden)\",\n",
    "            \"Hosmer-Lemeshow Ï‡2\"\n",
    "        ],\n",
    "        \"Value\": [\n",
    "            round(-2 * ll_full, 3),\n",
    "            round(1 - np.exp((2 / len(y_train)) * (ll_null - ll_full)), 3),\n",
    "            round((1 - np.exp((2 / len(y_train)) * (ll_null - ll_full))) / (1 - np.exp(2 * ll_null / len(y_train))), 3),\n",
    "            round(1 - (ll_full / ll_null), 3),\n",
    "            round(ll_diff, 3)\n",
    "        ],\n",
    "        \"Change_from_Base\": [\n",
    "            round(-2 * (ll_null - ll_full), 3),\n",
    "            \"\", \"\", \"\", \"\"\n",
    "        ],\n",
    "        \"Change_pvalue\": [\n",
    "            round(p_value, 4),\n",
    "            \"\", \"\", \"\", \"\"\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    # 2ï¸âƒ£ Variables in the Equation\n",
    "    coef_df = pd.DataFrame({\n",
    "        \"Independent Variable\": final_model.params.index,\n",
    "        \"B\": final_model.params.values,\n",
    "        \"Std. Error\": final_model.bse.values,\n",
    "        \"Wald\": (final_model.params / final_model.bse) ** 2,\n",
    "        \"df\": 1,\n",
    "        \"Sig.\": final_model.pvalues.values,\n",
    "        \"Exp(B)\": np.exp(final_model.params.values)\n",
    "    }).reset_index(drop=True)\n",
    "\n",
    "    # 3ï¸âƒ£ Variables Not in Equation\n",
    "    excluded_vars = [v for v in X_train.columns if v not in included]\n",
    "    not_in_eq = []\n",
    "    for var in excluded_vars:\n",
    "        try:\n",
    "            temp_model = sm.Logit(y_train, sm.add_constant(X_train[included + [var]])).fit(disp=False)\n",
    "            lr_stat = -2 * (final_model.llf - temp_model.llf)\n",
    "            p_val = stats.chi2.sf(lr_stat, 1)\n",
    "            not_in_eq.append({\n",
    "                \"Independent Variable\": var,\n",
    "                \"Score Statistic (LRT)\": round(lr_stat, 3),\n",
    "                \"Significance\": round(p_val, 4)\n",
    "            })\n",
    "        except Exception:\n",
    "            not_in_eq.append({\n",
    "                \"Independent Variable\": var,\n",
    "                \"Score Statistic (LRT)\": None,\n",
    "                \"Significance\": None\n",
    "            })\n",
    "\n",
    "    not_in_eq_df = pd.DataFrame(not_in_eq)\n",
    "\n",
    "    # ========= Train / Test Accuracy =========\n",
    "    train_pred = (final_model.predict(sm.add_constant(X_train[included])) > 0.5).astype(int)\n",
    "    test_pred = (final_model.predict(sm.add_constant(X_test[included])) > 0.5).astype(int)\n",
    "    train_acc = (train_pred == y_train).mean()\n",
    "    test_acc = (test_pred == y_test).mean()\n",
    "\n",
    "    print(f\"\\nâœ… Stepwise completed with {len(included)} variables: {included}\")\n",
    "    print(f\"Train Accuracy:   {train_acc:.4f}\")\n",
    "    print(f\"Test Accuracy:    {test_acc:.4f}\")\n",
    "\n",
    "    # ========= AUC & PR-AUC =========\n",
    "    train_probs = final_model.predict(sm.add_constant(X_train[included]))\n",
    "    test_probs = final_model.predict(sm.add_constant(X_test[included]))\n",
    "\n",
    "    train_auc = roc_auc_score(y_train, train_probs)\n",
    "    test_auc = roc_auc_score(y_test, test_probs)\n",
    "    train_pr_auc = average_precision_score(y_train, train_probs)\n",
    "    test_pr_auc = average_precision_score(y_test, test_probs)\n",
    "\n",
    "    print(f\"Train AUC:        {train_auc}\")\n",
    "    print(f\"Test AUC:         {test_auc}\")\n",
    "    print(f\"Train PR-AUC:     {train_pr_auc}\")\n",
    "    print(f\"Test PR-AUC:      {test_pr_auc}\")\n",
    "\n",
    "    return overall_fit, coef_df, not_in_eq_df, final_model\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# æ··æ·†çŸ©é™£åˆ†é¡è¡¨\n",
    "# -----------------------------------------------------------\n",
    "def classification_table(model, df, target_col=\"is_fraud\", cutoff=0.0015):\n",
    "    y_true = df[target_col].astype(int)\n",
    "    X = sm.add_constant(df[model.params.index.drop(\"const\")])\n",
    "    y_pred_prob = model.predict(X)\n",
    "    y_pred = (y_pred_prob >= cutoff).astype(int)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[1, 0])\n",
    "    TP, FN, FP, TN = cm.ravel()\n",
    "\n",
    "    fraud_total = TP + FN\n",
    "    normal_total = FP + TN\n",
    "\n",
    "    fraud_correct = TP / fraud_total if fraud_total > 0 else 0\n",
    "    normal_correct = TN / normal_total if normal_total > 0 else 0\n",
    "    overall_correct = (TP + TN) / (fraud_total + normal_total)\n",
    "\n",
    "    table = pd.DataFrame({\n",
    "        \"Actual Group\": [\"Fraud (1)\", \"Normal (0)\", \"Total\"],\n",
    "        \"Predicted Fraud (1)\": [TP, FP, TP + FP],\n",
    "        \"Predicted Normal (0)\": [FN, TN, FN + TN],\n",
    "        \"Total\": [fraud_total, normal_total, fraud_total + normal_total],\n",
    "        \"% Correct\": [\n",
    "            round(fraud_correct * 100, 1),\n",
    "            round(normal_correct * 100, 1),\n",
    "            round(overall_correct * 100, 1),\n",
    "        ],\n",
    "        \"cutoff\": [cutoff, \"\", \"\"],\n",
    "        \"F1 Score\": [round(2 * TP / (2 * TP + FP + FN), 4), \"\", \"\"],\n",
    "    })\n",
    "    return table\n",
    "\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "# ç‰ˆæœ¬ 2.0 (Full Model)\n",
    "# ===========================================================\n",
    "overall_fit, coef_df, not_in_eq_df, final_model = stepwise_logit_with_k_tables(\n",
    "    train_df, test_df, dep_var=\"is_fraud\", k=314657018, verbose=True\n",
    ")\n",
    "\n",
    "print(\"=== Overall Model Fit ===\")\n",
    "print(overall_fit)\n",
    "\n",
    "print(\"\\n=== Variables in the Equation ===\")\n",
    "print(coef_df)\n",
    "\n",
    "print(\"\\n=== Variables Not in the Equation ===\")\n",
    "print(not_in_eq_df)\n",
    "\n",
    "print(\"\\n================================================\")\n",
    "print(\"\\n=== Accuracy in Training and Testing dataset ===\")\n",
    "\n",
    "train_table = classification_table(final_model, train_df, target_col=\"is_fraud\")\n",
    "test_table = classification_table(final_model, test_df, target_col=\"is_fraud\")\n",
    "\n",
    "print(\"\\n=== Classification Matrix â€” Training Sample ===\")\n",
    "print(train_table.to_string(index=False))\n",
    "\n",
    "print(\"\\n=== Classification Matrix â€” Holdout (Test) Sample ===\")\n",
    "print(test_table.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "virtual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
